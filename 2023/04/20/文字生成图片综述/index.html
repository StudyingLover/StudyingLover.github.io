

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://proxy.thisis.plus/favicon.ico">
  <link rel="icon" href="https://proxy.thisis.plus/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="StudyingLover">
  <meta name="keywords" content="">
  
    <meta name="description" content="文字生成图片综述  背景 根据文字生成图像，是近几年大模型领域和多模态比较热门的研究。以NovelAI，waifu等为代表的二次元模型极大地拓展了 stable diffusion [5][24]模型和生态的想象空间。例如原本做AIGC生成小说的NovelAI推出了自己的二次元图像生成模型，基于 SD 算法框架和 Danbooru 二次元图库数据集进行训练和优化。像 NovelAI 这类的二次元">
<meta property="og:type" content="article">
<meta property="og:title" content="文字生成图片综述">
<meta property="og:url" content="https://studyinglover.com/2023/04/20/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="plus studio">
<meta property="og:description" content="文字生成图片综述  背景 根据文字生成图像，是近几年大模型领域和多模态比较热门的研究。以NovelAI，waifu等为代表的二次元模型极大地拓展了 stable diffusion [5][24]模型和生态的想象空间。例如原本做AIGC生成小说的NovelAI推出了自己的二次元图像生成模型，基于 SD 算法框架和 Danbooru 二次元图库数据集进行训练和优化。像 NovelAI 这类的二次元">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191252.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420095529.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191342.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190351.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190233.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419192846.png">
<meta property="og:image" content="https://proxy.thisis.plus/202305130808347.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312151044.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419193542.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420145907.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195507.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195421.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312143755.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312144711.png">
<meta property="og:image" content="https://proxy.thisis.plus/20230423220521.png">
<meta property="article:published_time" content="2023-04-20T15:30:00.000Z">
<meta property="article:modified_time" content="2023-06-23T13:02:17.329Z">
<meta property="article:author" content="StudyingLover">
<meta property="article:tag" content="图像生成">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191252.png">
  
  
  
     <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6818277566173475" crossorigin="anonymous"></script><script src="https://challenges.cloudflare.com/turnstile/v0/api.js" async defer></script> 
  
  <title>文字生成图片综述 - plus studio</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"studyinglover.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":"G-80B5GDZWVG","gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('https://www.google-analytics.com/analytics.js', function() {
          window.ga = window.ga || function() { (ga.q = ga.q || []).push(arguments) };
          ga.l = +new Date;
          ga('create', 'G-80B5GDZWVG', 'auto');
          ga('send', 'pageview');
        });
      }
    </script>
  

  

  

  

  

  



  <style>ins.adsbygoogle[data-ad-status="unfilled"] { display: none !important; }</style>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="plus studio" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="plus studio" type="application/rss+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>plus studio</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://drive.studyinglover.com/api/raw/?path=/photos/blog/background/1679396994125.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="文字生成图片综述"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        StudyingLover
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-04-20 15:30" pubdate>
          2023年4月20日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          19k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          156 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
<aside class="sidebar d-none d-xl-block" style="margin-right:-1rem;z-index:-1"><ins class="adsbygoogle" style="display:flex;justify-content:center;min-width:160px;max-width:300px;width:100%;height:600px;position:sticky;top:2rem" data-ad-client="ca-pub-6818277566173475" data-ad-slot="7216148023"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({}); </script></aside>
    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">文字生成图片综述</h1>
            
            
              <div class="markdown-body">
                
                <h1 style="text-align: center">
文字生成图片综述
</h1>
<h2 id="背景">背景</h2>
<p>根据文字生成图像，是近几年大模型领域和多模态比较热门的研究。以NovelAI，waifu等为代表的二次元模型极大地拓展了 stable diffusion <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup><sup id="fnref:24" class="footnote-ref"><a href="#fn:24" rel="footnote"><span class="hint--top hint--rounded" aria-label="Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022.">[24]</span></a></sup>模型和生态的想象空间。例如原本做AIGC生成小说的NovelAI推出了自己的二次元图像生成模型，基于 SD 算法框架和 Danbooru 二次元图库数据集进行训练和优化。像 NovelAI 这类的二次元模型对于用户输入的描述词的专业程度要求较高，也由社区自发整理了大量的魔典(prompt).精确控制图像的生成也是AI绘画的一个发展方向，各种可以控制人物动作，位置的方法<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models.">[10]</span></a></sup><sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup><sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup>被提出.最近openai也开源了他们最新的研究Consistency Models<sup id="fnref:20" class="footnote-ref"><a href="#fn:20" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models.">[20]</span></a></sup> ,可以1s内生成多张图片。此外，stable diffusion也被用在了3d模型的生成方面，例如 dreamfusion<sup id="fnref:25" class="footnote-ref"><a href="#fn:25" rel="footnote"><span class="hint--top hint--rounded" aria-label="Poole, B., Jain, A., Barron, J., Mildenhall, B., Research, G., &amp; Berkeley, U. (n.d.). DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION.">[25]</span></a></sup>,Point-E<sup id="fnref:26" class="footnote-ref"><a href="#fn:26" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., &amp; Chen, M. (2022). Point-E: A System for Generating 3D Point Clouds from Complex Prompts.">[26]</span></a></sup> 等。</p>
<h2 id="图像生成">图像生成</h2>
<h3 id="hypernetwork">hypernetwork</h3>
<p>hypernetwork是一种神经网络的处理方法<sup id="fnref:21" class="footnote-ref"><a href="#fn:21" rel="footnote"><span class="hint--top hint--rounded" aria-label="Abbas, M., Kivinen, J., &amp; Raiko, T. (2016). International Conference on Learning Representations (ICLR).">[21]</span></a></sup> 主要方法是通过一个神经网络影响另一个神经网络的参数，其中最具有代表性的就是GAN<sup id="fnref:22" class="footnote-ref"><a href="#fn:22" rel="footnote"><span class="hint--top hint--rounded" aria-label="Alaluf, Y., Tov, O., Mokady, R., Gal, R., &amp; Bermano, A. (2021). HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.">[22]</span></a></sup><sup id="fnref:23" class="footnote-ref"><a href="#fn:23" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dinh, TanM., Tran, A., Nguyen, R., &amp; Hua, B.-S. (n.d.). HyperInverter: Improving StyleGAN Inversion via Hypernetwork.">[23]</span></a></sup> 了.</p>
<h3 id="扩散模型">扩散模型</h3>
<p>扩散模型第一次在<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sohl-Dickstein, J., Weiss, EricL., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics.">[1]</span></a></sup> 中被提出,被称为Diffusion Probabilistic Model,之后提出的DDPM<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.">[2]</span></a></sup>中被改进。之后DDPM也衍生出了诸多版本。发布在CVPR2022的LDM<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>将图片放到隐空间上实现了图片高质量合成并提出了内容引导机制，可以通过prompt让图片生成特定内容。一年后LDM衍生除了stable diffusion<sup id="fnref:24" class="footnote-ref"><a href="#fn:24" rel="footnote"><span class="hint--top hint--rounded" aria-label="Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022.">[24]</span></a></sup><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>，掀起了ai画图的热潮。</p>
<h3 id="ddpm">DDPM</h3>
<p>DDPM分为前向过程和反向过程。DDPM假定整个过程都是一个参数化的马尔科夫链，在前向过程中对数据逐步增加高斯噪声直到数据变成一个高斯噪声，反向过程中使用U-Net<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234–241). https://doi.org/10.1007/978-3-319-24574-4_28">[4]</span></a></sup> 预测反向添加的噪声进行去噪。</p>
<p>从 <span class="math inline">\(\mathbf{X}_{T}\to\mathbf{X}_{o}\)</span> 是扩散模型的逆过程，这是在生成数据的时候是从一个随机的高斯分布采样一个信号，逐步通过去噪声恢复目标信号， <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_{t})\)</span> 这个过程的解析式是未知的。前向过程是从 <span class="math inline">\(\mathbf{X}_{0}\rightarrow\mathbf{X}_{T}\)</span> ，对一个真实信号逐步加噪声，通过选取合适的噪声尺度，理论上在一定步数以后真实信号也会变成高斯信号，可以把这个过程表示为 <span class="math inline">\(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})\)</span> 。</p>
<p>利用重参数化技术，我们可以得到从0到t的直接采样可以得到<span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}}\mathbf{x}_0,(1-\bar{\alpha})\mathbf{I})\)</span> ,其中 <span class="math inline">\(\alpha_{t}=1-\beta_{t},\bar{\alpha}t=\prod i=1^{t}\alpha_{i}\,\beta_{t}\)</span> 表示前向过程每一步的方差。这样在训练的时候我们就可以随机采样一个时刻，然后计算处这个时刻的 <span class="math inline">\(\mathbf{X}_t\)</span>。<span class="math display">\[\mathbf{x}_t(\mathbf{x}_0,\epsilon_t)=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t,\epsilon_t\sim\mathcal{N}(\mathbf{0},\mathbf{I})\]</span> <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span> 是未知的，但是可以求出<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})\)</span> 的解析解<span class="math display">\[q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}(\mathbf{x}_t,\mathbf{x}_0),\tilde{\beta}_t\mathbf{I})\]</span> <span class="math display">\[\tilde{\mu}_t\bigl(\mathbf{x}_t,\mathbf{x}_0\bigr)=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0,\tilde{\beta}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\]</span> DDPM中推导出了基于噪声误差的损失函数，即通过网络估计噪声，而不是直接估计 <span class="math inline">\(\mathbf{X}_t\)</span> ,损失函数是<span class="math display">\[\begin{aligned}L_t^{\text{simple}}&amp;=\mathbb{E}_{t\sim[1,T],\mathbf{x}_0,\epsilon_t}\left[\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\|^2\right]\\ &amp;=\mathbb{E}_{t\sim[1,T]\mathbf{x}_0,\epsilon_t}\left[\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_\theta(\sqrt{\boldsymbol{\alpha}_t}\mathbf{x}_0+\sqrt{1-\overline{\alpha}_t}\boldsymbol{\epsilon}_t,t)\|^2\right]\end{aligned}\]</span></p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191252.png" srcset="/img/loading.gif" lazyload alt="" /><figcaption>image.png</figcaption>
</figure>
<p>DDPM也有几个改进版本，例如DDIM<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising Diffusion Implicit Models.">[3]</span></a></sup> . DDIM采用更小的采样步数来加速生成过程。</p>
<h3 id="ldm">LDM</h3>
<p>为了降低训练模型时所需要的训练资源，使用latent space的LDM<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>被提出.尽管允许通过对相应损失项的低采样忽略感知上无关的细节，但这一步仍然需要在像素空间中进行昂贵的函数计算，这导致了巨大的计算时间和能源需求。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420095529.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<p>横轴是隐变量每个维度压缩的bit率，纵坐标是模型的损失。模型在学习的过程中，随着压缩率变大，刚开始模型的损失下降很快，后面下降很慢，但仍然在优化。模型首先学习到的是semantic部分的压缩/转换（大框架），这一阶段是人物semantic部分转变，然后学习到的是细节部分的压缩/转换，这是perceptual细节处的转变。</p>
<p>LDM将图像从变换到latent space上，采用了encoder-decoder的机制，图像生成又回到了以前的架构上,并引入了自注意力机制，将扩散模型转换为更有效的图像生成器。给定图像<span class="math inline">\(x\in\mathbb{R}^{H\times W\times3}\)</span> ,编码器<span class="math inline">\(\mathcal{E}\)</span> 会将图片编码到<span class="math inline">\(z=\mathcal{E}(x)\)</span> ,解码器<span class="math inline">\(\mathcal{D}\)</span> 会从latent space中重建图像。给定<span class="math inline">\(\quad\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))\)</span> ,<span class="math inline">\(z\in\mathbb{R}^{h\times w\times c}\)</span> 。更重要的是下采样倍数<span class="math inline">\({f}={H/h}=W/w\)</span> ,作者采用的是<span class="math inline">\(f=2^m,m \in N\)</span> <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191342.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<p>Conditioning Mechanisms,这里的条件可以是文字、图像等。将不同模态不同大小的条件转换为一个中间表达空间。通过这种方法可以实现prompt指导图象生成。</p>
<h3 id="consistency-models">Consistency Models</h3>
<p>Consistency Models<sup id="fnref:20" class="footnote-ref"><a href="#fn:20" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models.">[20]</span></a></sup> 是openai提出的最新的一种图片生成方法</p>
<p>diffusion<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.">[2]</span></a></sup> 的采样过程，从先验分布<span class="math inline">\(\left(x_{t_N},t_N\right)\)</span> 出发，推导采样过程<span class="math inline">\(\left(x_{t_N},t_N\right)\to\left(x_{t_{N-1}},t_{N-1}\right)\to...\to\left(x_{t_0},t_0\right)\)</span> .</p>
<p>Consistency Models 假设存在一个函数<span class="math inline">\(f\)</span>，对于上述过程中的每个点，<span class="math inline">\(f\)</span>都能输出一个相同的值,即<span class="math display">\[\begin{aligned}\boldsymbol{f}(\mathbf{x}_t,t)=\boldsymbol{f}(\mathbf{x}_{t&#39;},t&#39;)\text{for all}t,t&#39;\in[\epsilon,T]\end{aligned}\]</span> 对于轨迹起点<span class="math inline">\(x_0=\epsilon\)</span> ,有<span class="math inline">\(\boldsymbol{f}(\mathbf{x}_{\boldsymbol{\epsilon}},\epsilon)=\mathbf{x}_{\boldsymbol{\epsilon}}\)</span> .那么对于轨迹中任意一点，我们代入先验分布, 即可得到 <span class="math inline">\(f(x_{T},T)=x_{\epsilon}\)</span>  。这样也就完成了一步采样。</p>
<h2 id="文字生成图片">文字生成图片</h2>
<p>文字生成图片一个重要的前提条件是建立文字和图片的联系。CLIP首先通过对比学习的方式实现了文字图片联系。FLIP和A-CLIP对CLIP进行了改进。DALLE，GLIDE，DALLE2是OPENAI发布的文生图模型，GLIDE实现了无分类器引导的图片生成，DALLE2引入CLIP进行图片生成。Imagen主要使用文字内容进行训练，图片则先生成小图再超分放大。</p>
<h3 id="clip">CLIP</h3>
<p>OPENAI提出的CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup>通过对比学习的方式建立了文字和图片的联系.在训练过程文字和图像分别经过一个文字编码器和图像编码器得到一个对应的向量，将对应的文字向量和图像向量作为正样本，不对应的向量作为负样本进行对比学习。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190351.png" srcset="/img/loading.gif" lazyload alt="image.png" /> 考虑到大部分的数据集的标签都是以单词的形式存在的，比如“bird”，“cat”等等，然而在预训练阶段的文本描述大多都是某个短句，为了填补这种数据分布上的差别，作者考虑用“指示上下文”（guide context）对标签进行扩展。可以用<code>a photo of a &#123;object&#125;</code>作为文本端的输入。推理过程先给定一个提示<code>A photo of a &#123;object&#125;</code> ,这里的object可以填入任意的内容，然后通过一个文字编码器得到与输入内容分别对应的一组向量。同时图片经过一个图像编码器得到一个向量，将图片得到的向量分别和填入内容得到的向量计算余弦相似度，相似度最大的则是目标的描述。</p>
<h3 id="clip改进">CLIP改进</h3>
<p>何凯明团队提出的FLIP<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Li, Y., Fan, H., Hu, R., Feichtenhofer, C., &amp; He, K. (2022). Scaling Language-Image Pre-training via Masking.">[7]</span></a></sup>通过对图片加入mask有效提升了CLIP的推理速度，同期的A-CLIP<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., … Research, M. (n.d.). Attentive Mask CLIP.">[8]</span></a></sup>通过加入注意力机制保留了图像中具有语义信息的部分，避免随意加入mask对模型的训练造成影响。如图左侧是A-CLIP的过程，右侧是FLIP的结果。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190233.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<p>GLIDE<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … Chen, M. (n.d.). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.">[9]</span></a></sup>采用无分类器指导的扩散模型实现了图片生成。GLIDE， Guided Language to Image Diffusion for Generation and Editing，是 OpenAI 推出的文本引导图像生成模型，，但受到的关注相对较少。它甚至在 OpenAI 网站上也没有专门的帖子。GLIDE 生成分辨率为 256×256 像素的图像。实际上在论文中DALLE2被称为unCLIP。参数量上5B的GLIDE的FID得分超过了12B的DALLE</p>
<h3 id="dlall-e2">DLALL-E2</h3>
<p>DALL·E2<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (n.d.). Hierarchical Text-Conditional Image Generation with CLIP Latents.">[11]</span></a></sup>的架构加入了CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup>，通过锁住CLIP的文本编码器和图像编码器可以建立文字和图像的联系，加入prior和img decoder两个先验， 训练prior，使文本编码可以转换为图像编码，并训练decoder生成最终图像。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419192846.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<h3 id="imagen">Imagen</h3>
<p>谷歌的 Imagen<sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … Norouzi, M. (n.d.). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.">[12]</span></a></sup>的语言模型替换成了谷歌自家的T5-XXL，图像生成部分则是先生成小图像再上采样生成大图像，这是因为纯文本训练数据要比高质量图文对数据容易获取的多.</p>
<h3 id="lora微调">LoRA微调</h3>
<p>Low-Rank Adaptation of Large Language Models (LoRA)<sup id="fnref:29" class="footnote-ref"><a href="#fn:29" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hu, EdwardJ., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models.">[29]</span></a></sup> 是一种训练方法，可以加速大型模型的训练，同时消耗更少的内存。最初是被用在语言模型上的，在文本理解，文本生成上都取得了不错的效果</p>
<figure>
<img src="https://proxy.thisis.plus/202305130808347.png" srcset="/img/loading.gif" lazyload alt="" /><figcaption>image.png</figcaption>
</figure>
<p>做法是在原模型旁边增加一个旁路，通过低秩分解（先降维再升维）来模拟参数的更新量。训练时，原模型固定，只训练降维矩阵A和升维矩阵B，推理时，可将BA加到原参数上，不引入额外的推理延迟。此外这还是一个可拔插的模块，可以根据需要选择不同的rank</p>
<p>LoRA 的应用包括文字生成图片和图片生成图片.<sup id="fnref:31" class="footnote-ref"><a href="#fn:31" rel="footnote"><span class="hint--top hint--rounded" aria-label="cloneofsimo. (n.d.). _GitHub - Cloneofsimo/lora: Using Low-rank adaptation to quickly fine-tune diffusion models._ GitHub. Retrieved May 13, 2023, from https://github.com/cloneofsimo/lora">[31]</span></a></sup> 是第一个使用LoRA微调扩散模型的项目。Chinese-alpaca-lora<sup id="fnref:30" class="footnote-ref"><a href="#fn:30" rel="footnote"><span class="hint--top hint--rounded" aria-label="LC1332. (n.d.). _GitHub - LC1332/Chinese-alpaca-lora: 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技_. GitHub. Retrieved May 13, 2023, from https://github.com/LC1332/的应用包括文字生成图片和图片生成图片，Chinese-alpaca-lora">[30]</span></a></sup> 是一个由华中师范大学同学维护的中文语言模型，使用LoRA进行微调。</p>
<p>DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 也可以与LoRA结合进行微调<sup id="fnref:33" class="footnote-ref"><a href="#fn:33" rel="footnote"><span class="hint--top hint--rounded" aria-label="_Low-Rank adaptation of large language models (lora)_. (n.d.). Retrieved May 13, 2023, from https://huggingface.co/docs/diffusers/training/lora#dreambooth">[33]</span></a></sup></p>
<h2 id="图像编辑">图像编辑</h2>
<p>图像编辑也是文字生成图片的重要应用。Imagic输入一个文本图像和目标文本，通过多阶段的方法对齐文本和图像编辑图像。ControlNet通过复制参数为锁住的和可训练的，使模型可以为特定任务进行微调。同时ControlNet可以传入openpose人的位姿图，canny边缘图，深度图，Hough变换生成的图等各种图片可控得生成图片。Google的DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 提出了一种使用少量图片进行微调的方式，提供一种用户训练自己模型的方法。prompt2prompt通过更改图片对应的map的方式特定更改图片。</p>
<h3 id="imagic">Imagic</h3>
<p>Imagic<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models.">[10]</span></a></sup>提出的方法只需要一个输入图像和一个目标文本(所需的编辑)。它生成一个与输入图像和目标文本一致的文本嵌入，同时微调扩散模型以捕获特定于图像的外观。Imagic通过多阶段的方法实现了图片编辑，分为优化文本嵌入，微调扩散模型。在优化的嵌入和目标文本嵌入之间进行线性插值三个过程。首先优化文本嵌入，使其生成与输入图像相似的图像。然后，对预训练的生成扩散模型(以优化的嵌入为条件)进行微调，以更好地重建输入图像。最后，在目标文本嵌入和优化后的文本之间进行线性插值，得到一个结合了输入图像和目标文本的表示。然后将这种表示传递给带有微调模型的生成扩散过程，输出最终编辑的图像。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312151044.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<h3 id="controlnet">ControlNet</h3>
<p>ControlNet<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup>通过对参数复制，将参数分为锁住的和可训练的。锁着的参数从大量的图片文本对中学习更通用的信息，可学习的参数在特定的任务上进行微调,让模型在个人电脑和大型计算集群上都可以获得很好的训练效果。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419193542.png" srcset="/img/loading.gif" lazyload alt="image.png" /> 以2D图像为例，给定一张图像(特征图)<span class="math inline">\(\boldsymbol{x}\in\mathbb{R}^{h\times w\times c}\)</span> ,<span class="math inline">\(h,w,c\)</span> 分别代表高度，宽度，深度。一个将x转换为y的神经网络我们可以以将他记作<span class="math inline">\(\mathcal{F(\cdot;\Theta)}\)</span> 我们把zero convolution(就是1 <span class="math inline">\(*\)</span> 1卷积)记作<span class="math inline">\(\mathcal{Z}(\cdot;\cdot)\)</span> ,那么ControlNet就可以记作<span class="math display">\[\begin{matrix}\boldsymbol{y_c}=\mathcal{F}(\boldsymbol{x};\Theta)+\mathcal{Z}(\boldsymbol{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{z2}})\end{matrix}\]</span> 由于zero convolution的权重初始为0，那么就有<span class="math display">\[\begin{cases}\mathcal{Z}(c;\Theta_{\text{z1}})=\mathbf{0}\\ \mathcal{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{c}})=\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}})=\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}})\\ \mathcal{Z}(\mathcal{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{c}});\Theta_{\text{z2}})=\mathcal{Z}(\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}});\Theta_{\text{z2}})=\mathbf{0}\end{cases}\]</span> 可以得出<span class="math inline">\(y_c=y\)</span>,即当ControlNet被应用到任何一个网络上时，不会对这个网络的效果产生任何影响。它完美保留了任何神经网络块的能力、功能和结果质量，任何进一步优化都将随着微调而变得很快。</p>
<p>在训练过程中，作者随机的将50%的prompt换成了空的prompt，作者认为这可以增强模型从文本内容识别语义信息的能力。这主要是因为当 stable diffusion 模型看不到提示时，decoder倾向于从输入控制图中学习更多的语义作为提示的替代品。</p>
<p>ControlNet还给出了在个人电脑和大型计算集群上进行训练的方式。当计算设备有限时，作者发现部分打破ControlNet与stable diffusion之间的联系可以加速收敛。默认情况下是将ControlNet连接到“SD Middle Block”和“SD Decoder Block 1,2,3,4”(stable diffuion的模块)。作者发现，只连接Middle Block而不连接Decoder Block 1,2,3,4可以将训练速度提高1.6倍(在RTX 3070TI笔记本电脑GPU上测试)。当模型在结果和条件之间表现出合理的关联时，这些断开连接的链接可以在持续训练中再次连接，以促进精确控制。</p>
<p>openai在论文还比较了在不同的数据集上不同的编码器的效果</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420145907.png" srcset="/img/loading.gif" lazyload alt="" /><figcaption>image.png</figcaption>
</figure>
<p>MutilDiffusion<sup id="fnref:14" class="footnote-ref"><a href="#fn:14" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bar-Tal, O., Yariv, L., Lipman, Y., &amp; Dekel, T. (2023). MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation.">[14]</span></a></sup> 将图片分为几个部分分别进行diffusion，然后将他们拼在一起通过一个全局去噪网络可以更好的控制生成图片中物体的位置。</p>
<h3 id="dreambooth">DreamBooth</h3>
<p>DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 是Google提出的一个通过少量图片微调diffusion model 的方法。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195507.png" srcset="/img/loading.gif" lazyload alt="image.png" /> 要训练自己数据最直观的方法，就是把自己的图片加入模型迭代时一起训练。但会带来两个问题，一个是过拟合，另一个是语义漂移(language drift)。总的来说DreamBooth的贡献在两方面，一方面是给定主题可以生成特定的主题的图片，一方面给定少数镜头微调diffusion model的方法同时保留输入图片的语义信息。 而Dreambooth的优势就在于能避免上述的两个问题。主要方法就是使用一个具有特殊含义而且比较少见的词，训练的图片最好有不同角度和光线下的图片。下图是DreamBooth论文给出的不同模型效果的对比图 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195421.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<h4 id="cascade-ef-gan">Cascade EF-GAN</h4>
<p>Cascade EF-GAN<sup id="fnref:28" class="footnote-ref"><a href="#fn:28" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wu, R., Zhang, G., Lu, S., &amp; Chen, T. (2020, March 12). _Cascade EF-GAN: Progressive facial expression editing with local focuses_. arXiv.Org. https://arxiv.org/abs/2003.05905">[28]</span></a></sup> 是一种级联式的人脸编辑方式，可以更好地保留与身份相关的特征和细节，特别是在眼睛、鼻子和嘴巴周围，进一步帮助减少生成的面部图像中的伪影和模糊。</p>
<p>作者设计了一种级联式网络，同原本对一张人脸做更改变成了对一张人脸和脸上几个部分同时做更改。因为对一个人类来说分辨一个人的方式就是看人的眼睛，鼻子和嘴巴。Cascade EF-GAN能够识别面部表情编辑中局部重点的重要性，并通过几个局部重点捕捉身份相关特征，有效地减轻编辑产生的伪影和模糊。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312143755.png" srcset="/img/loading.gif" lazyload alt="" /><figcaption>image.png</figcaption>
</figure>
<p>Cascade EF-GAN中的生成模型由一个Expression Transformer和一个Refiner组成。Expression Transformer执行带有局部焦点的表情编辑，Refiner融合表情转换器的输出并细化最终编辑。</p>
<p>Expression Transformer通过在全局和局部分支中处理面部图像来解决这个问题，其中全局分支捕获全局面部结构，局部分支专注于更详细的面部特征。Transformer将面部图像和目标表情标签作为输入。<strong>所有分支共享相似的网络架构，但不共享权重</strong></p>
<p>此外注意力被引入到全局和局部分支，以更好地捕捉细节和抑制伪影。在GANimation [32]中，使用视觉注意力来引导网络集中于转换与表情相关的区域。然而，在单个全局图像中应用注意力往往会引入模糊的注意力响应，如图3的第4列所示。这是因为全局注意力倾向于关注最显著的变化，例如图3中的嘴部区域，而眼睛和鼻子周围的细微变化则没有受到足够的关注。前面提到的局部分支中的独占式注意力有助于在局部区域实现更锐利的响应，如图3的第3列所示。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312144711.png" srcset="/img/loading.gif" lazyload alt="image.png" /></p>
<p>每个分支输出颜色特征图M_C和注意图M_A。对于原始输入图像I_in，每个分支的初始输出通过以下方式生成 <span class="math display">\[\mathcal{I}_{init}=M_A\otimes M_C+(1-M_A)\otimes I_{in}\]</span></p>
<p>Refiner负责融合表情转换器不同分支的输出，生成最终的表情编辑。如图2所示，三个局部分支的输出首先根据它们在面部图像中的各自位置缝合成单个图像。缝合的图像然后与全局分支的输出连接，并馈送到细化器以生成最终的表情编辑。</p>
<h4 id="prompt2prompt">prompt2prompt</h4>
<p>prompt2prompt<sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup> 是Google提出的一种基于Imagen的图像编辑方法，相比于直接text2image生成，文本引导图片生成要求原来图像绝大部分区变化不大，先前的方法需要用户指定mask来引导生成。prompt2prompt的主要方法是将交叉注意力机制引入diffusion中，得到每个token对应的attention map，一种有三种操作的方式 1. token换词，那么直接替换attention map即可。 2. 加词，则是直接在对应位置加入新的attention map。 3. token增强——直接提高对应的map的权重。</p>
<p>但这种方法也有一些局限，例如需要用户给一个合理的prompt，细节的生成不太好，不能对图中的物体进行移位操作。</p>
<h4 id="instructpix2pix">InstructPix2Pix</h4>
<p>InstructPix2Pix<sup id="fnref:27" class="footnote-ref"><a href="#fn:27" rel="footnote"><span class="hint--top hint--rounded" aria-label="Brooks, T., Holynski, A., &amp; Efros, AlexeiA. (2022). InstructPix2Pix: Learning to Follow Image Editing Instructions.">[27]</span></a></sup> 是一种无需微调就可以快速编辑图像的方法，结合了两个大型预训练模型的知识——语言模型和文本到图像模型——生成了大量的图像编辑示例数据集。通过在这些数据上进行训练，并在推理时能够适用于真实图像和用户编写的指令。但也有一些局限例如数据带来的偏差，能会对图像进行不必要的过度更改。</p>
<h2 id="多模态">多模态</h2>
<p>多模态学习是指从多个模态表达或感知事物。 多模态可归类为同质性的模态，例如从两台相机中分别拍摄的图片，异质性的模态，例如图片与文本语言的关系。Jeff Dean在2019年年底NeurIPS大会上的一个采访报道，讲到了2020年机器学习趋势：多任务和多模态学习将成为突破口。</p>
<h3 id="clip-1">CLIP</h3>
<p>CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 是openai关于文本和图像的一片工作，采用对比学习实现了图片的理解。</p>
<p>AudioCLIP 在原本的CLIP架构中加入了声音的模态。</p>
<h3 id="i3d">I3D</h3>
<p>I3D<sup id="fnref:34" class="footnote-ref"><a href="#fn:34" rel="footnote"><span class="hint--top hint--rounded" aria-label="Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502">[34]</span></a></sup> 是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics</p>
<figure>
<img src="https://proxy.thisis.plus/20230423220521.png" srcset="/img/loading.gif" lazyload alt="" /><figcaption>image.png</figcaption>
</figure>
<p>这篇文章提出的模型被称为 Two-Stream Inflated 3D ConvNets</p>
<p>Inflate 是模型的核心操作，含义是将一个2d模型"膨胀"成3d模型，做法很简单，就是把一个<span class="math inline">\(N*N\)</span> 的层变成<span class="math inline">\(N*N*N\)</span> ,同时也将参数复制了<span class="math inline">\(N\)</span> 遍。</p>
<h3 id="segement-anything">Segement anything</h3>
<p>Segement anything<sup id="fnref:15" class="footnote-ref"><a href="#fn:15" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything.">[15]</span></a></sup> 是meta最近一篇图像分割的工作，使用一个SOTA的zero-shot目标检测器提取物体box和类别，然后输入给SAM模型出mask，使得模型可以根据文本输入检测和分割任意物体。</p>
<p>在社区的努力下，实现了Segement anything和stable diffusion的协同<sup id="fnref:36" class="footnote-ref"><a href="#fn:36" rel="footnote"><span class="hint--top hint--rounded" aria-label="IDEA-Research. (n.d.). _GitHub - IDEA-Research/Grounded-Segment-Anything: Marrying grounding DINO with segment anything &amp; stable diffusion &amp; tag2text &amp; BLIP &amp; whisper &amp; chatbot - Automatically detect , segment and generate anything with image, text, and audio inputs_. GitHub. Retrieved May 13, 2023, from https://github.com/IDEA-Research/Grounded-Segment-Anything">[36]</span></a></sup></p>
<h3 id="imagebind">ImageBind</h3>
<p>ImageBind<sup id="fnref:35" class="footnote-ref"><a href="#fn:35" rel="footnote"><span class="hint--top hint--rounded" aria-label="Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K., Joulin, A., &amp; Misra, I. (2023). ImageBind: One Embedding Space To Bind Them All.">[35]</span></a></sup> 是meta 的最新工作,是一个学习六种不同模态的方法-图像、文本、音频、深度、温度和IMU数据。此外在学习过程中不需要提供所有模态的信息，作者发现只要将每个模态的嵌入对齐到图像嵌入，就会导致所有模态的emergent alignment(涌现现象)。</p>
<p>ImageBind的目标是通过使用图像将所有模态绑定在一起，学习所有模态的单一联合嵌入空间。通过使用 Web 数据将每个模态的嵌入与图像嵌入对齐，例如使用带有 IMU 的自我中心相机捕获的视频数据将文本对齐到图像，将 IMU 对齐到视频，最后可以获得zero-shot的能力。然而，一个模态的不能直接应用于其他两个模态的组合，例如视频不能直接在图片-IMU上使用。</p>
<p>IMAGEBIND使用模态对<span class="math inline">\((I，M)\)</span> , 其中<span class="math inline">\(I\)</span>代表图像，<span class="math inline">\(M\)</span>是另一种模态，来学习单个联合嵌入.作者使用包含广泛语义概念的（图像，文本）配对的大规模网络数据集。也使用其他模态本身的自我监督配对，如音频、深度、热和惯性测量单元（IMU）与图像。使用InfoNCE<sup id="fnref:38" class="footnote-ref"><a href="#fn:38" rel="footnote"><span class="hint--top hint--rounded" aria-label="Oord, A., Li, Y., &amp; Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding.">[38]</span></a></sup>损失优化嵌入和编码器。<span class="math display">\[L_{\mathcal{I},\mathcal{M}}=-\log\frac{\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_i/\tau)}{\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_i/\tau)+\sum_{j\neq i}\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_j/\tau)}\]</span></p>
<p>尽管ImageBind只是用了六种模态进行训练，但是未来可以使用更多的数据和模态进行训练，将实现更丰富的以人为中心的 AI 模型。</p>
<h2 id="prompt">prompt</h2>
<p>prompt提示可以给文字生成图片提供语义信息。</p>
<h4 id="clip-2">CLIP</h4>
<p>CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 使用了<code>A photo of a &#123;object&#125;</code>作为prompt，<code>object</code> 是推理过程中的选择项，作者也讨论了大量的prompt相关的问题。一个常见的问题是一词多义。当一个类的名称是提供给CLIP文本编码器的唯一信息时，由于缺乏上下文，它无法区分哪个词的含义。在某些情况下，同一个单词的多个含义可能作为不同的类包含在同一个数据集中。另一个问题是，在我们的预训练数据集中，与图像配对的文本只是一个单词的情况相对较少。通常文本是一个完整的句子，以某种方式描述图像。通过使用<code>A photo of a &#123;object&#125;</code> 就可以使ImageNet的准确率提高1.3%。作者还发现在不同的数据集上使用不同的prompt可以取得不同的结果。</p>
<h4 id="controlnet-1">ControlNet</h4>
<p>在 ControlNet<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup> 模型采取了<em>三种</em> prompt 1. No prompt：也就是"" 2. Default prompt:由于stable diffusion本质上是使用prompt进行训练的，因此空字符串可能是模型的意外输入，如果没有提供提示，SD 往往会生成随机纹理图。更好的设置是使用无意义的提示，"an image", "a nice image", "a professional image",etc。在作者的设置中，使用"a professional, detailed, high-quality image"作为default prompt。 3. Automatic prompt:为了测试fully automatic pipeline的SOTA，作者还尝试使用fully automatic pipeline（例如，BLIP）使用“default prompt”模式获得的结果生成prompts。作者会使用生成的提示再次扩散。 4. User prompt：用户自定义的输入</p>
<h4 id="prompt2prompt-1">prompt2prompt</h4>
<p>prompt2prompt<sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup>的操作就是通过prompt进行的。</p>
<h3 id="clip-interrogator">CLIP-interrogator</h3>
<p>在背景图生成这个任务下有一个可能需要的步骤，从给出的人物图得到一些prompt生成图片，CLIP-interrogator 就是为了这样的任务而生的，开源的有CLIP-Interrogator<sup id="fnref:16" class="footnote-ref"><a href="#fn:16" rel="footnote"><span class="hint--top hint--rounded" aria-label="_CLIP interrogator_. (n.d.). A Hugging Face Space by Pharma. Retrieved April 19, 2023, from https://huggingface.co/spaces/pharma/CLIP-Interrogator">[16]</span></a></sup><sup id="fnref:18" class="footnote-ref"><a href="#fn:18" rel="footnote"><span class="hint--top hint--rounded" aria-label="pharmapsychotic. (n.d.). _GitHub - Pharmapsychotic/clip-interrogator: Image to prompt with BLIP and CLIP_. GitHub. Retrieved April 19, 2023, from https://github.com/pharmapsychotic/clip-interrogator">[18]</span></a></sup> 和CLIP-Interrogator2<sup id="fnref:17" class="footnote-ref"><a href="#fn:17" rel="footnote"><span class="hint--top hint--rounded" aria-label="_CLIP interrogator 2_. (n.d.). A Hugging Face Space by Fffiloni. Retrieved April 19, 2023, from https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">[17]</span></a></sup> .模型主要通过CLIP对以后数据集进行匹配获取prompt、而通过BLIP获得图像最直观的理解。Code底层也是需要CLIP和BLIP作为核心完成后面的工作。</p>
<h2 id="数据集">数据集</h2>
<h3 id="wit">WIT</h3>
<p>openai 在CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 提到他们构建了一个新的数据集，从互联网上各种公开可用的资源中收集了4亿(图像，文本)对。为了尝试覆盖尽可能广泛的视觉内容，对每一类都有大概有2000对对整个数据集进行平衡。结果数据集的总字数与用于训练GPT-2的WebText数据集相似。这个数据集称为WebImageText的WIT。</p>
<p>作者团队从直接与摄影师一起工作的提供商那里获得了一组新的高分辨率的11M图像。即使在下采样之后，这些图像的分辨率也明显高于许多现有的视觉数据集。</p>
<h3 id="sa-1b">SA-1B</h3>
<p>segment anything<sup id="fnref:15" class="footnote-ref"><a href="#fn:15" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything.">[15]</span></a></sup> 是Meta最新提出的一个用于目标分割的方法，他们为了更好的训练模型制作了一个迄今为止最大的分割数据集，1100万张在10亿次授权且尊重隐私的图像上的数据集，同时开源了他们的数据集，此外还有一种Data engine 的方法来快速生成数据集。</p>
<p>Data engine 分为三个阶段：（1）模型辅助手动注释阶段，（2）混合自动预测掩码和模型辅助注释的半自动阶段，以及（3）全自动阶段，</p>
<h4 id="手动阶段">手动阶段</h4>
<p>在第一阶段，类似于经典的交互式分割，一组专业注释者通过使用由 SAM 驱动的基于浏览器的交互分割工具点击前景/背景对象点来标记掩码。可以使用像素精确的“刷”和“擦除”工具来细化掩码。模型辅助注释直接在浏览器内实时运行（使用预先计算的图像嵌入），从而实现真正的交互体验。标注不受语义约束，可以自由地标注"stuff" and "things"</p>
<p><strong>注释者被要求按突出顺序标记对象，一旦掩码需要超过 30 秒进行注释，便鼓励继续下一个图像。</strong></p>
<p>在SOTA之后，SAM就开始使用公共数据集进行训练，在经过了足够多的数据标注后，就用新标注的数据重新训练。随着收集更多的掩码，图像使用了ViT-H作为编码器。这样的模型训练一共进行了六次。随着模型的改进，每个掩码的平均注释时间从 34 秒减少到 14 秒。随着SAM的改进，每张图像的平均掩码数从20个掩码增加到44个掩码。总体而言，作者在这个阶段从 120k 张图像收集了 4.3M 掩码。</p>
<h4 id="半自动化阶段">半自动化阶段</h4>
<p>这个阶段的目标是增加mask的多样性。为了将标记集中在不太突出的对象上，首先自动检测confident masks。然后向注释者展示了用这些掩码预先填充的图像，并要求他们注释任何额外的未注释对象。为了检测confident masks，作者使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界框检测器。在这个阶段，作者在 180k 图像中收集了一个额外的 5.9M 掩码（总共 10.2M 掩码）。在第一阶段，在新收集的数据（5 次）上定期重新训练模型。每个掩码的平均注释时间可以回到了到 34 秒（不包括自动掩码），因为这些对象对标签更具挑战性。每张图像的平均掩码数从 44 个掩码到 72 个掩码（包括自动掩码）。</p>
<h4 id="全自动化阶段">全自动化阶段</h4>
<p>这个阶段的主要目的是解决歧义</p>
<p>这个过程作者使用<span class="math inline">\(32*32\)</span>网格的点对图像进行预测，并为每个点预测一组可能对应于有效对象的掩码。如果说一个点位于一个部件或子部件上，我们的模型将返回子部分，部分和整个对象(subpart, part, and whole object)。利用模型中的IoU预测模块来选择confident mask,IOU阈值是0.7，那么这个掩码就被认为是是稳定的。为了进一步提高小mask的质量，还处理了多个重叠的放大mask。</p>
<h3 id="kinetics">Kinetics</h3>
<p>Kinetics 是<sup id="fnref:34" class="footnote-ref"><a href="#fn:34" rel="footnote"><span class="hint--top hint--rounded" aria-label="Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502">[34]</span></a></sup> 提出的一个视频理解数据集，Kinetics 有400个人体动作类，每个类有400多个例子，每个都来自一个独特的 YouTube 视频。</p>
<h2 id="参考文献">参考文献</h2>
<section class="footnotes">
<div class="footnote-list">
<ol>
<li>
<span id="fn:1" class="footnote-text"><span>Sohl-Dickstein, J., Weiss, EricL., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:2" class="footnote-text"><span>Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:3" class="footnote-text"><span>Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising Diffusion Implicit Models. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:4" class="footnote-text"><span>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234–241). https://doi.org/10.1007/978-3-319-24574-4_28 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:5" class="footnote-text"><span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:6" class="footnote-text"><span>Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:7" class="footnote-text"><span>Li, Y., Fan, H., Hu, R., Feichtenhofer, C., &amp; He, K. (2022). Scaling Language-Image Pre-training via Masking. <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:8" class="footnote-text"><span>Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., … Research, M. (n.d.). Attentive Mask CLIP. <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:9" class="footnote-text"><span>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … Chen, M. (n.d.). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:10" class="footnote-text"><span>Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models. <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:11" class="footnote-text"><span>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (n.d.). Hierarchical Text-Conditional Image Generation with CLIP Latents. <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:12" class="footnote-text"><span>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … Norouzi, M. (n.d.). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:13" class="footnote-text"><span>Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models. <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:14" class="footnote-text"><span>Bar-Tal, O., Yariv, L., Lipman, Y., &amp; Dekel, T. (2023). MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. <a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:15" class="footnote-text"><span>Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything. <a href="#fnref:15" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:16" class="footnote-text"><span><em>CLIP interrogator</em>. (n.d.). A Hugging Face Space by Pharma. Retrieved April 19, 2023, from https://huggingface.co/spaces/pharma/CLIP-Interrogator <a href="#fnref:16" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:17" class="footnote-text"><span><em>CLIP interrogator 2</em>. (n.d.). A Hugging Face Space by Fffiloni. Retrieved April 19, 2023, from https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2 <a href="#fnref:17" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:18" class="footnote-text"><span>pharmapsychotic. (n.d.). <em>GitHub - Pharmapsychotic/clip-interrogator: Image to prompt with BLIP and CLIP</em>. GitHub. Retrieved April 19, 2023, from https://github.com/pharmapsychotic/clip-interrogator <a href="#fnref:18" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:19" class="footnote-text"><span>Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models. <a href="#fnref:19" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:20" class="footnote-text"><span>Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models. <a href="#fnref:20" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:21" class="footnote-text"><span>Abbas, M., Kivinen, J., &amp; Raiko, T. (2016). International Conference on Learning Representations (ICLR). <a href="#fnref:21" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:22" class="footnote-text"><span>Alaluf, Y., Tov, O., Mokady, R., Gal, R., &amp; Bermano, A. (2021). HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing. <a href="#fnref:22" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:23" class="footnote-text"><span>Dinh, TanM., Tran, A., Nguyen, R., &amp; Hua, B.-S. (n.d.). HyperInverter: Improving StyleGAN Inversion via Hypernetwork. <a href="#fnref:23" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:24" class="footnote-text"><span>Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022. <a href="#fnref:24" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:25" class="footnote-text"><span>Poole, B., Jain, A., Barron, J., Mildenhall, B., Research, G., &amp; Berkeley, U. (n.d.). DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION. <a href="#fnref:25" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:26" class="footnote-text"><span>Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., &amp; Chen, M. (2022). Point-E: A System for Generating 3D Point Clouds from Complex Prompts. <a href="#fnref:26" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:27" class="footnote-text"><span>Brooks, T., Holynski, A., &amp; Efros, AlexeiA. (2022). InstructPix2Pix: Learning to Follow Image Editing Instructions. <a href="#fnref:27" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:28" class="footnote-text"><span>Wu, R., Zhang, G., Lu, S., &amp; Chen, T. (2020, March 12). <em>Cascade EF-GAN: Progressive facial expression editing with local focuses</em>. arXiv.Org. https://arxiv.org/abs/2003.05905 <a href="#fnref:28" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:29" class="footnote-text"><span>Hu, EdwardJ., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <a href="#fnref:29" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:30" class="footnote-text"><span>LC1332. (n.d.). <em>GitHub - LC1332/Chinese-alpaca-lora: 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技</em>. GitHub. Retrieved May 13, 2023, from https://github.com/LC1332/的应用包括文字生成图片和图片生成图片，Chinese-alpaca-lora <a href="#fnref:30" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:31" class="footnote-text"><span>cloneofsimo. (n.d.). <em>GitHub - Cloneofsimo/lora: Using Low-rank adaptation to quickly fine-tune diffusion models.</em> GitHub. Retrieved May 13, 2023, from https://github.com/cloneofsimo/lora <a href="#fnref:31" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:32" class="footnote-text"><span>Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. <a href="#fnref:32" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:33" class="footnote-text"><span><em>Low-Rank adaptation of large language models (lora)</em>. (n.d.). Retrieved May 13, 2023, from https://huggingface.co/docs/diffusers/training/lora#dreambooth <a href="#fnref:33" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:34" class="footnote-text"><span>Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502 <a href="#fnref:34" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:35" class="footnote-text"><span>Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K., Joulin, A., &amp; Misra, I. (2023). ImageBind: One Embedding Space To Bind Them All. <a href="#fnref:35" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:36" class="footnote-text"><span>IDEA-Research. (n.d.). <em>GitHub - IDEA-Research/Grounded-Segment-Anything: Marrying grounding DINO with segment anything &amp; stable diffusion &amp; tag2text &amp; BLIP &amp; whisper &amp; chatbot - Automatically detect , segment and generate anything with image, text, and audio inputs</em>. GitHub. Retrieved May 13, 2023, from https://github.com/IDEA-Research/Grounded-Segment-Anything <a href="#fnref:36" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:37" class="footnote-text"><span>Guzhov, A., Raue, F., Hees, J., &amp; Dengel, A. (2021). AudioCLIP: Extending CLIP to Image, Text and Audio. <a href="#fnref:37" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
<li>
<span id="fn:38" class="footnote-text"><span>Oord, A., Li, Y., &amp; Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. <a href="#fnref:38" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
</ol>
</div>
</section>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AC%94%E8%AE%B0/" class="category-chain-item">笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">#图像生成</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>文字生成图片综述</div>
      <div>https://studyinglover.com/2023/04/20/文字生成图片综述/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>StudyingLover</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年4月20日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>

<div style="width:100%;display:flex;justify-content:center;margin-bottom:1.5rem"><ins class="adsbygoogle" style="display:flex;justify-content:center;max-width:845px;width:100%;height:90px" data-ad-client="ca-pub-6818277566173475" data-ad-slot="7216148023"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({}); </script></div>

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/" title="ControlNet代码改造计划">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">ControlNet代码改造计划</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/04/07/Segment%20Anything%E7%AC%94%E8%AE%B0/" title="Segment Anything笔记">
                        <span class="hidden-mobile">Segment Anything笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"StudyingLover/StudyingLover.github.io","repo-id":"R_kgDOIQbc2w","category":"Announcements","category-id":"DIC_kwDOIQbc284CT0AH","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> <div style="width: 20vw; height: 15vh; margin: 0 auto; text-align: center;"> <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bLcIH0Eh5s3UeoICvFfbZ3tHKvl8FJUjfBLHkJp-6sc&cl=ffffff&w=a"></script> </div> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6818277566173475"crossorigin="anonymous"></script>

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
