<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>plus studio</title>
  
  
  <link href="https://studyinglover.com/atom.xml" rel="self"/>
  
  <link href="https://studyinglover.com/"/>
  <updated>2023-07-26T16:08:48.547Z</updated>
  <id>https://studyinglover.com/</id>
  
  <author>
    <name>StudyingLover</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DINO-v2笔记</title>
    <link href="https://studyinglover.com/2023/07/27/DINO-v2%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/27/DINO-v2%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-27T00:04:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dino-v2笔记">DINO-v2笔记</h1><p>DINO-v2一种无监督学习的预训练方法，可以生成具有强大泛化能力的视觉特征，适用于各种图像分布和任务，而无需进行微调。这篇论文重点介绍了数据和模型规模方面的技术贡献，包括自动构建一个多样化和精心筛选的图像数据集、在多个层级上进行训练、使用Sinkhorn-Knopp居中方法和KoLeo正则化等。实验结果表明，该方法在多个图像理解任务上的表现超过了目前公开的最佳无监督和半监督方法。</p><p>作者实际上花了大量的篇幅减少了数据如何创建，如何进行预训练和如何优化训练过程。</p><p><a href="https://dinov2.metademolab.com/">项目主页</a>,项目开源在<a href="https://github.com/facebookresearch/dinov2">GitHub</a></p><h2 id="数据集准备">数据集准备</h2><p>作者通过从一个大型未筛选数据池中检索与几个精选数据集中的图像接近的图像来组装他们的LVD-142M数据集。作者描述了数据处理流程的主要组成部分，包括精选/未筛选数据源，图像去重步骤和检索系统。整个流程不需要任何元数据或文本，直接使用图像。</p><p>整个处理分布在一个由20个节点组成的计算集群上，该集群配备了8个V100-32GB GPU，生成LVD-142M数据集不到两天。</p><h3 id="数据来源">数据来源</h3><p>作者在包含 ImageNet-22k，ImageNet-1k、Google Landmarks 和几个细粒度数据集的的数据集进行选择。对于不安全的数据源，爬取公开可用的网络数据存储库中收集了原始未过滤的图像数据集。从存储库中的每个网页中，作者从<img>标签中提取图像的 URL 链接。作者在构建数据集过程中丢弃了不受域限制或限制的 URL，并对下载的图像（PCA 哈希重复数据删除、NSFW 过滤和模糊可识别人脸）进行后处理。这导致 1.2B 个独特的图像。</p><h3 id="消除重复数据">消除重复数据</h3><p>作者将使用了A Self-Supervised Descriptor for Image Copy Detection 这篇论文中的方法来处理未经处理的数据，并去除接近重复的图像。这减少了冗余并增加了图像之间的多样性。此外还删除了这个工作中使用的任何基准测试或验证集中包含的几乎重复的图像。</p><h3 id="自监督的图像检索">自监督的图像检索</h3><p>首先使用在ImageNet-22k上预训练的自监督ViT-H/16网络来计算图像嵌入，并使用余弦相似性作为图像之间的距离度量。接下来对未分级的数据进行k-means聚类。给定要检索的查询数据集，如果它足够大，那么就为每个查询图像检索N个（通常是4个）最近的邻居。如果它很小，就从与每个查询图像相对应的聚类中采样M个图像。可以通过目视检查检索结果来调整N和M。</p><h2 id="判别式自监督的预培训">判别式自监督的预培训</h2><h3 id="图像级目标">图像级目标</h3><p>同一图像的不同裁剪中获得不同的部分，使用ViT进行编码，用过去迭代的指数移动平均值构建教师模型，从学生和教师网络中提取的特征之间的交叉熵损失学习学生模型的参数</p><h3 id="patch级目标">patch级目标</h3><p>随即屏蔽给学生的一些输入补丁，但不屏蔽给老师的。然后，我们在每个屏蔽补丁上的两个网络的补丁特征之间添加交叉熵损失。这种损失与图像级别的损失相结合。</p><h3 id="解绑两个目标的权重联系">解绑两个目标的权重联系</h3><p>将上面两个目标相关的权重捆绑在一起会使模型在patch上欠拟合，而在图像级别上过拟合。解开这些权重可以解决这个问题，并提高两个目标的性能。</p><h3 id="sinkhorn-knopp-centering">Sinkhorn-Knopp centering</h3><p>这是一种替代DINO和iBot模型中的teacher softmax-centering步骤的方法，即使用SwAV模型的Sinkhorn-Knopp（SK）批量归一化。作者在这个方法中运行了3次Sinkhorn-Knopp算法步骤，并对学生应用softmax归一化。这个方法的目的是提高自监督学习模型的性能。</p><h3 id="koleo-regularizer">KoLeo regularizer</h3><p>KoLeo正则化器源自Kozachenko-Leonenko差分熵估计器，它鼓励批处理中特征的均匀跨度。给定一组n个向量(x1, . . . , xn)，它被定义为<span class="math inline">\(\mathcal{L}_\text{koleo}=-\frac1n\sum_{i=1}^n\log(d_{n,i})\)</span> ，其中<span class="math inline">\(d_{n,i}=\min_{j\neq i}\left\|x_i-x_j\right\|\)</span>是<span class="math inline">\(x_i\)</span>和批处理中任何其他点之间的最小距离。在计算这个正则化器之前，我们还要对特征进行L2-归一化。</p><h3 id="adapting-the-resolution">Adapting the resolution</h3><p>在像素级别的下游任务中，如分割或检测，提高图像分辨率是非常重要的，因为低分辨率下小物体容易消失。但是高分辨率的训练需要更多的时间和内存，所以作者提出了一种方法，在预训练的最后一段时间内将图像的分辨率提高到518×518。</p><h2 id="有效的实施">有效的实施</h2><p>作者对于训练大规模模型的几个改进措施，包括使用A100 GPU和PyTorch 2.0进行训练，提供代码和预训练模型，并在附录的Table 17中详细描述了模型的细节。</p><p>另外，与iBOT实现相比，DINOv2的代码在相同硬件条件下，运行速度提高了2倍，内存使用量减少了三分之一。</p><h3 id="快速高效的注意力">快速高效的注意力</h3><p>作者自己实现了一个fastattention,需要注意的是<strong>作者的ViT-g架构略有不同，采用1536的嵌入维度和24个头（每个头64维），而不是1408的嵌入维度和16个头（每个头88维），以最大化计算效率</strong>。</p><h3 id="自注意中的嵌套张量">自注意中的嵌套张量</h3><p>作者使用了一种新的技术，可以在同一个正向传递中运行全局裁剪和局部裁剪（具有不同数量的补丁标记），与之前的实现相比，可以获得显着的计算效率提升。此外，作者提到他们使用的基础组件已经在xFormers库中提供。</p><h3 id="有效的随机深度">有效的随机深度</h3><p>作者使用了一种改进的随机深度（stochastic depth）方法，相比于传统的掩码方法，该方法跳过了被丢弃的残差计算，从而在一定程度上节省了内存和计算资源。在本次实验中，使用高丢弃率（d=40%）时，这种方法使计算效率和内存使用效率得到了显著提高。具体实现方法是通过在批处理维度上随机重新排列B个样本，并在块计算中仅对前<span class="math inline">\((1-d)×B\)</span>个样本进行计算。</p><h3 id="完全共享数据并行fsdp">完全共享数据并行（FSDP）</h3><p>通过将模型副本分配到多个GPU中，可以将模型大小限制在GPU节点总内存的范围内。此外，FSDP的实现方式可以将权重片段存储为float32，但在传播权重和梯度时使用float16，从而降低跨GPU通信成本。相较于DistributedDataParallel（DDP）中使用的float32梯度all-reduce操作，使用Pytorch-FSDP混合精度训练的通信成本减少了约50％，在扩展GPU节点数量时训练过程更加高效。总的来说，Pytorch-FSDP混合精度训练在几乎所有情况下都优于使用autocast的DDP。</p><h3 id="模型蒸馏">模型蒸馏</h3><p>作者发现即使对于一个规模较大的ViT-L模型，他们的预训练方法也能够取得比从头开始训练更好的性能。此外，他们还提出了一种知识蒸馏方法，与A simple recipe for competitive low-compute self supervised vision models. arXiv preprint arXiv:2301.09451 所描述的方法相似，但没有修改蒸馏的损失项，并评估了学生模型的指数移动平均值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;dino-v2笔记&quot;&gt;DINO-v2笔记&lt;/h1&gt;
&lt;p&gt;DINO-v2一种无监督学习的预训练方法，可以生成具有强大泛化能力的视觉特征，适用于各种图像分布和任务，而无需进行微调。这篇论文重点介绍了数据和模型规模方面的技术贡献，包括自动构建一个多样化和精心筛选的图像</summary>
      
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/categories/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
    
  </entry>
  
  <entry>
    <title>快速上手llama2.c</title>
    <link href="https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/"/>
    <id>https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/</id>
    <published>2023-07-25T16:19:00.000Z</published>
    <updated>2023-07-26T16:08:48.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速上手llama2.c">快速上手llama2.c</h1><p><a href="https://github.com/karpathy/llama2.c.git">llama2.c</a>一个完整的解决方案，可以使用PyTorch从头开始训练Llama 2 LLM（Lightweight Language Model）模型，并将权重导出为二进制文件，然后加载到一个简单的500行C文件（run.c）中进行推理。另外，你也可以加载、微调和推理Meta的Llama 2模型（但这部分仍在积极开发中）。因此，这个仓库提供了一个"全栈"的训练和推理方案，专注于极简和简洁性。你可能会认为只有拥有数十亿参数的LLM才能实现有用的功能，但事实上，如果领域足够狭窄，非常小的LLM也可以表现出惊人的性能。建议参考TinyStories论文以获得灵感。</p><p>需要注意的是，这个项目最初只是一个有趣的周末项目：作者在之前的nanoGPT基础上进行了调整，实现了Llama-2架构而不是GPT-2，并且主要的工作是编写了C推理引擎（run.c）。因此，这个项目还比较年轻，并且在快速发展中。特别感谢llama.cpp项目为此项目提供了灵感。作者希望保持超级简洁，所以选择了硬编码Llama 2架构，采用fp32精度，并仅使用纯C编写一个没有依赖项的推理文件。</p><p>首先clone整个仓库并编译 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/llama2.c.git<br><span class="hljs-built_in">cd</span> llama.c<br>gcc -O3 -o run run.c -lm<br></code></pre></td></tr></table></figure></p><p>接下来下载模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://karpathy.ai/llama2c/model.bin -P out<br></code></pre></td></tr></table></figure></p><p>或者下载更大的一个模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://karpathy.ai/llama2c/model44m.bin -P out44m<br></code></pre></td></tr></table></figure></p><p>接下来进行推理 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./run out/model.bin<br></code></pre></td></tr></table></figure></p><p>我们将会看到这样一段输出就代表运行成功 <figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">&lt;s&gt;<br> One day, <span class="hljs-keyword">a</span> little otter named Ollie went <span class="hljs-built_in">to</span> play <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> river. Ollie was very compassionate. He loved <span class="hljs-built_in">to</span> help his friends <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> town.<br>While playing, Ollie saw <span class="hljs-keyword">a</span> big fish. The fish was stuck <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> mud. <span class="hljs-string">&quot;Help me, please!&quot;</span> said <span class="hljs-keyword">the</span> fish. Ollie wanted <span class="hljs-built_in">to</span> help <span class="hljs-keyword">the</span> fish. He swam away, looking <span class="hljs-keyword">for</span> something <span class="hljs-built_in">to</span> break <span class="hljs-keyword">the</span> mud.<br>Ollie found <span class="hljs-keyword">a</span> small stick. He used <span class="hljs-keyword">the</span> stick <span class="hljs-built_in">to</span> break <span class="hljs-keyword">the</span> mud. The fish was free! <span class="hljs-string">&quot;Thank you, Ollie!&quot;</span> <span class="hljs-keyword">the</span> fish said. The fish was happy <span class="hljs-keyword">and</span> swam away.<br>Ollie felt good <span class="hljs-keyword">for</span> helping <span class="hljs-keyword">the</span> fish. He went back <span class="hljs-built_in">to</span> play <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> river. Ollie knew that helping others made him feel good. And <span class="hljs-built_in">from</span> that day, Ollie was always compassionate <span class="hljs-built_in">to</span> everyone.<br>&lt;s&gt;<br> Tom was <span class="hljs-keyword">a</span> big boy who liked <span class="hljs-built_in">to</span> help his mom. He saw his mom doing laundry <span class="hljs-keyword">and</span> asked <span class="hljs-keyword">if</span> he could join. His mom said yes, but he had <span class="hljs-built_in">to</span> be careful <span class="hljs-keyword">with</span> <span class="hljs-keyword">the</span> iron. The iron was hot <span class="hljs-keyword">and</span> had <span class="hljs-keyword">a</span> button <span class="hljs-keyword">on</span> <span class="hljs-title">it</span>.<br>Tom took <span class="hljs-keyword">the</span> iron <span class="hljs-keyword">and</span> ran <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> house. He wanted <span class="hljs-built_in">to</span> iron his shirt<br>achieved tok/s: <span class="hljs-number">178.148921</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;快速上手llama2.c&quot;&gt;快速上手llama2.c&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/karpathy/llama2.c.git&quot;&gt;llama2.c&lt;/a&gt;一个完整的解决方案，可以使用PyTorch从头开始训练Llama 2</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>AnyDoor笔记</title>
    <link href="https://studyinglover.com/2023/07/24/AnyDoor%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/24/AnyDoor%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-24T19:39:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="anydoor笔记">AnyDoor笔记</h1><p>在这项工作中，香港大学，阿里联合提出了提出了 AnyDoor，这是一种基于扩散的生成器，可以进行对象隐形传态。这项研究的核心贡献是使用判别 ID 提取器和频率感知细节提取器来表征目标对象。在视频和图像数据的不同组合上进行训练，我们在场景图像的特定位置合成对象。AnyDoor 为一般区域到区域的映射任务提供了通用解决方案，并且可以为各种应用有利可图。</p><p><a href="https://damo-vilab.github.io/AnyDoor-Page/">项目地址</a></p><p>AnyDoor的模型架构图如下图所示，看起来还是比较清晰的，我们一部分一部分来看 <img src="https://cdn.studyinglover.com/pic/2023/07/84e8fadaba321d5eb9be47f710d22997.png" alt="image.png" /></p><h2 id="id特征提取器">ID特征提取器</h2><p>一般都选择CLIP的图像编码器编码一个图像对象。但是CLIP 是由粗略描述的文本对训练而来的，所以CLIP 只能给出一些语义上的描述，但是很难对每个物体的特征做出分辨。所以作者做出了两点更新，背景移除和自监督的表示。</p><p>对应到整个pipeline就是这部分 <img src="https://cdn.studyinglover.com/pic/2023/07/e22b104405f6f2e4ace7a680c6d44e23.png" alt="image.png" /></p><h3 id="背景移除">背景移除</h3><p>背景移除就是使用一个分割模型将背景删除，然后将目标物体和背景中心对齐。可以使用一些自动的模型(例如Segment Anything),和可交互式的模型</p><h3 id="自监督的表示">自监督的表示</h3><p>作者说在这个工作中他们发现自监督的模型可以很好的保留物体的判别特征。在大规模数据集上进行预训练，自监督模型自然配备了实例检索能力，可以将对象投影到增强不变的特征空间(augmentation-invariant feature space，经过抱大佬大腿，这个特征空间是说经过图像增强语义不变)中。作者采用了DINO2作为编码器，得到了一个全局的特征<span class="math inline">\(\mathbf{T}_{\mathrm{g}}^{1 \times 1536}\)</span> 和一个局部的特征<span class="math inline">\(\mathbf{T}_{\mathrm{p}}^{256 \times 1536}\)</span> ,使用一个线性层将这两个向量投影到UNet需要的维度，然后合并俩个向量使用，最后的向量是<span class="math inline">\(\mathbf{T}_{\text {ID }}^{257 \times 1024}\)</span></p><h2 id="细节特征提取">细节特征提取</h2><p>作者认为，由于 ID 令牌会丢失空间分辨率，因此它们很难充分保持目标对象的精细细节。所以使用互补的细节作为生成过程中额外的指导。</p><p>使用拼贴作为控件可以提供强大的先验，作者尝试将“背景移除的对象”缝合到场景图像的给定位置。通过这个拼贴，可以观察到生成保真度的显着改进，但生成的结果与缺乏多样性的给定目标过于相似。面对这个问题，作者探索设置信息瓶颈以防止拼贴给出太多外观约束。实际上就是设计了一个高频映射来表示对象，它可以保持精细细节，但允许通用的局部变体，如手势、照明、方向等。</p><p>对应pipeline的这部分， <img src="https://cdn.studyinglover.com/pic/2023/07/02e880c65b826610ff0afc47e939fc40.png" alt="image.png" /></p><p>作者使用了这样一个公式来提取高频图<span class="math display">\[\mathbf{I}_h=\left(\mathbf{I} \otimes \mathbf{K}_h+\mathbf{I} \otimes \mathbf{K}_v\right) \odot \mathbf{I} \odot \mathbf{M}_{\text {erode }}\]</span> <span class="math inline">\(\mathbf{I}\)</span> 是一张RGB的图像(上一步背景移除得到的图片),<span class="math inline">\(\mathbf{K}_h\)</span> 和<span class="math inline">\(\mathbf{K}_v\)</span> 是水平和垂直Sobel kernel，这里被用作高频滤波器，<span class="math inline">\(\otimes\)</span>代表卷积,<span class="math inline">\(\odot\)</span>代表逐元素乘法。侵蚀掩码 <span class="math inline">\(\mathbf{M}_{\text {erode }}\)</span> 来过滤目标对象外部轮廓附近的信息。</p><p>在得到高频图后，根据给定的位置将其拼接到场景图像上，然后将拼贴传递给细节提取器。细节提取器是一个 ControlNet 中的UNet 编码器，它生成一系列具有分层分辨率的细节图。</p><h2 id="特征注入">特征注入</h2><p>在获得 ID 标记和细节图后，将它们注入到预训练的文本到图像扩散模型中以指导生成。作者选择了stable diffusion，它将图像投影到潜在空间中，并使用UNet进行概率采样。我们注意到预训练的 UNet 为 <span class="math inline">\(\hat{\mathbf{x}}_\theta\)</span> ，它从初始潜在噪声 <span class="math inline">\(\epsilon \sim \mathcal{U}([0,1])\)</span>开始去噪，并将文本嵌入 c 作为生成新图像潜在 的条件。训练监督是均方误差损失为<span class="math display">\[\mathbb{E}_{\mathbf{x},\mathbf{c},\epsilon,t}(\|\hat{\mathbf{x}}_\theta(\alpha_t\mathbf{x}+\sigma_t\epsilon,\mathbf{c})-\mathbf{x}\|_2^2)\]</span> <span class="math inline">\(\mathbf{x}\)</span> 是ground-truth,t是反向过程的步数,<span class="math inline">\(\alpha_t\)</span> <span class="math inline">\(\sigma_t\)</span> 是去噪的超参数。</p><p>在这项工作中，文本嵌入 c 被替换为前面的 ID 标记，这些标记通过交叉注意注入到每个 UNet 层。对于细节图，将它们与每个分辨率的 UNet 解码器特征连接起来。在训练期间，模型冻结 UNet 编码器的预训练参数以保留先验并调整 UNet 解码器以适应我们的新任务。</p><h2 id="训练策略">训练策略</h2><h3 id="图像文本对">图像文本对</h3><p>理想的训练样本是“不同场景中同一对象”的图像对，但是这些数据集不能直接由现有数据集提供。作为替代方案，以前的工作利用单个图像并应用旋转、翻转和弹性变换等增强。然而，这些幼稚的增强不能很好地代表姿势和视图的真实变体。</p><p>为了解决这个问题，在这项工作中，作者使用视频数据集来捕获包含相同对象的不同帧。</p><h3 id="自适应的训练步长">自适应的训练步长</h3><p>虽然视频数据有利于学习外观变化，但由于分辨率低或运动模糊，帧质量通常不能令人满意。相比之下，图像可以提供高质量的细节和通用的场景，但缺乏外观变化。</p><p>为了利用视频数据和图像数据，作者开发了自适应时间步采样，使不同模态的数据有利于去噪训练的不同阶段。stable dissusion为每个训练数据均匀地采样时间步长 (T)。然而，观察到初始去噪步骤主要集中在生成整体结构、姿势和视图；后面的步骤涵盖了纹理和颜色等精细细节 。因此，对于视频数据，可以增加了在训练期间采样早期去噪步骤（大 T）以更好地学习外观变化的可能性。对于图像，增加了后期步骤（小 T）的概率来学习如何覆盖精细细节。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;anydoor笔记&quot;&gt;AnyDoor笔记&lt;/h1&gt;
&lt;p&gt;在这项工作中，香港大学，阿里联合提出了提出了 AnyDoor，这是一种基于扩散的生成器，可以进行对象隐形传态。这项研究的核心贡献是使用判别 ID 提取器和频率感知细节提取器来表征目标对象。在视频和图像数据的</summary>
      
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/categories/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
    
  </entry>
  
  <entry>
    <title>Archlinux安装scrcpy加载共享库出错 error while loading shared libraries:libusb-1.0.so.0:wrong ELF class:ELFCLASS32</title>
    <link href="https://studyinglover.com/2023/07/21/Archlinux%E5%AE%89%E8%A3%85scrcpy%E5%8A%A0%E8%BD%BD%E5%85%B1%E4%BA%AB%E5%BA%93%E5%87%BA%E9%94%99/"/>
    <id>https://studyinglover.com/2023/07/21/Archlinux%E5%AE%89%E8%A3%85scrcpy%E5%8A%A0%E8%BD%BD%E5%85%B1%E4%BA%AB%E5%BA%93%E5%87%BA%E9%94%99/</id>
    <published>2023-07-21T16:13:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="archlinux安装scrcpy加载共享库出错">Archlinux安装scrcpy加载共享库出错</h1><p>在安装scrcpy时通过<code>sudo pacman -S scrcpy</code>顺利安装,但是运行报错 <figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">scrcpy:</span> <span class="hljs-keyword">error</span> <span class="hljs-keyword">while</span> loading <span class="hljs-keyword">shared</span> libraries: libusb-<span class="hljs-number">1.0</span>.so.<span class="hljs-number">0</span>: wrong ELF <span class="hljs-keyword">class</span>: ELFCLASS32<br></code></pre></td></tr></table></figure></p><p>这是在64位系统上运行32位库出错，我发现了这个10年的issue https://github.com/Rouji/Ergodone-Setup/issues/1 也就是说我们只需要运行<code>sudo pacman -S libusb-compat</code></p><p>但是运行之后出现了新的问题 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span> <br><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br></code></pre></td></tr></table></figure></p><p>一般来说已经有的库就不要动它了，运行<code>sudo pacman -Syu</code> 没有解决，会报同样的错误，说明libusb这个文件不是包管理器提供的，那就删掉现有的库然后让pacman帮我们安装</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> pacman -S libusb-compat<br></code></pre></td></tr></table></figure><p>插上手机，运行<code>scrcpy</code>,成功运行</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;archlinux安装scrcpy加载共享库出错&quot;&gt;Archlinux安装scrcpy加载共享库出错&lt;/h1&gt;
&lt;p&gt;在安装scrcpy时通过&lt;code&gt;sudo pacman -S scrcpy&lt;/code&gt;顺利安装,但是运行报错 &lt;figure class=&quot;</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
    <category term="ArchLinux" scheme="https://studyinglover.com/tags/ArchLinux/"/>
    
  </entry>
  
  <entry>
    <title>npc_gzip笔记</title>
    <link href="https://studyinglover.com/2023/07/18/npc_gzip%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/18/npc_gzip%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-18T16:57:00.000Z</published>
    <updated>2023-07-26T16:08:48.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="npc_gzip笔记">npc_gzip笔记</h1><h2 id="论文笔记">论文笔记</h2><p>npc_gzip 的论文名叫做 "Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors ,意为不需要参数，使用压缩器的文本分类方法。论文的代码也只有仅仅的十四行，就在部分数据集上取得了超越 <strong>bert</strong> 的效果。</p><p>npc_gzip由一个无损压缩器，一个基于距离的度量函数和K近邻算法组成。</p><p>使用压缩器进行分类的直觉是有两方面 1. 压缩器擅长捕捉规律性；<br />2. 来自同一类别的对象比不同类别的对象具有更多的规律性。</p><p>假设<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> 属于相同的类别，<span class="math inline">\(x_3\)</span> 属于不同的类别，用<span class="math inline">\(C(\cdot)\)</span> 代表压缩器， 我们会发现<span class="math inline">\(C\left(x_1 x_2\right)-C\left(x_1\right)&lt;C\left(x_1 x_3\right)-C\left(x_1\right)\)</span> , <span class="math inline">\(C\left(x_1 x_2\right)\)</span> 代表 x1 和 x2 的串联的压缩长度。换句话说<span class="math inline">\(C\left(x_1 x_2\right)\)</span> 可以解释为我们仍然需要根据 x1 的信息对 x2 进行编码多少字节： <figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs erlang">x1 = Japan&#x27;s Seiko Epson Corp. has developed a <span class="hljs-number">12</span>-gram flying microrobot.<br><br>x2 = The latest tiny flying robot has been unveiled in Japan.<br><br>x3 = Michael Phelps won the gold medal in the <span class="hljs-number">400</span> individual medley.<br></code></pre></td></tr></table></figure></p><p>这种直觉可以形式化为源自 Kolmogorov 复杂度的距离度量。Kolmogorov 复杂度 K(x) 表征了可以生成 x 的最短二进制程序的长度。K(x) 理论上是信息测量的最终下限。</p><p><span class="math display">\[\begin{aligned} E(x, y) &amp; =\max \{K(x \mid y), K(y \mid x)\} \\ &amp; =K(x y)-\min \{K(x), K(y)\}\end{aligned}\]</span></p><p>由于 Kolmogorov 复杂度的可计算性质使得 E(x,y) 不可计算，所以可以使用归一化压缩距离 (NCD)，利用压缩长度 C(x) 来近似 Kolmogorov 复杂度 K(x)。形式上是<span class="math display">\[N C D(x, y)=\frac{C(x y)-\min \{C(x), C(y)\}}{\max \{C(x), C(y)\}}\]</span> 使用压缩长度背后的直觉是压缩器最大压缩的 x 的长度接近 K(x)。一般来说，压缩比越高，C(x)越接近K(x)。</p><p>实验的结果使用 gzip 作为压缩器，这里的<span class="math inline">\(C(x)\)</span> 表示 gzip 压缩后 x 的长度。<span class="math inline">\(C(xy)\)</span> 是 x 和 y 的串联的压缩长度。NCD 提供距离矩阵使用 k-最近邻来执行分类。</p><p>核心代码真的真的就非常简单了 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gzip2 <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">for</span> ( x1 , _ ) <span class="hljs-keyword">in</span> test_set :<br>Cx1 = <span class="hljs-built_in">len</span> ( gzip . compress ( x1 . encode () ) )<br>distance_from_x1 = []<br><span class="hljs-keyword">for</span> ( x2 , _ ) <span class="hljs-keyword">in</span> training_set :<br>Cx2 = <span class="hljs-built_in">len</span> ( gzip . compress ( x2 . encode () )<br>x1x2 = <span class="hljs-string">&quot; &quot;</span> . join ([ x1 , x2 ])<br>Cx1x2 = <span class="hljs-built_in">len</span> ( gzip . compress ( x1x2 . encode () )<br>ncd = ( Cx1x2 - <span class="hljs-built_in">min</span> ( Cx1 , Cx2 )) / <span class="hljs-built_in">max</span> ( Cx1 , Cx2 )<br>distance_from_x1 . append ( ncd )<br>sorted_idx = np . argsort ( np . array ( distance_from_x1 ) )<br>top_k_class = training_set [ sorted_idx [: k ] , <span class="hljs-number">1</span>]<br>predict_class = <span class="hljs-built_in">max</span> ( <span class="hljs-built_in">set</span> ( top_k_class ) , key = top_k_class . count )<br></code></pre></td></tr></table></figure></p><p>这种方法是 DNN 的简单、轻量级和通用的替代方案。很简单，因为它不需要任何预处理或训练。它的轻量级在于它不需要参数或 GPU 资源进行分类。由于压缩器是数据类型不可知的，非参数方法不会带来潜在的假设。</p><h2 id="代码实践">代码实践</h2><p>作者在GitHub上开源了他的代码 <a href="https://github.com/bazingagin/npc_gzip">npc_gzip</a> .我们先把代码拉到本地 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/bazingagin/npc_gzip<br></code></pre></td></tr></table></figure> 接下来安装依赖项，有条件的话创建一个虚拟环境 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ./npc_gzip<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure></p><p>安装完了之后运行<code>main_text.py</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python main_text.py<br></code></pre></td></tr></table></figure></p><p>注意，如果你遇到了这个问题 <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">Traceback (most recent <span class="hljs-keyword">call</span> last):<br>  File &quot;main_text.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">2</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> data <span class="hljs-keyword">import</span> *<br>  File &quot;/home/npc_gzip/data.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">12</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/datasets/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">43</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> .arrow_dataset <span class="hljs-keyword">import</span> Dataset<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/datasets/arrow_dataset.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">59</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> HfApi, HfFolder<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/huggingface_hub/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">322</span>, <span class="hljs-keyword">in</span> __getattr__<br>    submod = importlib.import_module(submod_path)<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/importlib/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">127</span>, <span class="hljs-keyword">in</span> import_module<br>    <span class="hljs-keyword">return</span> _bootstrap._gcd_import(<span class="hljs-type">name</span>[<span class="hljs-keyword">level</span>:], package, <span class="hljs-keyword">level</span>)<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/huggingface_hub/hf_api.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">32</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">import</span> requests<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/requests/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">43</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">import</span> urllib3<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/urllib3/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">42</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    &quot;urllib3 v2.0 only supports OpenSSL 1.1.1+, currently &quot;<br>ImportError: urllib3 v2<span class="hljs-number">.0</span> <span class="hljs-keyword">only</span> supports OpenSSL <span class="hljs-number">1.1</span><span class="hljs-number">.1</span>+, currently the <span class="hljs-string">&#x27;ssl&#x27;</span> module <span class="hljs-keyword">is</span> compiled <span class="hljs-keyword">with</span> <span class="hljs-string">&#x27;OpenSSL 1.0.2u  20 Dec 2019&#x27;</span>. See: https://github.com/urllib3/urllib3/issues/<span class="hljs-number">2168</span><br></code></pre></td></tr></table></figure></p><p>urllib3 v2.0（您安装的版本）需要 OpenSSL 1.1.1+ 才能正常工作，因为它依赖于 OpenSSL 1.1 的一些新功能.</p><p>安装旧版本即可解决 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install urllib3==1.26.6 <br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;npc_gzip笔记&quot;&gt;npc_gzip笔记&lt;/h1&gt;
&lt;h2 id=&quot;论文笔记&quot;&gt;论文笔记&lt;/h2&gt;
&lt;p&gt;npc_gzip 的论文名叫做 &quot;Low-Resource&quot; Text Classification: A Parameter-Free Classifi</summary>
      
    
    
    
    <category term="自然语言处理" scheme="https://studyinglover.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>python调用c++函数</title>
    <link href="https://studyinglover.com/2023/07/15/python%E8%B0%83%E7%94%A8c++%E5%87%BD%E6%95%B0/"/>
    <id>https://studyinglover.com/2023/07/15/python%E8%B0%83%E7%94%A8c++%E5%87%BD%E6%95%B0/</id>
    <published>2023-07-15T09:30:00.000Z</published>
    <updated>2023-07-26T16:08:48.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python调用c函数">python调用c++函数</h1><p>当我们需要在Python中使用C++编写的函数时，可以将C++代码编译成共享库文件（.so文件），然后来调用这些函数。这里介绍两种方法。</p><h2 id="使用python的api">使用python的api</h2><p>首先要安装安装<code>python-dev</code> 和<code>cmake</code></p><p>在Archlinux下<code>yay python-dev</code> ，<code>yay cmake</code>即可。其他平台需要自行搜索</p><p>首先创建一个C++文件 <code>main.cpp</code> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;Python.h&gt;</span></span><br>  <br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span> </span>&#123;<br><span class="hljs-keyword">return</span> a + b;<br>&#125;<br>  <br><span class="hljs-function"><span class="hljs-type">static</span> PyObject* <span class="hljs-title">py_add</span><span class="hljs-params">(PyObject* self, PyObject* args)</span> </span>&#123;<br><span class="hljs-type">int</span> a, b;<br><span class="hljs-keyword">if</span> (!<span class="hljs-built_in">PyArg_ParseTuple</span>(args, <span class="hljs-string">&quot;ii&quot;</span>, &amp;a, &amp;b)) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;<br>&#125;<br><span class="hljs-type">int</span> result = <span class="hljs-built_in">add</span>(a, b);<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">PyLong_FromLong</span>(result);<br>&#125;<br>  <br><span class="hljs-type">static</span> PyMethodDef module_methods[] = &#123;<br>&#123;<span class="hljs-string">&quot;add&quot;</span>, py_add, METH_VARARGS, <span class="hljs-string">&quot;Add two integers.&quot;</span>&#125;,<br>&#123;<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, <span class="hljs-literal">NULL</span>&#125;<br>&#125;;<br>  <br><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> <span class="hljs-title class_">PyModuleDef</span> module_def = &#123;<br>PyModuleDef_HEAD_INIT,<br><span class="hljs-string">&quot;my_module&quot;</span>,<br><span class="hljs-string">&quot;My custom module.&quot;</span>,<br><span class="hljs-number">-1</span>,<br>module_methods<br>&#125;;<br>  <br><span class="hljs-function">PyMODINIT_FUNC <span class="hljs-title">PyInit_my_module</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span> </span>&#123;<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">PyModule_Create</span>(&amp;module_def);<br>&#125;<br></code></pre></td></tr></table></figure></p><p>接着用cmake构建<code>.so</code>文件，<code>CMakeLists.txt</code> 内容如下 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs CMakeLists">cmake_minimum_required(VERSION 3.0)<br>  <br>project(my_module)<br>  <br>find_package(Python REQUIRED COMPONENTS Interpreter Development)<br>  <br>add_library(my_module SHARED main.cpp)<br>  <br>target_include_directories(my_module PRIVATE $&#123;Python_INCLUDE_DIRS&#125;)<br>target_link_libraries(my_module PRIVATE $&#123;Python_LIBRARIES&#125;)<br>  <br>set_target_properties(my_module PROPERTIES PREFIX &quot;&quot;)<br>set_target_properties(my_module PROPERTIES SUFFIX &quot;.so&quot;)<br></code></pre></td></tr></table></figure></p><p>构建完成后会有一个名为<code>my_module.so</code> 的文件</p><p>接下来使用python调用,注意将python文件和<code>my_module.so</code> 放到同一个目录下 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> my_module<br>  <br>result = my_module.add(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure></p><h2 id="按照-c-语言的规则来编译和链接">按照 C 语言的规则来编译和链接</h2><p>首先，我们需要编写一个C++文件<code>mylib.cpp</code> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">extern</span> <span class="hljs-string">&quot;C&quot;</span> <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> a + b;<br>&#125;<br></code></pre></td></tr></table></figure></p><p>接下来，编译<code>mylib.cpp</code> 为一个<code>.so</code>文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">g++ -shared -o mylib.so -fPIC mylib.cpp<br></code></pre></td></tr></table></figure></p><p>最后使用python加载<code>mylib.so</code> 文件并调用 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> ctypes<br><br><span class="hljs-comment"># 加载共享库文件</span><br>mylib = ctypes.cdll.LoadLibrary(<span class="hljs-string">&#x27;./mylib.so&#x27;</span>)<br><br>result = mylib.add(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python调用c函数&quot;&gt;python调用c++函数&lt;/h1&gt;
&lt;p&gt;当我们需要在Python中使用C++编写的函数时，可以将C++代码编译成共享库文件（.so文件），然后来调用这些函数。这里介绍两种方法。&lt;/p&gt;
&lt;h2 id=&quot;使用python的api&quot;&gt;使用</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>Filesystem type ntfs3,ntfs not configured in kernel</title>
    <link href="https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/"/>
    <id>https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/</id>
    <published>2023-07-14T09:35:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="filesystem-type-ntfs3ntfs-not-configured-in-kernel">Filesystem type ntfs3,ntfs not configured in kernel</h1><p>昨天卸载硬盘的时候卡住了，然后我就直接拔下了硬盘，再插上就出现了这个问题 <img src="https://cdn.studyinglover.com/pic/2023/07/7da166adca81943084fbc25dae0a3e16.png" alt="image.png" /></p><p>我先用备份恢复了一下，但是重新插上硬盘问题依然存在。接下来google了一下，Archwiki中有提到<a href="https://wiki.archlinux.org/title/NTFS">这个问题</a>，但是标记这个问题是已经过时的，所描述的问题已得到解决。从内核版本6.2开始，ntfs3支持<code>windows_names</code>选项。我就先按照文档说的做了，但是问题依然没有解决。 <img src="https://cdn.studyinglover.com/pic/2023/07/92f0be4c455602d2eda6b9ecd6229969.png" alt="image.png" /></p><p>接下来翻了下reddit，发现有人存在类似的问题 https://www.reddit.com/r/archlinux/comments/s3w6uu/cannot_mount_ntfs_drives_on_516/ ， 有人提到需要安装<code>ntfs-3g</code> ,那么就是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yay ntfs-3g<br></code></pre></td></tr></table></figure></p><p>问题解决</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;filesystem-type-ntfs3ntfs-not-configured-in-kernel&quot;&gt;Filesystem type ntfs3,ntfs not configured in kernel&lt;/h1&gt;
&lt;p&gt;昨天卸载硬盘的时候卡住了，然后我就直接拔</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>open_clip编码图像和文本</title>
    <link href="https://studyinglover.com/2023/07/13/open_clip%E7%BC%96%E7%A0%81%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC/"/>
    <id>https://studyinglover.com/2023/07/13/open_clip%E7%BC%96%E7%A0%81%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC/</id>
    <published>2023-07-13T23:14:00.000Z</published>
    <updated>2023-07-26T16:08:48.551Z</updated>
    
    <content type="html"><![CDATA[<p>open_clip是CLIP的开源实现版本，只训练了CLIP效果最好的几个模型。</p><p>安装是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install open_clip_torch<br></code></pre></td></tr></table></figure></p><p>首先导入 open_clip，并创建相关模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> open_clip<br><span class="hljs-keyword">import</span> torch<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>clip_model_name = <span class="hljs-string">&quot;ViT-L-14&quot;</span><br>clip_model,_,clip_preprocess = open_clip.create_model_and_transforms(clip_model_name<br>clip_model_name,pretrained = <span class="hljs-string">&quot;openai&quot;</span>,precision=<span class="hljs-string">&#x27;fp16&#x27;</span> <span class="hljs-keyword">if</span> device == <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;fp32&#x27;</span>,device=device,<br>)<br><br>tokenize = open_clip.get_tokenizer(clip_model_name)<br></code></pre></td></tr></table></figure></p><p><code>tokenize</code> 是分词器，所有的文本都要先经过分析器才能放入模型进行推理。</p><h4 id="编码图像">编码图像</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">image_to_features</span>(<span class="hljs-params">image: Image.Image</span>) -&gt; torch.Tensor:<br>images = clip_preprocess(image).unsqueeze(<span class="hljs-number">0</span>).to(device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>image_features = clip_model.encode_image(images)<br><span class="hljs-keyword">return</span> image_features<br>  <br>img = cv.imread(<span class="hljs-string">&quot;/path/to/example.png&quot;</span>)<br>img = Image.fromarray(img)<br><br>image_feature = image_to_features(img)<br></code></pre></td></tr></table></figure><p><code>/path/to/example.png</code> 替换成自己图片的路径</p><p><code>image_to_features</code> 函数是一个封装过的将图像转成文本的函数，传入的参数是一个<code>image_to_features</code>格式的图片。</p><p><code>image_feature</code> 就是经过CLIP的编码器得到的特征</p><h4 id="编码文本">编码文本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;a photo of a cat&quot;</span><br>text_tokens = tokenize([prompt]).to(device)<br>text_features = clip_model.encode_text(text_tokens)<br></code></pre></td></tr></table></figure><p><code>text_features</code> 就是得到的特征。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;open_clip是CLIP的开源实现版本，只训练了CLIP效果最好的几个模型。&lt;/p&gt;
&lt;p&gt;安装是 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;</summary>
      
    
    
    
    <category term="多模态" scheme="https://studyinglover.com/categories/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    
  </entry>
  
  <entry>
    <title>PicGo配置CloudflareR2图片储存</title>
    <link href="https://studyinglover.com/2023/07/09/PicGo%E9%85%8D%E7%BD%AECloudflareR2%E5%9B%BE%E7%89%87%E5%82%A8%E5%AD%98/"/>
    <id>https://studyinglover.com/2023/07/09/PicGo%E9%85%8D%E7%BD%AECloudflareR2%E5%9B%BE%E7%89%87%E5%82%A8%E5%AD%98/</id>
    <published>2023-07-09T20:24:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="picgo配置cloudflarer2图片储存">PicGo配置CloudflareR2图片储存</h1><p>首先需要安装PicGo,并购买CloudFlare R2。CloudFlare R2选择免费计划即可，只是需要绑定银行卡或者paypal(淘宝两块钱解君忧)。</p><p>在R2的管理界面选择管理R2 API Tokens， <img src="https://cdn.studyinglover.com/pic/2023/07/c5fa048794dc5eab45d6e83efef1df8e.png" alt="image.png" /></p><p>创建一个API Token <img src="https://cdn.studyinglover.com/pic/2023/07/eed2d1b23fb75a7abc5ac334688baba7.png" alt="image.png" /> 注意选择权限为edit <img src="https://cdn.studyinglover.com/pic/2023/07/0a7ece4445c1a2f190adc2dd82351f62.png" alt="image.png" /></p><p>创建API Token之后，保存Access Key ID和Secret Access Key。</p><p>接下来返回R2的管理界面，创建一个储存桶 <img src="https://cdn.studyinglover.com/pic/2023/07/6de3892cb5f8a0bf1c53bb83d2070ca6.png" alt="image.png" /> 填入名字并创建桶，点击进入储存桶的管理界面，进入setting界面。 <img src="https://cdn.studyinglover.com/pic/2023/07/674ad9e98a4d4c064cd135353f967fce.png" alt="image.png" /></p><p>自定义自己的域名并允许公开访问，选择Connect Domain绑定到自己的域名，选择AllowAccess允许公开访问。 <img src="https://cdn.studyinglover.com/pic/2023/07/135bb11e6b475ed4d7acdf491003cf52.png" alt="image.png" /></p><p>接下来打开PicGo,安装s3插件 <img src="https://cdn.studyinglover.com/pic/2023/07/bc0d82dee02bc1a2b114477b827b125c.png" alt="image.png" /></p><p>应用密钥ID和应用密钥填入在API Token获取的Access Key ID和Secret Access Key，桶名填入创建的桶的名称，自定义节点填入储存桶管理界面中途中对应的路径。自定义域名填入前面绑定的自己的域名。 <img src="https://cdn.studyinglover.com/pic/2023/07/0c0cc997c92cd807ecb48c3b2b08e394.png" alt="image.png" /></p><p>尝试上传不出意外就上传成功了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;picgo配置cloudflarer2图片储存&quot;&gt;PicGo配置CloudflareR2图片储存&lt;/h1&gt;
&lt;p&gt;首先需要安装PicGo,并购买CloudFlare R2。CloudFlare R2选择免费计划即可，只是需要绑定银行卡或者paypal(淘宝两块钱解</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>ArchlinuxGnome快捷键打开终端</title>
    <link href="https://studyinglover.com/2023/06/28/ArchlinuxGnome%E9%85%8D%E7%BD%AE%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF/"/>
    <id>https://studyinglover.com/2023/06/28/ArchlinuxGnome%E9%85%8D%E7%BD%AE%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF/</id>
    <published>2023-06-28T19:30:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<p>网上大量教程说命令打开终端的命令是<code>gnome-terminal</code> ， 然而</p><figure><img src="https://proxy.thisis.plus/202306281925975.png" alt="" /><figcaption>image.png</figcaption></figure><p>经过一番搜索，我发现 https://www.omglinux.com/gnome-console-tab-overview/ <img src="https://proxy.thisis.plus/202306281927089.png" alt="image.png" /></p><p>emmmmm,意思是终端命令是<code>kgx</code> ?</p><p>果然，又被一些教程坑了 <img src="https://proxy.thisis.plus/202306281928138.png" alt="image.png" /></p><p>最后配置如图 <img src="https://proxy.thisis.plus/202306211810384.png" alt="image.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网上大量教程说命令打开终端的命令是&lt;code&gt;gnome-terminal&lt;/code&gt; ， 然而&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://proxy.thisis.plus/202306281925975.png&quot; alt=&quot;&quot; /&gt;&lt;figcapt</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
    <category term="ArchLinux" scheme="https://studyinglover.com/tags/ArchLinux/"/>
    
  </entry>
  
  <entry>
    <title>clip-interrogator代码解析</title>
    <link href="https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    <id>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</id>
    <published>2023-06-23T22:59:40.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="clip-interrogator代码解析">clip-interrogator代码解析</h1><p>clip-interrogator 的的主要代码在仓库的<code>./clip-interrogator</code> 文件夹下 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">.<br>├── clip_interrogator<span class="hljs-selector-class">.py</span><br>├── data<br>│   ├── artists<span class="hljs-selector-class">.txt</span><br>│   ├── flavors<span class="hljs-selector-class">.txt</span><br>│   ├── mediums<span class="hljs-selector-class">.txt</span><br>│   ├── movements<span class="hljs-selector-class">.txt</span><br>│   └── negative<span class="hljs-selector-class">.txt</span><br>└── __init__<span class="hljs-selector-class">.py</span><br><br></code></pre></td></tr></table></figure></p><p>这里主要解析<code>clip-interrogator.py</code> 文件。</p><h2 id="init.py"><strong>init</strong>.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> .clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator, LabelTable, list_caption_models, list_clip_models, load_list<br><br>__version__ = <span class="hljs-string">&#x27;0.6.0&#x27;</span><br>__author__ = <span class="hljs-string">&#x27;pharmapsychotic&#x27;</span><br></code></pre></td></tr></table></figure><p>这个 <code>__init__.py</code> 文件的作用是在包被导入时执行初始化操作，并提供了版本号和作者信息。</p><h2 id="clip_interrogator.py">clip_interrogator.py</h2><p>文件的大致结构是这样的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> 需要的库<br><br>CAPTION_MODELS = &#123;<br><span class="hljs-string">&#x27;blip-base&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-base&#x27;</span>, <span class="hljs-comment"># 990MB</span><br><span class="hljs-string">&#x27;blip-large&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-large&#x27;</span>, <span class="hljs-comment"># 1.9GB</span><br><span class="hljs-string">&#x27;blip2-2.7b&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-opt-2.7b&#x27;</span>, <span class="hljs-comment"># 15.5GB</span><br><span class="hljs-string">&#x27;blip2-flan-t5-xl&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-flan-t5-xl&#x27;</span>, <span class="hljs-comment"># 15.77GB</span><br><span class="hljs-string">&#x27;git-large-coco&#x27;</span>: <span class="hljs-string">&#x27;microsoft/git-large-coco&#x27;</span>, <span class="hljs-comment"># 1.58GB</span><br>&#125;<br><br>CACHE_URL_BASE = <span class="hljs-string">&#x27;https://huggingface.co/pharma/ci-preprocess/resolve/main/&#x27;</span><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Config</span>:<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Interrogator</span>():<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LabelTable</span>():<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_download_file</span>(<span class="hljs-params">url: <span class="hljs-built_in">str</span>, filepath: <span class="hljs-built_in">str</span>, chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">4</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>, quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge_tables</span>(<span class="hljs-params">tables: <span class="hljs-type">List</span>[LabelTable], ci: Interrogator</span>) -&gt; LabelTable:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prompt_at_max_len</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_truncate_to_fit</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">str</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_caption_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_clip_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_list</span>(<span class="hljs-params">data_path: <span class="hljs-built_in">str</span>, filename: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br></code></pre></td></tr></table></figure></p><p><code>CAPTION_MODELS</code> 定义了各个所需要的模型在huggingface 地址。<code>CACHE_URL_BASE</code> 是缓存地址</p><h3 id="config-class">Config class</h3><p>首先定义了CLIP和BILP模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">caption_model = <span class="hljs-literal">None</span><br>caption_processor = <span class="hljs-literal">None</span><br>clip_model = <span class="hljs-literal">None</span><br>clip_preprocess = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure></p><p>接下来对BLIP和CLIP进行了详细的设置2 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># blip settings</span><br>caption_max_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span><br>caption_model_name: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-string">&#x27;blip-large&#x27;</span> <span class="hljs-comment"># use a key from CAPTION_MODELS or None</span><br>caption_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br>  <br><span class="hljs-comment"># clip settings</span><br>clip_model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;ViT-L-14/openai&#x27;</span><br>clip_model_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span><br>clip_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure></p><p>这段代码是Config类中与Interrogator类相关的配置参数。</p><p>接下来定义了interrogator的相关设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">cache_path: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;cache&#x27;</span> <span class="hljs-comment"># 存储缓存的文本嵌入的路径</span><br>download_cache: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span> <span class="hljs-comment"># 是否从huggingface下载缓存的嵌入向量</span><br>chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span> <span class="hljs-comment"># CLIP的批处理大小</span><br>data_path: <span class="hljs-built_in">str</span> = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data&#x27;</span>)<span class="hljs-comment"># 数据文件的路径</span><br>device: <span class="hljs-built_in">str</span> = (<span class="hljs-string">&quot;mps&quot;</span> <span class="hljs-keyword">if</span> torch.backends.mps.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>flavor_intermediate_count: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span><br>quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span> <span class="hljs-comment"># 是否显示进度条</span><br></code></pre></td></tr></table></figure></p><p><code>apply_low_vram_defaults</code>方法，用于将配置参数设置为适合低显存设备的默认值。在该方法中，将一些参数设置为较小的值，以减少显存的使用。</p><h3 id="interrogator-class">Interrogator class</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Config</span>):<br>self.config = config<br>self.device = config.device<br>self.dtype = torch.float16 <span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">else</span> torch.float32<br>self.caption_offloaded = <span class="hljs-literal">True</span><br>self.clip_offloaded = <span class="hljs-literal">True</span><br>self.load_caption_model()<br>self.load_clip_model()<br></code></pre></td></tr></table></figure><p>继承了<code>Config</code> 类中的一些配置。</p><h4 id="load_caption_model">load_caption_model</h4><p>这个方法用于加载图像描述模型。首先判断配置中是否直接传入了图像描述模型对象，并且是否指定了图像描述模型名称。如果没有直接传入模型对象并且指定了模型名称，则根据模型名称加载对应的模型。加载过程中根据模型名称的不同选择不同的加载方式。加载完成后，将模型设置为eval模式，并根据配置决定是否将模型移动到指定的设备上</p><h4 id="load_clip_model">load_clip_model</h4><p>这个方法用于加载CLIP模型。首先根据配置中指定的CLIP模型名称解析出模型名称和预训练模型名称。然后判断配置中是否直接传入了CLIP模型对象。如果没有直接传入模型对象，则根据模型名称和预训练模型名称加载模型。加载过程中会调用<code>open_clip.create_model_and_transforms()</code>方法创建模型和预处理函数，并设置模型为eval模式。加载完成后，将模型和预处理函数保存到对应的属性中。</p><p>接下来，根据配置中的数据路径加载一些标签数据，并创建<code>LabelTable</code>对象。<code>LabelTable</code>类用于管理标签和对应的嵌入向量。这里创建了artists、flavors、mediums、movements、trendings和negative等LabelTable对象。</p><p>最后，打印加载CLIP模型和数据所花费的时间。</p><h4 id="chain">chain</h4><p>这个方法用于它用于在一组短语中选择最佳的短语，以构建一个完整的提示。</p><p>首先调用_prepare_clip()方法，准备CLIP模型。</p><p>然后，将短语列表转换为一个集合，方便操作。如果没有指定最佳提示，则通过调用rank_top()方法选择当前短语列表中与图像特征最相似的短语作为最佳提示，并计算其相似度。然后从短语集合中移除最佳提示。</p><p>接下来，使用curr_prompt和curr_sim变量保存当前的提示和相似度。</p><p>定义了一个名为check的内部函数，用于检查给定的附加短语是否应该成为当前提示的一部分。该函数会根据相似度比较结果更新最佳提示和最佳相似度，并判断是否需要更新当前提示。</p><p>使用一个循环遍历max_count次，每次迭代中选择当前短语列表中与当前提示加上附加短语后最相似的短语作为最佳短语。然后将该短语的一部分（从curr_prompt的长度加2开始）作为附加短语。调用check()函数进行相似度比较和更新。</p><p>在循环过程中，如果当前提示已经达到了最大长度，则停止迭代。最后，返回最佳提示。</p><h4 id="generate_caption">generate_caption</h4><p>使用BILP生成图像的描述。它首先对图像进行预处理，然后使用图像描述模型生成描述的tokens，最后将tokens解码为文本描述。</p><h4 id="image_to_features">image_to_features</h4><p>使用CLIP的图像编码器将图片转换成torch格式的特征</p><h4 id="interrogate">interrogate</h4><p><code>interrogate_classic</code> 首先生成一个标准格式的提示，描述图像，然后列出艺术家、趋势、风格和口味等文本修饰符。它使用了mediums、artists、trendings、movements和flavors等LabelTable对象来选择相应的修饰符。</p><p><code>interrogate_fast</code> 在生成的描述后面简单地添加排名靠前的词语。它通常比经典模式产生更好的生成提示和图像之间的相似度，但提示的可读性较差。它使用了artists、flavors、mediums、movements和trendings等LabelTable对象来选择排名靠前的词语。</p><p><code>interrogate_negative</code> 主要生成负面词汇，将与图像最不相似的词语连接在一起。它可以用于构建与正面提示相对应的负面提示，并且通常可以改善生成图像的结果，特别是在使用稳定扩散2（Stable Diffusion 2）时。它使用了flavors和negative等LabelTable对象来选择最不相似的词语。</p><p><code>interrogate</code> 会生成一个完整的提示。首先生成一个基于图像的描述，然后根据图像特征和LabelTable对象生成一组修饰符。然后使用chain方法选择最佳的修饰符，并根据相似度和一些条件选择最佳提示。最后，根据生成的多个提示的相似度，选择最终的生成提示。</p><h4 id="prepare_caption">_prepare_caption</h4><p>用于加载BLIP模型。</p><h4 id="prepare_clip">_prepare_clip</h4><p>用于加载CLIP模型。</p><h4 id="rank_top">rank_top</h4><p>这个方法用于对文本进行排名，并返回排名最高的文本。</p><p>首先加载CLIP模型。使用tokenize方法将文本数组转换为文本tokens，并将其移动到设备上。</p><p>然后，使用<code>clip_model</code>的<code>encode_text</code>方法对文本tokens进行编码，得到文本的特征向量。对特征向量进行归一化处理，使其长度为1。接着，计算文本特征向量与图像特征向量之间的相似度。通过计算特征向量的点积得到相似度。如果<code>reverse</code>为<code>True</code>，则将相似度取负，以实现按相似度降序排列。最后，返回排名最高的文本，即相似度最大的文本。</p><h4 id="similarity和similarities">similarity和similarities</h4><p>通过计算点积的方式计算了相似度</p><h3 id="labeltable-class">LabelTable class</h3><p>这个类创建标签，并对标签进行排名</p><h4 id="init"><strong>init</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, labels:<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], desc:<span class="hljs-built_in">str</span>, ci: Interrogator</span>):<br>clip_model, config = ci.clip_model, ci.config<br>self.chunk_size = config.chunk_size<br>self.config = config<br>self.device = config.device<br>self.embeds = []<br>self.labels = labels<br>self.tokenize = ci.tokenize<br>  <br><span class="hljs-built_in">hash</span> = hashlib.sha256(<span class="hljs-string">&quot;,&quot;</span>.join(labels).encode()).hexdigest()<br>sanitized_name = self.config.clip_model_name.replace(<span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>).replace(<span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>)<br>self._load_cached(desc, <span class="hljs-built_in">hash</span>, sanitized_name)<br>  <br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.labels) != <span class="hljs-built_in">len</span>(self.embeds):<br>self.embeds = []<br>chunks = np.array_split(self.labels, <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(self.labels)/config.chunk_size))<br><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> tqdm(chunks, desc=<span class="hljs-string">f&quot;Preprocessing <span class="hljs-subst">&#123;desc&#125;</span>&quot;</span> <span class="hljs-keyword">if</span> desc <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>, disable=self.config.quiet):<br>text_tokens = self.tokenize(chunk).to(self.device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>text_features = clip_model.encode_text(text_tokens)<br>text_features /= text_features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>text_features = text_features.half().cpu().numpy()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(text_features.shape[<span class="hljs-number">0</span>]):<br>self.embeds.append(text_features[i])<br>  <br><span class="hljs-keyword">if</span> desc <span class="hljs-keyword">and</span> self.config.cache_path:<br>os.makedirs(self.config.cache_path, exist_ok=<span class="hljs-literal">True</span>)<br>cache_filepath = os.path.join(self.config.cache_path, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sanitized_name&#125;</span>_<span class="hljs-subst">&#123;desc&#125;</span>.safetensors&quot;</span>)<br>tensors = &#123;<br><span class="hljs-string">&quot;embeds&quot;</span>: np.stack(self.embeds),<br><span class="hljs-string">&quot;hash&quot;</span>: np.array([<span class="hljs-built_in">ord</span>(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">hash</span>], dtype=np.int8)<br>&#125;<br>save_file(tensors, cache_filepath)<br>  <br><span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cpu&#x27;</span> <span class="hljs-keyword">or</span> self.device == torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>):<br>self.embeds = [e.astype(np.float32) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> self.embeds]<br></code></pre></td></tr></table></figure><p>继承了<code>Interrogator</code> 中的一些内容，同时对embeds 做了预处理。</p><h4 id="load_cached">_load_cached</h4><p>用于加载缓存的嵌入向量。</p><h4 id="rank和rank">_rank和rank</h4><p>用于对图像特征和文本嵌入向量进行排名。<code>_rank</code>方法计算图像特征与文本嵌入向量之间的相似度，并返回排名最高的文本索引。<code>rank</code>方法根据<code>chunk_size</code>的大小，将文本嵌入向量分成多个批次进行排名，然后返回排名最高的文本标签。</p><h2 id="data">data</h2><p>存储了常用的文字生成图片的prompt</p><h2 id="clip-interrogator究竟做了什么">clip-interrogator究竟做了什么</h2><p>首先，clip-interrogator会使用BILP生成一段对图片的自然语言描述。</p><p>接下来会根据四种模式，从data文件夹下的txt文件中组合出文字生成图片常用的prompt,通过CLIP进行编码，然后将图片也用CLIP进行编码，计算出相似度最大的一组prompt,和BILP生成的prompt拼接到一起，就得到了一组prompt。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;clip-interrogator代码解析&quot;&gt;clip-interrogator代码解析&lt;/h1&gt;
&lt;p&gt;clip-interrogator 的的主要代码在仓库的&lt;code&gt;./clip-interrogator&lt;/code&gt; 文件夹下 &lt;figure class</summary>
      
    
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>GroundingDINO安装报错解决</title>
    <link href="https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/"/>
    <id>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</id>
    <published>2023-06-21T17:25:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="groundingdino安装报错解决">GroundingDINO安装报错解决</h1><p>在安装会遇到这个错误 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs bash">  ERROR: Command errored out with <span class="hljs-built_in">exit</span> status 1:<br>   <span class="hljs-built_in">command</span>: /usr/bin/python3 /tmp/tmpmhvo4wyp build_wheel /tmp/tmp3a4xwmi4<br>       cwd: /tmp/pip-install-x0mg8qpf/pycocotools<br>  Complete output (77 lines):<br>  running bdist_wheel<br>  running build<br>  running build_py<br>  creating build<br>  creating build/lib.linux-x86_64-cpython-38<br>  creating build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/coco.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/mask.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/cocoeval.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/__init__.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  running build_ext<br>  cythoning pycocotools/_mask.pyx to pycocotools/_mask.c<br>  building <span class="hljs-string">&#x27;pycocotools._mask&#x27;</span> extension<br>  creating build/temp.linux-x86_64-cpython-38<br>  creating build/temp.linux-x86_64-cpython-38/common<br>  creating build/temp.linux-x86_64-cpython-38/pycocotools<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c ./common/maskApi.c -o build/temp.linux-x86_64-cpython-38/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToBbox’:<br>  ./common/maskApi.c:151:32: warning: unused variable ‘xp’ [-Wunused-variable]<br>    151 |     uint h, w, xs, ys, xe, ye, xp, cc; siz j, m;<br>        |                                ^~<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrPoly’:<br>  ./common/maskApi.c:197:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |   ^~~<br>  ./common/maskApi.c:197:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |                                                      ^<br>  ./common/maskApi.c:198:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |   ^~~<br>  ./common/maskApi.c:198:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |                                                      ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToString’:<br>  ./common/maskApi.c:243:7: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |       ^~<br>  ./common/maskApi.c:243:27: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |                           ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrString’:<br>  ./common/maskApi.c:251:3: warning: this ‘<span class="hljs-keyword">while</span>’ clause does not guard... [-Wmisleading-indentation]<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |   ^~~~~<br>  ./common/maskApi.c:251:22: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">while</span>’<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |                      ^~~~<br>  ./common/maskApi.c:259:5: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |     ^~<br>  ./common/maskApi.c:259:34: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |                                  ^~~~<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-cpython-38/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99<br>  pycocotools/_mask.c:6:10: fatal error: Python.h: No such file or directory<br>      6 | <span class="hljs-comment">#include &quot;Python.h&quot;</span><br>        |          ^~~~~~~~~~<br>  compilation terminated.<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/setuptools/dist.py:745: SetuptoolsDeprecationWarning: Invalid dash-separated options<br>  !!<br>  <br>          ********************************************************************************<br>          Usage of dash-separated <span class="hljs-string">&#x27;index-url&#x27;</span> will not be supported <span class="hljs-keyword">in</span> future<br>          versions. Please use the underscore name <span class="hljs-string">&#x27;index_url&#x27;</span> instead.<br>  <br>          By 2023-Sep-26, you need to update your project and remove deprecated calls<br>          or your builds will no longer be supported.<br>  <br>          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html <span class="hljs-keyword">for</span> details.<br>          ********************************************************************************<br>  <br>  !!<br>    opt = self.warn_dash_deprecation(opt, section)<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive <span class="hljs-string">&#x27;language_level&#x27;</span> not <span class="hljs-built_in">set</span>, using 2 <span class="hljs-keyword">for</span> now (Py2). This will change <span class="hljs-keyword">in</span> a later release! File: /tmp/pip-install-x0mg8qpf/pycocotools/pycocotools/_mask.pyx<br>    tree = Parsing.p_module(s, pxd, full_module_name)<br>  error: <span class="hljs-built_in">command</span> <span class="hljs-string">&#x27;/usr/bin/x86_64-linux-gnu-gcc&#x27;</span> failed with <span class="hljs-built_in">exit</span> code 1<br>  ----------------------------------------<br>  ERROR: Failed building wheel <span class="hljs-keyword">for</span> pycocotools<br>Failed to build pycocotools<br>ERROR: Could not build wheels <span class="hljs-keyword">for</span> pycocotools <span class="hljs-built_in">which</span> use PEP 517 and cannot be installed directly<br></code></pre></td></tr></table></figure> 细读报错，我们会发现是编译过程中少了一个<code>Python.h</code> 的头文件导致编译pycocotools失败。</p><p>我们尝试直接安装<code>pycocotools</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install pycocotools<br></code></pre></td></tr></table></figure><p>会出现和上面一样的错误。</p><p>google一番,提示说<code>sudo apt-get install libsuitesparse-dev</code></p><p>受到报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) ... error<br> error: subprocess-exited-with-error<br> <br> × Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) did not run successfully.<br> │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">1</span><br> ╰─&gt; [<span class="hljs-number">77</span> lines of output]<br></code></pre></td></tr></table></figure></p><p>最后的结果依然是 <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs subunit">  note: This error originates from a subprocess, and is likely not a problem with pip.<br>  ERROR: Failed building wheel for pycocotools<br>Failed to build pycocotools<br><span class="hljs-keyword">ERROR: </span>Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects<br></code></pre></td></tr></table></figure></p><p>尝试通过安装<code>pip install "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI"</code> 解决</p><p>获得报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs awk">fatal: unable to access <span class="hljs-string">&#x27;https://github.com/philferriere/cocoapi.git/&#x27;</span>: GnuTLS recv error (-<span class="hljs-number">110</span>): The TLS connection was non-properly terminated.<br>  error: subprocess-exited-with-error<br>  <br>  × git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>  │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>  ╰─&gt; See above <span class="hljs-keyword">for</span> output.<br>  <br>  note: This error originates from a subprocess, and is likely not a problem with pip.<br>error: subprocess-exited-with-error<br><br>× git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>│ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>╰─&gt; See above <span class="hljs-keyword">for</span> output.<br></code></pre></td></tr></table></figure></p><p>运行<code>sudo apt install python3.8-dev</code></p><p>然后<code>git clone https://github.com/cocodataset/cocoapi.git</code> , <code>cd ./cocoapi/PythonAPI</code> ,接下来 <code>make</code></p><p>运行<code>pip install -e .</code> ,成功安装<code>pycocotools</code> .</p><p>再次运行<code>pip install GroundingDINO</code> , 成功。</p><figure><img src="https://proxy.thisis.plus/202306211724652.png" alt="" /><figcaption>image.png</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;groundingdino安装报错解决&quot;&gt;GroundingDINO安装报错解决&lt;/h1&gt;
&lt;p&gt;在安装会遇到这个错误 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span </summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</title>
    <link href="https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/"/>
    <id>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/</id>
    <published>2023-06-19T23:06:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评">2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</h1><h2 id="鲲鹏活动">鲲鹏活动</h2><p>我是白吃白喝来的你真以为我是来学技术的？</p><h3 id="上午场">上午场</h3><p><img src="https://proxy.thisis.plus/202306190655993.jpg" /></p><p>院士发言，讲了从教多年开设公共课的历程，还有将核心技术掌握在自己手里的重要性，举了上世纪欧洲软件和20年哈工大哈工程被禁用matlab的例子。 <img src="https://proxy.thisis.plus/202306190655529.jpg" /></p><p>然后我就牙疼的受不了看牙去了……</p><h3 id="下午场">下午场</h3><p>下午场有三部分，星享会，鲲鹏训练营还有人才发展论坛。星享会是关于互联网+产业命题赛道和鲲鹏应用创新大赛的分享。鲲鹏训练营没有参与，我猜是用类似华为云的沙盒做实验。人才发展论坛是大佬们发言讲自己做的一些研究和人才培养模式 <img src="https://proxy.thisis.plus/202306190703495.jpg" alt="IMG_20230617_135653.jpg" /></p><figure><img src="https://proxy.thisis.plus/202306190704505.jpg" alt="" /><figcaption>IMG_20230617_173651.jpg</figcaption></figure><figure><img src="https://proxy.thisis.plus/202306190704514.jpg" alt="" /><figcaption>IMG_20230617_165145.jpg</figcaption></figure><h2 id="零食午饭测评">零食午饭测评</h2><p>因为牙疼刚做了根管的缘故没有吃的太全，所以只能聊一聊自己吃了的部分</p><h3 id="午饭">午饭</h3><p><img src="https://proxy.thisis.plus/202306190642829.jpg" /></p><p>左边粥是皮蛋瘦肉粥，绝对好评，好喝还适合我这种牙刚做了手术的人，我喝了三碗。</p><p>右边的甜点里我们最远的那一排左边的是蒸饺，正常。右边的不知道叫什么，夹心，正常水平。</p><p>中间的一排虽然长得不一样，都是千层饼。有一点咸味，有点硬不适合那天的牙。</p><p>离我们最近的一排最左边的小蛋糕，我吃了两个，第一个没啥味道，第二个有苦味。中间的豆沙。最右边的，流心绿豆糕，非常好吃，很软口感很好，很适合我的牙，吃了六个吧。</p><h3 id="零食">零食</h3><p><img src="https://proxy.thisis.plus/202306190642391.jpg" /></p><p>右边的饮料据工作人员说是他们自己调的，气泡莫吉托，挺好喝的，有碳酸饮料的感觉 <img src="https://proxy.thisis.plus/202306190642335.jpg" /></p><p>左边盘子里左上是芒果蛋糕，右上草莓蛋糕。芒果蛋糕整个是芒果和奶油，草莓蛋糕正常。左下不好吃，很硬没啥味道。右下核桃芯还是很好吃的。</p><h2 id="收获">收获</h2><p>一本基于鲲鹏的大数据挖掘de书，两个水杯，一个肩带，一个文化衫</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&quot;&gt;2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&lt;/h1&gt;
&lt;h2 id=&quot;鲲鹏活动&quot;&gt;鲲鹏活动&lt;/h2&gt;
&lt;p&gt;我是白吃白喝来的你真以为我是来学技术的？&lt;/p&gt;
&lt;h3 id=&quot;上午场&quot;&gt;上午场&lt;/h3</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>RoboMaster开源仓库汇总(长期更新)</title>
    <link href="https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/"/>
    <id>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/</id>
    <published>2023-06-18T22:40:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h2 id="视觉">视觉</h2><h3 id="陈君">陈君</h3><p><a href="https://gitlab.com/rm_vision">陈君视觉1</a></p><p><a href="https://github.com/rm-vision-archive">陈君视觉2</a></p><h3 id="沈航">沈航</h3><ol type="1"><li><a href="https://github.com/tup-robomaster/TUP-NN-Train-2">TUP-NN-Train-2:项目结构修改，增加几种新网络结构</a></li><li><a href="https://github.com/tup-robomaster/TUP2023-Sentry-Framework">TUP2023-Sentry-Framework:哨兵框架开源，包括全向感知，决策，导航 提供VIO和LIO方案</a></li><li><a href="https://github.com/tup-robomaster/RM_Radar2023">RM_Radar2023:沈阳航空航天大学2023年雷达程序</a></li><li><a href="https://github.com/tup-robomaster/TRTInferenceForYolo">TRTInferenceForYoloX:YOLOXTensorRT推理</a></li></ol><h3 id="西浦">西浦</h3><p><a href="https://github.com/zRzRzRzRzRzRzR/YOLO-of-RoboMaster-Keypoints-Detection-2023">四点识别模型</a></p><h2 id="工具">工具</h2><p><a href="http://shenyibo.me/RM-labeling-tool/">装甲板数据集制作</a></p><p><a href="https://github.com/tup-robomaster/AutoLabel">AutoLabel:自动标注pipeline，集成角点坐标分布分析,图像去重等工具</a></p><p><a href="https://github.com/ifr-cv/rm_part_visual_tag_identify">RoboMaster视觉标签识别器</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;视觉&quot;&gt;视觉&lt;/h2&gt;
&lt;h3 id=&quot;陈君&quot;&gt;陈君&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://gitlab.com/rm_vision&quot;&gt;陈君视觉1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/rm-vision-arc</summary>
      
    
    
    
    
    <category term="RoboMaster" scheme="https://studyinglover.com/tags/RoboMaster/"/>
    
  </entry>
  
  <entry>
    <title>没有手都可以在腾讯云创建镜像</title>
    <link href="https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
    <id>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</id>
    <published>2023-06-16T21:15:00.000Z</published>
    <updated>2023-07-26T16:08:48.551Z</updated>
    
    <content type="html"><![CDATA[<p>腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。</p><h2 id="创建">创建</h2><p>首先登陆腾讯云的账号，进入控制台界面</p><figure><img src="https://proxy.thisis.plus/202306162104428.png" alt="" /><figcaption>image.png</figcaption></figure><p>选择实例的的更多选项 <img src="https://proxy.thisis.plus/202306162106168.png" alt="image.png" /></p><p>选择制作镜像 <img src="https://proxy.thisis.plus/202306162107031.png" alt="image.png" /></p><p>在弹出的窗口填入镜像名称，标签和备注即可 <img src="https://proxy.thisis.plus/202306162108164.png" alt="image.png" /></p><p>选择制作镜像后就会进入制作界面，稍等片刻我们就可以看到制作的镜像了 <img src="https://proxy.thisis.plus/202306162111218.png" alt="image.png" /></p><h2 id="应用">应用</h2><p>想要使用创建好的实例也很简单，在左侧选择镜像 <img src="https://proxy.thisis.plus/202306162113093.png" alt="image.png" /></p><p>然后点击创建镜像即可 <img src="https://proxy.thisis.plus/202306162113912.png" alt="image.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。&lt;/p&gt;
&lt;h2 id=&quot;创建&quot;&gt;创建&lt;/h2&gt;
&lt;p&gt;首先登陆腾讯云的账号，进入控制台界面&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;ht</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>Arch下PicGo不能从剪切板上传图片</title>
    <link href="https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/"/>
    <id>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</id>
    <published>2023-06-13T00:12:40.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错</p><figure><img src="https://proxy.thisis.plus/202306162118377.png" alt="" /><figcaption>image.png</figcaption></figure><p>解决方法很简单啦，进入PicGo设置，打开最下面的使用内置剪贴板上传。 <img src="https://proxy.thisis.plus/202306162122805.png" alt="image.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://proxy.thisis.plus/202306162118377.p</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>LoRA 笔记</title>
    <link href="https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/</id>
    <published>2023-06-13T00:12:40.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lora-笔记">LoRA 笔记</h1><p>自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="LoRA: Low-Rank Adaptation of Large Language Models. (n.d.).">[1]</span></a></sup>冻结预训练模型权重并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数的数量减少了 10,000 倍，GPU 内存需求减少了 3 倍。</p><h2 id="什么是low-rank">什么是low-rank</h2><p>首先需要明确一些什么什么是矩阵的秩，rank</p><p>在国内的本科线性代数课程中我们是这样定义矩阵的秩的</p><blockquote><p>设在矩阵<span class="math inline">\(A\)</span> 中有一个有一个不等于<span class="math inline">\(0\)</span> 的<span class="math inline">\(r\)</span> 阶子式<span class="math inline">\(D\)</span> ,且所有<span class="math inline">\(r+1\)</span> 阶子式(如果存在的话)都等于<span class="math inline">\(0\)</span> ，那么<span class="math inline">\(D\)</span> 称为矩阵<span class="math inline">\(A\)</span> 的最高阶非零子式，数<span class="math inline">\(r\)</span> 成为矩阵的秩，记为<span class="math inline">\(R(A)\)</span> 。并规定零矩阵的秩为0。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="_同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解_. (2015).">[2]</span></a></sup></p></blockquote><p>怎么求矩阵的秩呢，很简单啦就是把一个矩阵化成RREF(课本上管这个叫行最简行矩阵)然后数一下每一行第一个非零元素所在列为单位向量的个数就可以了。</p><p>好的，发生了什么？好像并没有解释清楚秩到底是什么。</p><p>实际上啊，秩反映了矩阵里列向量线性相关的程度，意思就是你矩阵里的那几个向量能“支”出来几维，假如说我有一个矩阵里面有五个向量，但是他的矩阵秩是3,这就说明五个向量只能撑起一个3维空间，剩下两个向量可以被三个不能被互相表示的向量表示(课本上管这个叫线性相关和线性无关)，用李宏毅的话说就是这里有两个向量在"耍废"。</p><blockquote><p>推荐一下3Blue1Brown的视频https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0，线性代数讲的很清楚。</p></blockquote><p>该清楚了秩是什么，低秩是什么就很好理解了，就是有个矩阵他的秩很低，小于矩阵里面向量的个数(向量组线性相关/有向量在"耍废")。</p><p>你可能会想问，LoRA作为一个微调大语言模型和图文大模型的方法，关矩阵的秩什么事？在2020年，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568">[3]</span></a></sup> 指出大模型的训练实际发生在low-rank空间上的,所以说我们只需要构造一个低秩空间下的训练方法就可以了。</p><h2 id="为什么需要lora">为什么需要LoRA</h2><p>LoRA并不是第一个进行微调大模型的，从迁移学习开始有很多的尝试，以语言建模为例，在有效适应方面有两种突出的策略：添加适配器层或优化某种形式的输入层激活。然而，这两种策略都有其局限性，尤其是在大规模和延迟敏感的生产场景中。 ### 添加适配器层(引入推理延迟) 适配层(Adapter) 实际上就是在原本的架构上添加一些层，让他学到新的东西。例如<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning.">[4]</span></a></sup> <img src="https://proxy.thisis.plus/202306132022661.png" /> 左侧为每个 Transformer 层添加适配器模块两次：在多头注意力的投影和两个前馈层之后。右侧适配器由一个瓶颈组成，该瓶颈包含相对于原始模型中的注意力层和前馈层的参数很少。适配器还包含跳过连接。在适配器调整期间，绿色层在下游数据上进行训练，这包括适配器、层归一化参数和最终分类层（图中未显示）。</p><p>虽然可以通过修剪层或利用多任务设置来减少整体延迟，但没有直接的方法绕过适配器层中的额外计算。在单个 GPU 上对 GPT-2介质运行推理，我们看到在使用适配器时延迟显着增加，即使瓶颈维度非常小。</p><h3 id="优化某种形式的输入层激活很难进行">优化某种形式的输入层激活(很难进行)</h3><p>作者观察到前缀调整很难优化，并且它的性能在可训练参数中非单调地变化，证实了原始论文中的类似观察结果。更根本的是，保留序列长度的一部分进行适应必然会降低可用于处理下游任务的序列长度，所以作者怀疑与其他方法相比，调整提示的性能较低。</p><h2 id="lora到底怎么工作">LoRA到底怎么工作</h2><p>神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。对于预训练的权重矩阵 <span class="math inline">\(W_0 ∈ R^{d×k}\)</span>，我们通过使用低秩分解 <span class="math inline">\(W_0 + ΔW = W_0 + BA\)</span> 表示后者来约束其更新，其中 <span class="math inline">\(B ∈ R^{d×r} , A ∈ R^{r×k}\)</span>，秩<span class="math inline">\(r\)</span> 为 <span class="math inline">\(min(d, k)\)</span>。在训练期间，<span class="math inline">\(W_0\)</span> 被冻结并且不接收梯度更新，而 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 包含可训练的参数。注意 <span class="math inline">\(W_0\)</span> 和 <span class="math inline">\(ΔW = BA\)</span> 都乘以相同的输入，它们各自的输出向量按坐标求和。对于 <span class="math inline">\(h = W_0x\)</span>，我们修改后的前向传递产生：<span class="math display">\[h=W_0x+\Delta Wx=W_0x+BAx\]</span> 参数初始化时，我们对 A 使用随机高斯初始化，B 使用零，因此 ΔW = BA 在训练开始时为零。所以 <span class="math inline">\(\Delta W = BA\)</span> 在训练开始时为零.用<span class="math inline">\(\frac{\alpha}{r}\)</span> 缩放 <span class="math inline">\(ΔWx\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是 <span class="math inline">\(r\)</span> 中的一个常数。在使用 Adam 进行优化时，如果我们适当地缩放初始化，调整 <span class="math inline">\(\alpha\)</span> 与调整学习率大致相同。因此，我们只需将 <span class="math inline">\(\alpha\)</span> 设置为我们尝试的第一个 r，而不对其进行调整。当我们改变时，这种缩放有助于减少重新调整超参数的需要</p><p>这种微调方式有两个好处</p><ol type="1"><li>完全泛化的微调方式</li><li>不会引入推理延迟</li></ol><p>在推理的时候，只需要把<span class="math inline">\(B\)</span>和<span class="math inline">\(A\)</span> 两个矩阵乘起来然后加回到原先的参数矩阵就完成了参数的更新</p><p><img src="https://proxy.thisis.plus/202306132038132.png" /></p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>LoRA: Low-Rank Adaptation of Large Language Models. (n.d.). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><em>同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解</em>. (2015). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;lora-笔记&quot;&gt;LoRA 笔记&lt;/h1&gt;
&lt;p&gt;自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA&lt;sup id=&quot;fnref:1&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>Diffusers去除NSFW限制</title>
    <link href="https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/"/>
    <id>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/</id>
    <published>2023-06-11T00:02:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，<del>涩涩是文字生成图片技术发展的重大推动力</del> . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(<strong>not safe for work</strong>)的内容，<del>这就给我们涩涩带来了不必要的麻烦</del>。所以我将介绍如何去除限制</p><p>该方法来自网友，<a href="https://www.reddit.com/r/StableDiffusion/comments/wxba44/disable_hugging_face_nsfw_filter_in_three_step/">原链接</a></p><p>先给一段示例代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure><p>我们只需要设置<code>StableDiffusionPipeline</code> 这个类的<code>safety_checker</code>函数，更改之后的代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dummy</span>(<span class="hljs-params">images, **kwargs</span>): <br><span class="hljs-keyword">return</span> images, <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>pipe.safety_checker = dummy<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure></p><p>成功实现<del>涩涩自由</del></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;众所周知，&lt;del&gt;涩涩是文字生成图片技术发展的重大推动力&lt;/del&gt; . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(&lt;strong&gt;not safe for work&lt;/st</summary>
      
    
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>StableDiffusion笔记</title>
    <link href="https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/</id>
    <published>2023-05-29T15:36:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<p>Stable Diffusion 是一个图像生成方法，由 <em><a href="https://stability.ai/">Stability AI</a> and <a href="https://runwayml.com/">Runway</a></em> 在LDM<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[1]</span></a></sup> 的基础上提出。在GitHub有很多他的实现和应用<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="AUTOMATIC1111. (n.d.). _GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI_. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui">[4]</span></a></sup> ,其中<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup> 是最早的实现版本，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup> 是V2版本，由 Stability AI 完成。</p><h2 id="整体结构">整体结构</h2><pre><code class=" mermaid">flowchart TDsubgraph Input-noisyRandom-seed --&gt; latent-Gaussian-noise endsubgraph Input-promptprompt --&gt; TextEncoder --&gt; TextEmbaddingsendlatent-Gaussian-noise --&gt;Unet&#123;Unet-with-MultiAttention&#125;TextEmbaddings--&gt;UnetUnet --&gt; predict-noisy --sampling-steps--&gt;Unetpredict-noisy --&gt; Decoder --&gt; Image </code></pre><p>在一开始，StableDiffusion会通过一个随机数种子生成一张在隐空间下的随机噪声，同时通过一个文本编码器对输入的prompt进行编码，生成一个文本向量。随机噪声和文本向量会一块送入Unet，经过DDPM的步骤得到一张隐空间下的图片，通过一个解码器得到完整的图片。这里的Unet做出了改进，中间加入了交叉注意力机制。</p><h3 id="unet-with-multiattention">Unet-with-MultiAttention</h3><p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IRTbG2rYv0IUH8HHAxWRrQ.png" alt="Unet-with-MultiAttention 图源medium.com" /> 图中Switch用于在不同的输入之间调整。</p><ul><li>文本数据通过一个文本编码器(一般是CLIP的文本编码器)将文本转换为向量，投影到Unet上</li><li>图像，语义图，表示等直接送入Unet</li></ul><p>反向扩散过程中输入的文本向量和隐空间下的噪声图片需要经过 <span class="math inline">\(t\)</span>轮的Unet网络，每一轮预测一个噪声，噪声图减去这个噪声，得到的图片继续送入Unet进行下一轮</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>CompVis. (n.d.). <em>GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model</em>. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Stability-AI. (n.d.). <em>GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models</em>. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>AUTOMATIC1111. (n.d.). <em>GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI</em>. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Steins. (2023, January 2). Stable diffusion clearly explained! - Steins. <em>Medium</em>. https://medium.com/<span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e"><span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e">@steinsfu/stable-diffusion-clearly-explained-ed008044e07e</span></span> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Stable Diffusion 是一个图像生成方法，由 &lt;em&gt;&lt;a href=&quot;https://stability.ai/&quot;&gt;Stability AI&lt;/a&gt; and &lt;a href=&quot;https://runwayml.com/&quot;&gt;Runway&lt;/a&gt;&lt;/em&gt; 在LD</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>huggingface 和相关库</title>
    <link href="https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/"/>
    <id>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/</id>
    <published>2023-05-09T12:35:00.000Z</published>
    <updated>2023-07-26T16:08:48.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="huggingface-和相关库">huggingface 和相关库</h1><h2 id="huggingface">huggingface</h2><p><a href="https://huggingface.co/">Hugging Face</a>是一个专注于自然语言处理（NLP）的开源平台，它旨在让NLP变得更加易用和普及。Hugging Face推出了多个库，例如Transformers，Datasets，Tokenizers和Accelerate，它们分别提供了预训练的模型，大规模的数据集，高效的分词器和分布式训练的工具。Hugging Face还拥有一个活跃的社区，其中有数千名研究人员，开发者和爱好者共同交流和贡献NLP的最新进展。 <img src="https://proxy.thisis.plus/202305092233241.png" alt="image.png" /></p><p>Hugging Face的名字来源于一个可爱的表情符号，它代表了平台的愿景：让人类和机器之间的交流更加自然和亲密。Hugging Face的核心产品是Transformers库，它包含了超过10000个预训练的模型，涵盖了各种NLP任务，如文本分类，问答，文本生成，情感分析等。Transformers库支持多种深度学习框架，如PyTorch，TensorFlow，JAX和Flax，并且可以轻松地在不同的设备上运行，如CPU，GPU和TPU。Hugging Face还提供了一个在线平台，Spaces，它可以让用户快速地部署和分享他们的模型和应用。 <img src="https://proxy.thisis.plus/202305092235498.png" alt="image.png" /></p><p>近年来，Hugging Face托管的模型已经不局限于NLP领域，而是涉及到了更多的领域，如计算机视觉（CV），语音识别（ASR），音乐生成（MG）等。这些模型都可以在Hugging Face的网站上找到，并且可以通过Transformers库或者其他的库来使用。Hugging Face还提供了一个数据集库，叫做Datasets，它包含了超过1000个数据集，覆盖了各种领域和语言。Datasets库可以帮助用户快速地加载，处理和缓存数据，以及进行数据分析和可视化。</p><h2 id="accelerate">Accelerate</h2><p>Accelerate 是一个可以让训练变得更加简单的库，它可以通过几行代码来在分布式设备上运行相同的pytorch代码</p><p>可以通过pypi 和 conda安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install accelerate<br>conda install -c conda-forge accelerate<br></code></pre></td></tr></table></figure></p><p>你可能会遇到这种报错 <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure></p><p>这里的依赖已经安装成功了，只是被安装到了未被添加到PATH的目录，接下来运行的时候只需要指明目录即可。例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p><p>通过<code>accelerate config</code> 命令可以配置当前文件夹启用。(如果啥都不知道就全部选No)</p><h2 id="transformers">Transformers</h2><p><a href="https://huggingface.co/docs/transformers/index">Transformers</a> 收集了所有的SOTA的NLP研究方法，并提供了对应的预训练模型和接口。Transformers 支持 PyTorch、TensorFlow 和 JAX 之间的框架互操作性。这提供了在模型生命周期的每个阶段使用不同框架的灵活性；在一个框架中用三行代码训练一个模型，然后将其加载到另一个框架中进行推理。模型还可以导出为 ONNX 和 TorchScript 等格式，以便在生产环境中部署。</p><p>安装非常简单 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers[torch]&#x27;</span><br></code></pre></td></tr></table></figure> 这回安装torch对应的api，当然也可以安装完整版 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers&#x27;</span> <br></code></pre></td></tr></table></figure></p><p>21年冬天在家上网课的时候，我看到了这样的一个教程<a href="https://zhuanlan.zhihu.com/p/421642560">这篇文章是我用AI生成出来的</a> ,他就是使用了transformers 库构建了一个生成式语言模型。</p><h2 id="diffusers">Diffusers</h2><p><a href="https://huggingface.co/docs/diffusers/index">Diffusers</a> 收集了所有SOTA的扩散模型，用于生成图像、音频，甚至分子的 3D 结构。diffusers提供了扩散模型的完整pipeline ，包括DDPM，DDIM，stable_diffusion_2，VAE，controlnet等等，可以使用简单的几行代码完成推理。</p><p>安装和上面一样 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers[<span class="hljs-string">&quot;torch&quot;</span>]<br></code></pre></td></tr></table></figure> 或者 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers <br></code></pre></td></tr></table></figure></p><h3 id="pipeline">pipeline</h3><p>pipeline 是diffusers 甚至huggingface各个库的一个重要概念，他封装了各个模型加载权重，构建网络结构，推理和训练的全部过程。</p><p>这里以stable diffusion 1.5为例，首先创建pipeline，并指明stable diffusion 的版本 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DiffusionPipeline<br><br>model_id = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br>pipeline = DiffusionPipeline.from_pretrained(model_id)<br></code></pre></td></tr></table></figure></p><p>接下来给出提示(prompt) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;portrait photo of a old warrior chief&quot;</span><br></code></pre></td></tr></table></figure></p><p>为了加速推理，我们可以把数据放到gpu上 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>设置生成器，并生成图像 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><p>当然，huggingface推荐我们在float16上做推理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)<br>pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br>generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><h3 id="lora">LoRA</h3><p><a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation of Large Language Models (LoRA)</a>是一种训练方法，可以加速大型模型的训练，同时消耗更少的内存，最有用的例子莫过于生成人脸了。Diffusers 现在支持使用 LoRA 进行<a href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-lora">文本到图像生成</a>和<a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora">DreamBooth</a>微调。</p><p><a href="https://arxiv.org/abs/2208.12242">DreamBooth</a>是Google提出的微调技术，用于个性化文本到图像模型（如 Stable Diffusion），可以以在给定几张主题图像的情况下生成不同背景下主题的逼真图像。</p><p>在<a href="https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py">这里</a> 你可以找到完整的代码，在<a href="https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ">Google Drive</a> 下载完整的图像用于训练</p><p>先设置基本信息，分别是模型名，示例图片和模型输出文件夹 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MODEL_NAME=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br><span class="hljs-built_in">export</span> INSTANCE_DIR=<span class="hljs-string">&quot;path-to-instance-images&quot;</span><br><span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;path-to-save-model&quot;</span><br></code></pre></td></tr></table></figure></p><p>接下来运行代码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate launch train_dreambooth_lora.py \<br>  --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_NAME</span>  \<br>  --instance_data_dir=<span class="hljs-variable">$INSTANCE_DIR</span> \<br>  --output_dir=<span class="hljs-variable">$OUTPUT_DIR</span> \<br>  --instance_prompt=<span class="hljs-string">&quot;a photo of sks dog&quot;</span> \<br>  --resolution=512 \<br>  --train_batch_size=1 \<br>  --gradient_accumulation_steps=1 \<br>  --checkpointing_steps=100 \<br>  --learning_rate=1e-4 \<br>  --report_to=<span class="hljs-string">&quot;wandb&quot;</span> \<br>  --lr_scheduler=<span class="hljs-string">&quot;constant&quot;</span> \<br>  --lr_warmup_steps=0 \<br>  --max_train_steps=500 \<br>  --validation_prompt=<span class="hljs-string">&quot;A photo of sks dog in a bucket&quot;</span> \<br>  --validation_epochs=50 \<br>  --seed=<span class="hljs-string">&quot;0&quot;</span> \<br>  --push_to_hub<br></code></pre></td></tr></table></figure></p><p>推理也是使用起来很简单的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><br>pipe.unet.load_attn_procs(lora_model_path)<br>pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>image = pipe(<span class="hljs-string">&quot;A picture of a sks dog in a bucket.&quot;</span>, num_inference_steps=<span class="hljs-number">25</span>, guidance_scale=<span class="hljs-number">7.5</span>).images[<span class="hljs-number">0</span>]<br>image.save(<span class="hljs-string">&quot;bucket-dog.png&quot;</span>)<br></code></pre></td></tr></table></figure></p><h3 id="controlnet">[[ControlNet]]</h3><p>可以看之前的文章<a href="https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/">ControlNet训练自己数据集</a></p><h2 id="gradio">Gradio</h2><p>gradio 是一个可以快速构建交互式网页的工具，Webui就是用它做出来的，使用他的核心代码就是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">demo = gradio.Interface(fn, inputs, outputs, ···)<br>demo.launch()<br></code></pre></td></tr></table></figure> 传入一个函数和参数，获取返回值</p><p>剩下的就是你写好fn，设计一个好看的界面，然后launch就可以了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;huggingface-和相关库&quot;&gt;huggingface 和相关库&lt;/h1&gt;
&lt;h2 id=&quot;huggingface&quot;&gt;huggingface&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;是</summary>
      
    
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
</feed>
