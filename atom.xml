<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>plus studio</title>
  
  
  <link href="https://studyinglover.com/atom.xml" rel="self"/>
  
  <link href="https://studyinglover.com/"/>
  <updated>2023-09-07T07:50:45.743Z</updated>
  <id>https://studyinglover.com/</id>
  
  <author>
    <name>StudyingLover</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>m2cgen生成机器学习c语言推理代码</title>
    <link href="https://studyinglover.com/2023/09/07/m2cgen%E7%94%9F%E6%88%90c%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81/"/>
    <id>https://studyinglover.com/2023/09/07/m2cgen%E7%94%9F%E6%88%90c%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81/</id>
    <published>2023-09-07T15:48:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="m2cgen生成机器学习c语言推理代码">m2cgen生成机器学习c语言推理代码</h1><p>众所周知，cubemx是一个用于生成嵌入式的代码的好东西虽然我没用过。它的原理是将原本的矩阵运算和tensor变成了一个c的数组，同时会对代码进行优化，然后进行运算。</p><p>但是如果我们需要在其他平台上使用其他语言就很尴尬了，因为我们没有cubemx来做生成和优化。感谢蓬勃发展的社区，<a href="https://github.com/BayesWitnesses/m2cgen">m2cgen</a>解决了我们的问题。</p><p>使用起来非常简单，我们使用xgboost举例，先训练一个xgboost模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_diabetes<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> linear_model<br><br>X, y = load_diabetes(return_X_y=<span class="hljs-literal">True</span>)<br><br>estimator = linear_model.LinearRegression()<br>estimator.fit(X, y)<br></code></pre></td></tr></table></figure></p><p>然后导出c代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> m2cgen <span class="hljs-keyword">as</span> m2c<br>code = m2c.export_to_c(estimator)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span> (<span class="hljs-string">&#x27;model.c&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>   f.write(code)<br></code></pre></td></tr></table></figure></p><p>我们可以看到导出的代码已经是纯c语言的代码了，是以一个函数保存的 <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">double</span> <span class="hljs-title function_">score</span><span class="hljs-params">(<span class="hljs-type">double</span> * input)</span> &#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">152.13348416289597</span> + input[<span class="hljs-number">0</span>] * <span class="hljs-number">-10.009866299810508</span> + input[<span class="hljs-number">1</span>] * <span class="hljs-number">-239.81564367242302</span> + input[<span class="hljs-number">2</span>] * <span class="hljs-number">519.845920054461</span> + input[<span class="hljs-number">3</span>] * <span class="hljs-number">324.38464550232334</span> + input[<span class="hljs-number">4</span>] * <span class="hljs-number">-792.1756385522302</span> + input[<span class="hljs-number">5</span>] * <span class="hljs-number">476.73902100525737</span> + input[<span class="hljs-number">6</span>] * <span class="hljs-number">101.04326793803405</span> + input[<span class="hljs-number">7</span>] * <span class="hljs-number">177.06323767134606</span> + input[<span class="hljs-number">8</span>] * <span class="hljs-number">751.2736995571034</span> + input[<span class="hljs-number">9</span>] * <span class="hljs-number">67.62669218370456</span>;<br>&#125;<br></code></pre></td></tr></table></figure></p><p>如果你遇到了这样的一个错误 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">base_score = -math.log(1.0 / self._base_score - 1.0)<br>                           ~~~~^~~~~~~~~~~~~~~~~~<br>TypeError: unsupported operand <span class="hljs-built_in">type</span>(s) <span class="hljs-keyword">for</span> /: <span class="hljs-string">&#x27;float&#x27;</span> and <span class="hljs-string">&#x27;NoneType&#x27;</span><br></code></pre></td></tr></table></figure> 这是由于xgboost模型字段发生变化导致的，在<code>m2c.export_to_c</code>之前加入<code>model.base_score = 0</code> 就行 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> m2cgen <span class="hljs-keyword">as</span> m2c<br>model.base_score = <span class="hljs-number">0</span><br>code = m2c.export_to_c(estimator)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span> (<span class="hljs-string">&#x27;model.c&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>   f.write(code)<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;m2cgen生成机器学习c语言推理代码&quot;&gt;m2cgen生成机器学习c语言推理代码&lt;/h1&gt;
&lt;p&gt;众所周知，cubemx是一个用于生成嵌入式的代码的好东西虽然我没用过。它的原理是将原本的矩阵运算和tensor变成了一个c的数组，同时会对代码进行优化，然后进行运算。</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
    <category term="机器学习" scheme="https://studyinglover.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>xgboost模型序列化存储并推理</title>
    <link href="https://studyinglover.com/2023/09/07/xgboost%E6%A8%A1%E5%9E%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%AD%98%E5%82%A8%E5%B9%B6%E6%8E%A8%E7%90%86/"/>
    <id>https://studyinglover.com/2023/09/07/xgboost%E6%A8%A1%E5%9E%8B%E5%BA%8F%E5%88%97%E5%8C%96%E5%AD%98%E5%82%A8%E5%B9%B6%E6%8E%A8%E7%90%86/</id>
    <published>2023-09-07T15:03:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="xgboost模型序列化存储并推理">xgboost模型序列化存储并推理</h1><p>参考了博客 https://github.com/apachecn/ml-mastery-zh/blob/master/docs/xgboost/save-gradient-boosting-models-xgboost-python.md ，但是修改了一些过时的部分。</p><p>我们在 <a href="https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes">Pima 印第安人糖尿病数据集</a> 上训练xgboost模型，训练数据集在<a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv">GitHub</a> 下载 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv<br></code></pre></td></tr></table></figure></p><h2 id="pickle">Pickle</h2><p>Pickle是一个python序列化的标准方法。</p><p>先训练一个模型,然后将模型按照Pickle的形式存储，接下来读取模型并进行推理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> loadtxt<br><span class="hljs-keyword">import</span> xgboost<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection <span class="hljs-keyword">as</span> cross_validation<br><span class="hljs-comment"># load data</span><br>dataset = loadtxt(<span class="hljs-string">&#x27;pima-indians-diabetes.data.csv&#x27;</span>, delimiter=<span class="hljs-string">&quot;,&quot;</span>)<br><span class="hljs-comment"># split data into X and y</span><br>X = dataset[:,<span class="hljs-number">0</span>:<span class="hljs-number">8</span>]<br>Y = dataset[:,<span class="hljs-number">8</span>]<br><span class="hljs-comment"># split data into train and test sets</span><br>seed = random.randint(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>)<br>test_size = <span class="hljs-number">0.33</span><br><br>X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)<br><span class="hljs-comment"># fit model no training data</span><br>model = xgboost.XGBClassifier()<br>model.fit(X_train, y_train)<br><br><span class="hljs-comment"># save model to file</span><br>pickle.dump(model, <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;pima.pickle.dat&quot;</span>, <span class="hljs-string">&quot;wb&quot;</span>))<br><br></code></pre></td></tr></table></figure></p><p>读取模型并推理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># load model from file</span><br>loaded_model = pickle.load(<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;pima.pickle.dat&quot;</span>, <span class="hljs-string">&quot;rb&quot;</span>))<br><span class="hljs-comment"># train model again</span><br>loaded_model.fit(X_train, y_train)<br><br><span class="hljs-comment"># make predictions for test data</span><br>y_pred = loaded_model.predict(X_test)<br>predictions = [<span class="hljs-built_in">round</span>(value) <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> y_pred]<br><span class="hljs-comment"># evaluate predictions</span><br>accuracy = accuracy_score(y_test, predictions)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %.2f%%&quot;</span> % (accuracy * <span class="hljs-number">100.0</span>))<br></code></pre></td></tr></table></figure></p><h2 id="joblib">joblib</h2><p>Joblib 是一组在 Python 中提供<strong>轻量级流水线</strong>的工具，<strong>joblib 在大型 numpy 数组上通常要快得多</strong></p><p>用法实际上和pickle基本相同。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Train XGBoost model, save to file using joblib, load and make predictions</span><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> loadtxt<br><span class="hljs-keyword">import</span> xgboost<br><span class="hljs-keyword">import</span> joblib<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> model_selection <span class="hljs-keyword">as</span> cross_validation<br><span class="hljs-comment"># load data</span><br>dataset = loadtxt(<span class="hljs-string">&#x27;pima-indians-diabetes.data.csv&#x27;</span>, delimiter=<span class="hljs-string">&quot;,&quot;</span>)<br><span class="hljs-comment"># split data into X and y</span><br>X = dataset[:,<span class="hljs-number">0</span>:<span class="hljs-number">8</span>]<br>Y = dataset[:,<span class="hljs-number">8</span>]<br><span class="hljs-comment"># split data into train and test sets</span><br>seed = random.randint(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>)<br>test_size = <span class="hljs-number">0.33</span><br>X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)<br><span class="hljs-comment"># fit model no training data</span><br>model = xgboost.XGBClassifier()<br>model.fit(X_train, y_train)<br><span class="hljs-comment"># save model to file</span><br>joblib.dump(model, <span class="hljs-string">&quot;pima.joblib.dat&quot;</span>)<br><br></code></pre></td></tr></table></figure></p><p>读取模型并推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># load model from file</span><br>loaded_model = joblib.load(<span class="hljs-string">&quot;pima.joblib.dat&quot;</span>)<br><span class="hljs-comment"># make predictions for test data</span><br>y_pred = loaded_model.predict(X_test)<br>predictions = [<span class="hljs-built_in">round</span>(value) <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> y_pred]<br><span class="hljs-comment"># evaluate predictions</span><br>accuracy = accuracy_score(y_test, predictions)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %.2f%%&quot;</span> % (accuracy * <span class="hljs-number">100.0</span>))<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;xgboost模型序列化存储并推理&quot;&gt;xgboost模型序列化存储并推理&lt;/h1&gt;
&lt;p&gt;参考了博客 https://github.com/apachecn/ml-mastery-zh/blob/master/docs/xgboost/save-gradient-</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
    <category term="机器学习" scheme="https://studyinglover.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>speculative-sampling笔记</title>
    <link href="https://studyinglover.com/2023/09/05/speculative-sampling%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/09/05/speculative-sampling%E7%AC%94%E8%AE%B0/</id>
    <published>2023-09-05T19:40:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="speculative-sampling笔记">speculative-sampling笔记</h1><p>speculative-sampling,投机采样是一种加速llm推理的方法。</p><p>论文<a href="https://arxiv.org/abs/2302.01318">arxiv</a> ,参考博客<a href="https://jaykmody.com/blog/speculative-sampling/">jaykmody.com</a></p><p>这个方法需要用到两个模型，一个小模型，称为 draft model，一个大模型，称为target model。</p><p>speculative-sampling使用了一种直觉，对于一些序列下一个token预测是i很明显的，小模型也可以完成。因此，如果draft model和target model在给定的很明显的序列上的分布之间存在很强的一致性，那么就允许targrt model被调用时一次输出多个token</p><p><img src="https://cdn.studyinglover.com/pic/2023/09/a74b5ced4e8f8945acc8cf6b4fbbdfb7.png" alt="image.png" /> 自回归采样，就是说给一个序列模型预测下一个token。</p><p><img src="https://cdn.studyinglover.com/pic/2023/09/5867fc09bb99e8709725e0813d4ad7cf.png" alt="image.png" /> 对于大模型来说，主要是三个部分拖慢了推理速度，线性层，注意力机制和通信。</p><p>拒绝采样的公式被修改为<span class="math display">\[\min\left(1,\frac{q(\tilde{x}_{n+1}|x_1,\ldots,x_n)}{p(\tilde{x}_{n+1}|x_1,\ldots,x_n)}\right)\]</span> 给定一个序列<span class="math inline">\(x_0,\ldots,x_t\)</span> 和一个<span class="math inline">\(K\)</span> ,用draft model先采样<span class="math inline">\(\tilde{x}_t\sim p(x|,x_1,\ldots,x_n,\tilde{x}_1,\ldots,\tilde{x}_{t-1})\)</span> ，循环<span class="math inline">\(K\)</span>词</p><p>然后并行计算<span class="math inline">\(q(x|,x_1,\ldots,x_n),~q(x|,x_1,\ldots,x_n,\tilde{x}_1),~\ldots,~q(x|,x_1,\ldots,x_n,\tilde{x}_1,\ldots,\tilde{x}_K)\)</span></p><p>采样一个<span class="math inline">\(r\sim U[0,1]\)</span> ,如果<span class="math inline">\(r&lt;\min\left(1,\frac{q(x|x_1,...,x_{n+t-1})}{p(x|x_1,...,x_{n+t-1})}\right)\)</span> 就把<span class="math inline">\({\tilde{x}_t}\)</span> 拼到序列<span class="math inline">\(x_{n+t-1}\)</span> 后面，这里的<span class="math inline">\(n\)</span> 是序列长度。</p><p>如果<span class="math inline">\(\tilde{x}_{n+1}\)</span> 被拒绝了，也就是说<span class="math inline">\(r&gt;\min\left(1,\frac{q(x|x_1,...,x_{n+t-1})}{p(x|x_1,...,x_{n+t-1})}\right)\)</span>,那么就直接按照<span class="math inline">\(x_{n+1}\sim(q(x|x_1,\ldots,x_n)-p(x|x_1,\ldots,x_n))_+\)</span>采样一个<span class="math inline">\(x_{n+1}\)</span></p><p><span class="math inline">\((.)_{+}\)</span> 被定义为<span class="math display">\[(f(x))_+=\frac{\max(0,f(x))}{\sum_x\max(0,f(x))}\]</span> 如果所有的token都被接受了，那就再采样一个拼到序列后面，然后结束。</p><p>使用标准采样方法，如核、top-k 采样和调整温度，可以在应用这种拒绝采样方案之前相应地修改概率。作者观察到整体接受率对使用的确切参数具有鲁棒性。</p><p>因为speculative-sampling没有改变transformer的结构，所以<strong>可以和其他方法结合使用</strong> ,例如量化，multi-query attention。</p><p>在选择draft model方面，可以简单地使用较小版本的目标语言模型作为草稿并获得较高的接受率。从工程和工作流程的角度来看，这也很方便，因为应该首先存在对此类模型的稳健工具来训练目标模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;speculative-sampling笔记&quot;&gt;speculative-sampling笔记&lt;/h1&gt;
&lt;p&gt;speculative-sampling,投机采样是一种加速llm推理的方法。&lt;/p&gt;
&lt;p&gt;论文&lt;a href=&quot;https://arxiv.org/a</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="自然语言处理" scheme="https://studyinglover.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>prompt2model笔记</title>
    <link href="https://studyinglover.com/2023/09/05/prompt2model%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/09/05/prompt2model%E7%AC%94%E8%AE%B0/</id>
    <published>2023-09-05T09:15:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="prompt2model笔记">prompt2model笔记</h1><p>prompt2model是一个通过提示自动生成语言模型的方法</p><p>项目地址<a href="https://github.com/neulab/prompt2model">GitHub</a></p><figure><img src="https://cdn.studyinglover.com/pic/2023/09/b125b68b936c4cce09d451a5f790cb35.png" alt="" /><figcaption>image.png</figcaption></figure><p>模型分为Prompt Parser，Dataset Retriever，Dataset Generator，Model Retriever几个部分</p><h2 id="prompt-parser">Prompt Parser</h2><figure><img src="https://cdn.studyinglover.com/pic/2023/09/b8ddca1c0daa867307144f0a5546230a.png" alt="" /><figcaption>image.png</figcaption></figure><p>作者使用具有上下文学习的 LLM 来分割用户提示，在实验中使用 OpenAI gpt-3.5-turbo-0613。如果提供的指令被识别为英语以外的语言，就使用 DeepL API.2 将其转换为英语</p><h2 id="dataset-retriever">Dataset Retriever</h2><figure><img src="https://cdn.studyinglover.com/pic/2023/09/8acdf0823c2fb3757b694f71e982a0ce.png" alt="" /><figcaption>image.png</figcaption></figure><p>给定一个提示，首先尝试发现现有的手动注释的数据，可以支持用户的任务描述。数据集检索器有几个设计决策：</p><ol type="1"><li>搜索哪些数据集。</li><li>如何索引数据集以供搜索。</li><li>3.用户任务需要哪些数据集列，应该忽略哪些列。 作者选用了 Viswanathan et al. (2023) 的方案，称为DataFinder</li></ol><p>作者利用 DataFinder 训练的双编码器检索器对最相关的数据集进行排名。一旦确定了相关数据集，下一步是确定数据集的哪些列对应于用户指定的输入和期望输出。由于自动为任何数据集诱导正确的模式可能具有挑战性，所以作者采用了 human-inthe-loop 中的方法。将前 k 个数据集（默认情况下 k = 25）呈现给用户，并允许用户要么选择最相关的数据集，要么声明没有一个非常适合他们的任务。然后，要求用户从数据集的模式中识别输入和输出的适当列。</p><h2 id="dataset-generator">Dataset Generator</h2><p><img src="https://cdn.studyinglover.com/pic/2023/09/622f2321a6504a87d67b8866a2c5c0b5.png" alt="image.png" /> 作者使用自动提示工程来生成不同的数据集，使用退火算法对生成的数据集进行排名。自一致性过滤来防止llm生成的伪标签。具体做法是通过选择最频繁的答案为每个唯一输入创建一个共识输出；在平局的情况下，启发式地选择最短的答案。使用了zeno-build做并行。</p><h2 id="model-retriever">Model Retriever</h2><figure><img src="https://cdn.studyinglover.com/pic/2023/09/413a97d48cf68b35b004601c0fbd4446.png" alt="" /><figcaption>image.png</figcaption></figure><p>这是一个检索类问题。作者选择encoder-decoder的架构，但是仍然有非常多的选择，像Salesforce/codet5-base，MaryaAI/opus-mt-ar-en-finetuned-ar-to-en，所以作为一个检索类问题使用用户的指令作为查询，搜索 Hugging Face 上模型的所有文本描述。 <img src="https://cdn.studyinglover.com/pic/2023/09/7f7790be882200d87972f87e06697d8f.png" alt="image.png" /> ，考虑到对模型的描述一般是比较稀疏并且包含大量模板文本，这里作者使用gpt-3.5-turbo生成了模型可能的描述，用 BM25 算法来计算查询模型相似度分数。</p><p>为了模型易部署，作者过滤了大于3gb的所有模型，同时引入了一个直觉，下载量越高的模型效果越好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;prompt2model笔记&quot;&gt;prompt2model笔记&lt;/h1&gt;
&lt;p&gt;prompt2model是一个通过提示自动生成语言模型的方法&lt;/p&gt;
&lt;p&gt;项目地址&lt;a href=&quot;https://github.com/neulab/prompt2model&quot;&gt;Gi</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="自然语言处理" scheme="https://studyinglover.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>RoboTAP笔记</title>
    <link href="https://studyinglover.com/2023/09/01/RoboTAP%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/09/01/RoboTAP%E7%AC%94%E8%AE%B0/</id>
    <published>2023-09-01T12:35:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="robotap笔记">RoboTAP笔记</h1><p>RoboTAP是一种基于点追踪技术的少样本视觉模仿方法，可以实现机器人在多个任务和场景中的精准操作。</p><p>项目主页<a href="https://robotap.github.io/">GitHub</a></p><p>RoboTAP不需要任何特定于任务的训练或神经网络微调。由于TAP的普适性，作者发现添加新任务（包括调整超参数）只需几分钟，这比我们熟悉的任何操纵系统都快几个数量级。作者认为这种能力在大规模自主数据收集和作为解决现实任务的解决方案方面可能非常有用。RoboTAP在需要快速教授视觉运动技能并且可以轻松演示所需行为的情况下最有用。</p><p>RoboTAP存在一些重要的限制。首先，低级控制器是纯视觉的，这排除了复杂的运动规划或力控制行为。其次，目前计算运动计划一次并在没有重新规划的情况下执行它，这可能会导致单个行为失败或环境意外改变。</p><p>作者在论文中指出他有四个贡献</p><ol type="1"><li>在密集跟踪方面制定多任务操作问题</li><li>RoboTAP的具体实现是什么，在哪里以及如何以visual-saliency，temporal-alignment, 和 visual-servoing的形式解决问题</li><li>一个新的密集跟踪数据集，其中包含为RoboTAP任务量身定制的ground-truth人工注释，并在专注于真实世界机器人操作的TAP-Vid基准上进行评估</li><li>描述了RoboTAP在涉及精确多体重排、变形物体和不可逆行动的一系列操作任务中的成功和失败模式的实证结果。</li></ol><figure><img src="https://cdn.studyinglover.com/pic/2023/08/15ff4915dff842e47e91d580d0d0fe5c.png" alt="" /><figcaption>image.png</figcaption></figure><p>RoboTAP方法的核心是利用TAPIR密集地跟踪一组演示，将演示分段，并自动发现每个阶段的活动点集q，该点集覆盖在该动作阶段相关的物体上。然后，我们形成一个可以在机器人上执行的运动计划，其中包括模仿视觉运动和基本的电机原语，例如关闭和打开夹爪的阶段。通过使用TAPIR检测点q，找到最近的演示，显示如何移动这些点，并找到可以用作运动目标的单个附近帧来实现视觉伺服。将目标帧（g）和在线TAPIR检测之间的位移用作经典视觉伺服的运动目标，从而产生出奇异复杂和强健的行为。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;robotap笔记&quot;&gt;RoboTAP笔记&lt;/h1&gt;
&lt;p&gt;RoboTAP是一种基于点追踪技术的少样本视觉模仿方法，可以实现机器人在多个任务和场景中的精准操作。&lt;/p&gt;
&lt;p&gt;项目主页&lt;a href=&quot;https://robotap.github.io/&quot;&gt;GitH</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="多模态" scheme="https://studyinglover.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>自建obsidian同步服务</title>
    <link href="https://studyinglover.com/2023/08/31/%E8%87%AA%E5%BB%BAobsidian%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1/"/>
    <id>https://studyinglover.com/2023/08/31/%E8%87%AA%E5%BB%BAobsidian%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1/</id>
    <published>2023-08-31T15:47:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自建obsidian同步服务">自建obsidian同步服务</h1><p>最近GitHub上有这样一个项目<a href="https://github.com/acheong08/rev-obsidian-sync">rev-obsidian-sync</a> ,他逆向了obsidian的同步服务，使其可以在本地运行。</p><h2 id="服务端">服务端</h2><h3 id="安装">安装</h3><p>首先安装服务端， <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/acheong08/rev-obsidian-sync<br><span class="hljs-built_in">cd</span> obsidian-sync<br>go run cmd/obsidian-sync/main.go<br></code></pre></td></tr></table></figure> go会下载一堆依赖，然后你会在最下面看到这个 <img src="https://cdn.studyinglover.com/pic/2023/08/c41aa8aca8c8033d319317ee2dbc3643.png" alt="image.png" /></p><p>当然你也可以自定义域名，设置环境变量，<code>DOMAIN_NAME</code> 设置域名，<code>ADDR_HTTP</code>设置监听端口，<code>DATA_DIR</code> 设置数据保存的文件夹，<code>SIGNUP_KEY</code> 设置注册的密钥。</p><h3 id="创建用户">创建用户</h3><p>需要新建一个用户给自己 <code>go run cmd/signup/main.go</code> 在命令行按照提示输入邮箱密码。</p><p>或者使用http请求的方式 <figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">curl <span class="hljs-params">--request</span> POST \<br>  <span class="hljs-params">--url</span> https:<span class="hljs-string">//yourdomain.com/user/signup</span> \<br>  <span class="hljs-params">--header</span> &#x27;Content-Type: application/json&#x27; \<br>  <span class="hljs-params">--data</span> &#x27;&#123;<br><span class="hljs-string">&quot;email&quot;</span>: <span class="hljs-string">&quot;example@example.com&quot;</span>,<br><span class="hljs-string">&quot;password&quot;</span>: <span class="hljs-string">&quot;example_password&quot;</span>,<br><span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Example User&quot;</span>,<br><span class="hljs-string">&quot;signup_key&quot;</span>: <span class="hljs-string">&quot;&lt;SIGNUP_KEY&gt;&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure></p><h2 id="客户端">客户端</h2><p>在obsidian仓库打开命令行，然后 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /path/to/vault/.obsidian<br><span class="hljs-built_in">mkdir</span> -p plugins/custom-sync-plugin &amp;&amp; <span class="hljs-built_in">cd</span> plugins/custom-sync-plugin<br>wget https://github.com/acheong08/rev-obsidian-sync-plugin/raw/master/main.js https://github.com/acheong08/rev-obsidian-sync-plugin/raw/master/manifest.json<br></code></pre></td></tr></table></figure> 打开obsidian设置界面，选择第三方插件，启用<code>Custom Native Sync</code> <img src="https://cdn.studyinglover.com/pic/2023/08/0a124be82a4a2fe13b1943ab320c839d.png" alt="image.png" /></p><p>设置服务端地址 <img src="https://cdn.studyinglover.com/pic/2023/08/9b7c177f4b69baed6686fffca3a04df5.png" alt="image.png" /></p><p>同时打开核心插件的同步 <img src="https://cdn.studyinglover.com/pic/2023/08/e3944c723e5b4ea0740f729fdd4a1c73.png" alt="image.png" /> 在点击左侧出现的同步按钮，输入前面设置的账号密码，就可以体验到官方的同步功能了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;自建obsidian同步服务&quot;&gt;自建obsidian同步服务&lt;/h1&gt;
&lt;p&gt;最近GitHub上有这样一个项目&lt;a href=&quot;https://github.com/acheong08/rev-obsidian-sync&quot;&gt;rev-obsidian-sync&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>MediaPipe即将推出图像生成服务</title>
    <link href="https://studyinglover.com/2023/08/23/MediaPipe%E5%8D%B3%E5%B0%86%E6%8E%A8%E5%87%BA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%9C%8D%E5%8A%A1/"/>
    <id>https://studyinglover.com/2023/08/23/MediaPipe%E5%8D%B3%E5%B0%86%E6%8E%A8%E5%87%BA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%9C%8D%E5%8A%A1/</id>
    <published>2023-08-23T20:42:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mediapipe即将推出图像生成服务">MediaPipe即将推出图像生成服务</h1><p>今天我逛GitHub Trending的时候突然发现MediaPipe的示例库被顶到了前排 <img src="https://cdn.studyinglover.com/pic/2023/08/0bc3379fab3273262e8b6f14799b629a.png" alt="image.png" /></p><p>这不对劲，我赶紧去mediapipe的储存库，发现7个小时前Google推送了新的内容 <a href="https://github.com/google/mediapipe/commit/2ebdb01d4326c934e0628e7ff45cadda6575d23f">ImageGenerator Java API</a> <img src="https://cdn.studyinglover.com/pic/2023/08/b744863d78b3347dc0cfb23c7a0cd29d.png" alt="image.png" /></p><p>原来MediaPipe也要推出文字生成图片内容啊，还是移动端设备上的，这让我想起来GitHub最近有人开始写stable-diffusion.cpp，一个使用了ggml量化加速的sd。</p><p>顺藤摸瓜我们可以找到MediaPipe的<a href="https://developers.google.com/mediapipe/solutions/vision/image_generator">文档</a>。 <img src="https://cdn.studyinglover.com/pic/2023/08/6c50982c58e1d65562e230b0bb601d15.png" alt="image.png" /></p><p>还是即将推出状态，但是给了一个简单示例。</p><p>用法超级简单，就是下载下面几个模型中的一个 - <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt">runwayml/stable-diffusion-v1-5</a> - <a href="https://huggingface.co/justinpinkney/miniSD/blob/main/miniSD.ckpt">justinpinkney/miniSD</a> - <a href="https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/models/wd-1-3-penultimate-ucg-cont.ckpt">hakurei/waifu-diffusion-v1-4</a> - <a href="https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model/blob/main/PaperCut_v1.ckpt">Fictiverse/Stable_Diffusion_PaperCut_Model</a></p><p>安装依赖 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install torch typing_extensions numpy Pillow requests pytorch_lightning absl-py<br></code></pre></td></tr></table></figure> 把这个文件copy下来,<a href="https://github.com/googlesamples/mediapipe/blob/main/tools/image_generator_converter/convert.py">地址</a></p><p>然后 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 convert.py --ckpt_path &lt;ckpt_path&gt; --output_path &lt;output_path&gt;<br></code></pre></td></tr></table></figure></p><p>接着将文件夹内容<code>&lt;output_path&gt;</code>推送到 Android 设备。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">adb shell <span class="hljs-built_in">rm</span> -r /data/local/tmp/image_generator/ <br><br>adb shell <span class="hljs-built_in">mkdir</span> -p /data/local/tmp/image_generator/<br><br>adb push &lt;output_path&gt;/. /data/local/tmp/image_generator/bins<br></code></pre></td></tr></table></figure> 安装 Android 演示应用程序,在<a href="https://storage.googleapis.com/mediapipe-tasks/image_generator/imagegenerator.apk">这里</a>下载 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">adb install imagegenerator.apk<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mediapipe即将推出图像生成服务&quot;&gt;MediaPipe即将推出图像生成服务&lt;/h1&gt;
&lt;p&gt;今天我逛GitHub Trending的时候突然发现MediaPipe的示例库被顶到了前排 &lt;img src=&quot;https://cdn.studyinglover.c</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Dual-Stream Diffusion Net for Text-to-Video Generation笔记</title>
    <link href="https://studyinglover.com/2023/08/23/DSDN%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/08/23/DSDN%E7%AC%94%E8%AE%B0/</id>
    <published>2023-08-23T10:32:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dual-stream-diffusion-net-for-text-to-video-generation笔记">Dual-Stream Diffusion Net for Text-to-Video Generation笔记</h1><p>这篇论文提出的模型架构是Dual-Stream Diffusion Net（DSDN），它是一种双流扩散网络。</p><figure><img src="https://cdn.studyinglover.com/pic/2023/08/3021b6624ee4f2093c6166b6a80cd643.png" alt="" /><figcaption>image.png</figcaption></figure><p>首先，视频内容通过一个一个编码器编码成内容特征和一个动作编码器编码成动作特征，并通过一个增量学习模块进行更新。前向扩散过程没有使用DDPM而是使用了 Hierarchical Text-Conditional Image Generation with CLIP Latents 这篇论文提出的方法。</p><p>为了对齐生成的内容和运动，设计了一个双流转换交互模块来通过交叉注意力实现两个分支之间的信息交互和对齐。</p><p>最后引入了运动合成器来简化运动信息的操作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;dual-stream-diffusion-net-for-text-to-video-generation笔记&quot;&gt;Dual-Stream Diffusion Net for Text-to-Video Generation笔记&lt;/h1&gt;
&lt;p&gt;这篇论文提出的模型</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>ViT在DDPM取代UNet(DiT)</title>
    <link href="https://studyinglover.com/2023/08/20/ViT%E5%9C%A8DDPM%E5%8F%96%E4%BB%A3UNet(DiT)/"/>
    <id>https://studyinglover.com/2023/08/20/ViT%E5%9C%A8DDPM%E5%8F%96%E4%BB%A3UNet(DiT)/</id>
    <published>2023-08-20T09:43:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="vit在ddpm取代unetdit">ViT在DDPM取代UNet(DiT)</h1><p><a href="https://www.wpeebles.com/DiT.html">项目主页</a></p><p>这篇论文主要是尝试使用ViT取代DDPM中的UNet，叫做Diffusion Transformer-DiT，作者训练了DiT-S、DiT-B、DiT-L 和 DiT-XL四种模型，每种模型的patch取8,4,2, 一共训练了12个模型。</p><p>作者探索的完整 DiT 设计空间是补丁大小、变压器块架构和模型大小。</p><p>模型第一层是对 sequences of patches 进行操作(就是ViT把图片看成<span class="math inline">\(16*16\)</span>的的单词之后单词构成的序列) 。 <img src="https://cdn.studyinglover.com/pic/2023/08/d9b9a168f177471d890c1bd3e3f2cc2d.png" alt="image.png" /></p><p>如图所示，给定的patch是<span class="math inline">\(p\times p\)</span> ,VAE采样出来的噪声大小是<span class="math inline">\(I\times I\times C\)</span> ,那么patches会变成长度为<span class="math inline">\(T=(I/\hat{p})^{2}\)</span> 的一个序列,每个patch维度是<span class="math inline">\(d\)</span> ,位置嵌入用的是sine-cosine。</p><p>接下来就是diffusion transformers的设计。 <img src="https://cdn.studyinglover.com/pic/2023/08/f68c4f271029a484e97822dbb9fb2569.png" alt="image.png" /></p><p>作者提到了一点，就是获取到path序列之后应该在后面加上去噪步数和类别标签，并在最后一个DiT块之后删掉。</p><p>在最终的 DiT 块之后，需要将输出解码为噪声预测和对角协方差预测。这两个输出的形状都等于整个模型的输入。作者使用标准线性解码器来做到这一点。如果使用 adaLN 自适应就应用最后一层范数，并将每个标记线性解码为 <span class="math inline">\(p\times p\times2C\)</span> 张量，其中 <span class="math inline">\(C\)</span> 是输入到DiT的空间大小。最后，将解码的token重新排列到其原始空间布局中，得到预测的噪声和协方差。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;vit在ddpm取代unetdit&quot;&gt;ViT在DDPM取代UNet(DiT)&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.wpeebles.com/DiT.html&quot;&gt;项目主页&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇论文主要是尝试使用ViT取代DDPM中的UNe</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>arch4edu搞崩了我的flutter</title>
    <link href="https://studyinglover.com/2023/08/19/arch4edu%E6%90%9E%E5%B4%A9%E4%BA%86%E6%88%91%E7%9A%84flutter/"/>
    <id>https://studyinglover.com/2023/08/19/arch4edu%E6%90%9E%E5%B4%A9%E4%BA%86%E6%88%91%E7%9A%84flutter/</id>
    <published>2023-08-19T21:36:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="arch4edu搞崩了我的flutter">arch4edu搞崩了我的flutter</h1><p>今天是快乐的一天，适合滚包 <figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">yay</span><br></code></pre></td></tr></table></figure> 一切安好，arch4edu说我的flutter需要更新 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">==&gt; 要排除的包: (示例: <span class="hljs-string">&quot;1 2 3&quot;</span>, <span class="hljs-string">&quot;1-3&quot;</span>, <span class="hljs-string">&quot;^4&quot;</span> 或软件库名称)<br> -&gt; 排除软件包可能会导致不完整的升级并破坏系统<br>==&gt; <br><br></code></pre></td></tr></table></figure> 没什么需要排除的，接下来就是愉快的自动安装</p><p>突然我看到了这个</p><figure><img src="https://cdn.studyinglover.com/pic/2023/08/d257220b6c5bc01465f92fdd72320344.png" alt="" /><figcaption>image.png</figcaption></figure><p>警告啦，没啥好担心的啦，待会跑一下看好着没</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">flutter doctor                     <br>Found an existing Pub cache at /home/zjh/.pub-cache.<br>It can be repaired by running `dart pub cache repair`.<br>It can be reset by running `dart pub cache clean`.<br>Found an existing Dart Analysis Server cache at /home/zjh/.dartServer.<br>It can be reset by deleting /home/zjh/.dartServer.<br>Flutter failed to write to a file at <span class="hljs-string">&quot;/opt/flutter/packages/flutter_tools/.dart_tool/version&quot;</span>.<br>Please ensure that the SDK and/or project is installed <span class="hljs-keyword">in</span> a location that has <span class="hljs-built_in">read</span>/write<br>permissions <span class="hljs-keyword">for</span> the current user.<br>Try running:<br>  sudo <span class="hljs-built_in">chown</span> -R $(<span class="hljs-built_in">whoami</span>) /opt/flutter/packages/flutter_tools/.dart_tool/version<br><br></code></pre></td></tr></table></figure><p>好的他炸了</p><p>看着问题不大，就是读写权限的问题，的问题？鬼知道会有啥问题，我决定让arch4edu滚蛋</p><p>先<code>sudo pacman -Rns flutter</code>把arch4edu的flutter删掉，然后去<code>/etc/pacman.conf</code> 删除了arch4edu镜像，再<code>sudo pacman -Syu</code>滚一遍包，最后<code>yay flutter</code></p><p>中间会有一个问题 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">错误：无法提交处理 (有冲突的文件)<br>flutter: 文件系统中已存在 <span class="hljs-regexp">/opt/</span>flutter<span class="hljs-regexp">/bin/</span>cache/flutter_version_check.stamp <br>发生错误，没有软件包被更新。<br></code></pre></td></tr></table></figure> ok,sudo直接删就行，反正是cache</p><p>最后<code>flutter docker</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">Doctor summary (to see all details, run flutter doctor -v):<br>[✓] Flutter (Channel stable, 3.13.0, on Arch Linux 6.4.10-arch1-1, locale zh_CN.UTF-8)<br>[✓] Android toolchain - develop <span class="hljs-keyword">for</span> Android devices (Android SDK version 34.0.0)<br>[✓] Chrome - develop <span class="hljs-keyword">for</span> the web<br>[✓] Linux toolchain - develop <span class="hljs-keyword">for</span> Linux desktop<br>[✓] Android Studio (version 2022.2)<br>[✓] Connected device (2 available)<br>[✓] Network resources<br><br>• No issues found!<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;arch4edu搞崩了我的flutter&quot;&gt;arch4edu搞崩了我的flutter&lt;/h1&gt;
&lt;p&gt;今天是快乐的一天，适合滚包 &lt;figure class=&quot;highlight ebnf&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LISA(推理分割)笔记</title>
    <link href="https://studyinglover.com/2023/08/18/LISA(%E6%8E%A8%E7%90%86%E5%88%86%E5%89%B2)%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/08/18/LISA(%E6%8E%A8%E7%90%86%E5%88%86%E5%89%B2)%E7%AC%94%E8%AE%B0/</id>
    <published>2023-08-18T15:05:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lisa推理分割笔记">LISA(推理分割)笔记</h1><h2 id="简介">简介</h2><p>这篇论文题目中文翻译是 基于大型语言模型的语义分割， 提出了一个新任务-推理分割。大概就是给一张图和一段话，模型使用大语言模型分割出目标。作者给了一个例子，从图片中分割出富含维生素C的物品。</p><p>作者说这篇论文有三个贡献，提出了推理分割的任务，建立了一个推理分割基准，ReasonSeg， 还有训练了一个模型。</p><p>项目主页<a href="https://github.com/dvlab-research/LISA">GitHub</a></p><p>LISA可以完成四种任务 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation</p><h2 id="模型架构">模型架构</h2><h3 id="生成mask">生成mask</h3><p>这里作者提出了一些问题，就是大部分llm是不具备视觉能力，有视觉能力的泛化型不好还不好训练。相比之下，训练 LISA-7B 在 8 个 NVIDIA 24G 3090 GPU 上只需要 10,000 个训练步骤。(嗯8块3090)</p><p><img src="https://cdn.studyinglover.com/pic/2023/08/ded90e7e3f84739b187dd679c39bd8dd.png" alt="image.png" /> 模型结构就是上面这张图，右下角标了火花的就说明是需要训练或者微调的。首先扩充词表，加入<code>&lt;SEG&gt;</code> ,接下来给出一张图片<span class="math inline">\(x_{img}\)</span>和一段文本<span class="math inline">\(x_{txt}\)</span>, 将他们送入大语言模型<span class="math inline">\(\mathcal{F}\)</span> ,写成公式就是<span class="math display">\[\hat{\boldsymbol{y}}_{txt}=\mathcal{F}(x_{img},\boldsymbol{x}_{txt}).\]</span> 当LLM倾向于生成二进制分割掩码时，输出<span class="math inline">\(\hat{\boldsymbol{y}}_{txt}\)</span>应该包含一个<code>&lt;SEG&gt;</code>令牌。所以提取最后一层嵌入<span class="math inline">\(\hat{h}_{seg}\)</span> (因为他和<code>&lt;SEG&gt;</code> token 是相关的)， 并用一个MLP <span class="math inline">\(\gamma\)</span> 将其投影到<span class="math inline">\(h_{seg}\)</span>。</p><p>同时，视觉编码器<span class="math inline">\(\mathcal{F_{enc}}\)</span> 会从图片中提取出视觉特征<span class="math inline">\(\text{f}\)</span> 。</p><p>最后<span class="math inline">\(h_{seg}\)</span>和<span class="math inline">\(\text{f}\)</span> 会被送入一个和SAM有相同架构的解码器，获得最后的mask.</p><p>整个过程表示出来就是<span class="math display">\[\begin{gathered}\boldsymbol{h}_{seg}=\gamma(\hat{\boldsymbol{h}}_{seg}),\quad\boldsymbol{f}=\mathcal{F}_{enc}(\boldsymbol{x}_{img}),\\\hat{\boldsymbol{M}}=\mathcal{F}_{dec}(\boldsymbol{h}_{seg},\boldsymbol{f}).\end{gathered}\]</span> ### 训练目标 训练目标是文本生成损失 <span class="math inline">\(\mathcal{L}_{txt}\)</span> 和分割掩码损失 <span class="math inline">\(\mathcal{L}_{mask}\)</span> 进行端到端训练。总体目标 <span class="math inline">\(L\)</span> 是这些损失的加权和，由 <span class="math inline">\(\lambda_{txt}\)</span> 和 <span class="math inline">\(\lambda_{mask}\)</span> 确定<span class="math display">\[\mathcal{L}=\lambda_{txt}\mathcal{L}_{txt}+\lambda_{mask}\mathcal{L}_{mask}.\]</span> ## 训练 ### 数据集 训练数据由三部分组成，都是开源数据集 1. Semantic Segmentation Dataset 2. Vanilla Referring Segmentation Dataset 3. Visual Question Answering Dataset</p><p><strong>值得注意的是，LISA具有zero-shot能力，因为训练集不包含任何推理分割的内容。</strong></p><h3 id="需要训练的参数">需要训练的参数</h3><p>为了保持llm的泛化能力作者用了lora,解码器可以被微调，llm的词嵌入和投影最后一层潜入的mlp也可以微调</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;lisa推理分割笔记&quot;&gt;LISA(推理分割)笔记&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;这篇论文题目中文翻译是 基于大型语言模型的语义分割， 提出了一个新任务-推理分割。大概就是给一张图和一段话，模型使用大语言模型分割出目标。作者给了一个例子，从</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="多模态" scheme="https://studyinglover.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>在终端绘制GPU显存使用曲线</title>
    <link href="https://studyinglover.com/2023/08/13/%E5%9C%A8%E7%BB%88%E7%AB%AF%E7%BB%98%E5%88%B6GPU%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E6%9B%B2%E7%BA%BF/"/>
    <id>https://studyinglover.com/2023/08/13/%E5%9C%A8%E7%BB%88%E7%AB%AF%E7%BB%98%E5%88%B6GPU%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E6%9B%B2%E7%BA%BF/</id>
    <published>2023-08-13T11:44:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在终端绘制gpu显存使用曲线">在终端绘制GPU显存使用曲线</h1><p>这个东西的灵感来自于写torch的时候想实时看到loss和gpu使用情况，突然想到可以在终端实时显示，经过与ai的一番激烈讨，最终有了这个代码。</p><p>我们首先要获取GPU的显存使用数据，先检查是否安装了<code>nvidia-smi</code>, 在终端输入有正常输出即可。</p><p>首先导入所有需要的库 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> asciichartpy<br><span class="hljs-keyword">import</span> platform<br></code></pre></td></tr></table></figure></p><p>通过<code>nvidia-smi</code> 的命令获取已经使用的显存和所有现存 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpu_used_memory</span>():<br>output = subprocess.check_output([<span class="hljs-string">&#x27;nvidia-smi&#x27;</span>, <span class="hljs-string">&#x27;--query-gpu=memory.used&#x27;</span>, <span class="hljs-string">&#x27;--format=csv,nounits&#x27;</span>])<br>output = output.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>lines = output.strip().split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>used_memory = <span class="hljs-built_in">int</span>(lines[<span class="hljs-number">1</span>])<br><span class="hljs-keyword">return</span> used_memory<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpu_total_memory</span>():<br>output = subprocess.check_output([<span class="hljs-string">&#x27;nvidia-smi&#x27;</span>, <span class="hljs-string">&#x27;--query-gpu=memory.total&#x27;</span>, <span class="hljs-string">&#x27;--format=csv,nounits&#x27;</span>])<br>output = output.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>lines = output.strip().split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>total_memory = <span class="hljs-built_in">int</span>(lines[<span class="hljs-number">1</span>])<br><span class="hljs-keyword">return</span> total_memory<br></code></pre></td></tr></table></figure></p><p><code>asciichartpy</code> 是一个 Python 库，用于在终端中绘制 ASCII 图表。我们用他来在终端绘制图标。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_gpu_memory</span>(<span class="hljs-params">gpu_memory_history</span>):<br>    used_memory = get_gpu_used_memory()<br>    total_memory = get_gpu_total_memory()<br><br>    used_percentage = used_memory / total_memory * <span class="hljs-number">100</span><br>    gpu_memory_history.append(used_percentage)<br><br>    <span class="hljs-comment"># 绘制字符图表</span><br>    chart = asciichartpy.plot(gpu_memory_history, &#123;<span class="hljs-string">&#x27;height&#x27;</span>: <span class="hljs-number">20</span>, <span class="hljs-string">&#x27;width&#x27;</span>: <span class="hljs-number">10</span>, <span class="hljs-string">&#x27;timestamp&#x27;</span>: <span class="hljs-literal">True</span>&#125;)<br>    <br>    <span class="hljs-comment"># 清空终端屏幕</span><br>    <span class="hljs-keyword">if</span> platform.system() == <span class="hljs-string">&#x27;Windows&#x27;</span>:<br>        subprocess.call(<span class="hljs-string">&#x27;cls&#x27;</span>, shell=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">else</span>:<br>        subprocess.call(<span class="hljs-string">&#x27;clear&#x27;</span>, shell=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-built_in">print</span>(chart)<br></code></pre></td></tr></table></figure></p><p>最后运行上面的代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    <span class="hljs-keyword">try</span>:<br>        gpu_memory_history = []<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            draw_gpu_memory(gpu_memory_history)<br>            time.sleep(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">except</span> KeyboardInterrupt:<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure> 运行效果 <img src="https://cdn.studyinglover.com/pic/2023/08/c320d69a8169e36fab4c82f1725c298b.png" alt="image.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;在终端绘制gpu显存使用曲线&quot;&gt;在终端绘制GPU显存使用曲线&lt;/h1&gt;
&lt;p&gt;这个东西的灵感来自于写torch的时候想实时看到loss和gpu使用情况，突然想到可以在终端实时显示，经过与ai的一番激烈讨，最终有了这个代码。&lt;/p&gt;
&lt;p&gt;我们首先要获取GPU的显存</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>GPTBot介绍</title>
    <link href="https://studyinglover.com/2023/08/11/GPTBot%E4%BB%8B%E7%BB%8D/"/>
    <id>https://studyinglover.com/2023/08/11/GPTBot%E4%BB%8B%E7%BB%8D/</id>
    <published>2023-08-11T20:58:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gptbot介绍">GPTBot介绍</h1><p>最近，openai公布了<a href="https://platform.openai.com/docs/gptbot/gptbot">GPTBot</a> 的相关信息，并给出了禁止GPTBot的方法。以下是全文翻译。</p><p>GPTBot是OpenAI的网络爬虫，可以通过以下User agent和字符串来识别。 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">User</span> agent token: GPTBot<br><span class="hljs-attribute">Full</span> user-agent string: Mozilla/<span class="hljs-number">5</span>.<span class="hljs-number">0</span> AppleWebKit/<span class="hljs-number">537</span>.<span class="hljs-number">36</span> (KHTML, like Gecko; compatible; GPTBot/<span class="hljs-number">1</span>.<span class="hljs-number">0</span>; +https://openai.com/gptbot)<br></code></pre></td></tr></table></figure></p><h2 id="使用">使用</h2><p>使用 GPTBot 用户代理爬取的网页可能会用于改进未来的模型，并且会过滤掉需要付费访问、已知收集个人身份信息（PII）或含有违反我们政策的文本的来源。允许 GPTBot 访问您的网站可以帮助 AI 模型变得更准确，提高它们的一般能力和安全性。在下面，我们还分享了如何禁止 GPTBot 访问您的网站。</p><h3 id="禁止-gptbot">禁止 GPTBot</h3><p>要禁止 GPTBot 访问您的网站，您可以将 GPTBot 添加到您网站的 robots.txt： <figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-attribute">User-agent</span><span class="hljs-punctuation">: </span>GPTBot<br><span class="hljs-attribute">Disallow</span><span class="hljs-punctuation">: </span>/<br></code></pre></td></tr></table></figure></p><h3 id="自定义-gptbot-访问">自定义 GPTBot 访问</h3><p>要允许 GPTBot 仅访问您网站的部分内容，您可以将 GPTBot 令牌添加到您网站的 robots.txt，如下所示： <figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">User-agent: GPTBot<br>Allow: <span class="hljs-regexp">/directory-1/</span><br>Disallow: <span class="hljs-regexp">/directory-2/</span><br></code></pre></td></tr></table></figure></p><h3 id="ip-出口范围">IP 出口范围</h3><p>对于 OpenAI 的爬虫，它会从 <a href="https://openai.com/gptbot-ranges.txt">OpenAI 网站</a>上记录的 IP 地址段向网站发出请求。</p><p>这里我给出IP 地址段 <figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">20.15.240.64</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.80</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.96</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.176</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.241.0</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.128</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.144</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.192</span>/<span class="hljs-number">28</span><br><span class="hljs-number">40.83.2.64</span>/<span class="hljs-number">28</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;gptbot介绍&quot;&gt;GPTBot介绍&lt;/h1&gt;
&lt;p&gt;最近，openai公布了&lt;a href=&quot;https://platform.openai.com/docs/gptbot/gptbot&quot;&gt;GPTBot&lt;/a&gt; 的相关信息，并给出了禁止GPTBot的方法。以下是</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>arch蓝牙无法连接</title>
    <link href="https://studyinglover.com/2023/08/10/arch%E8%93%9D%E7%89%99%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/"/>
    <id>https://studyinglover.com/2023/08/10/arch%E8%93%9D%E7%89%99%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/</id>
    <published>2023-08-10T17:18:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="arch蓝牙无法连接">arch蓝牙无法连接</h1><p>在arcchlinux成功安装并且已经安装蓝牙的相关包之后，在设置打开蓝牙发现需要先开启蓝牙。</p><p>没啥好的解决办法，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl <span class="hljs-built_in">enable</span>  --now bluetooth <br></code></pre></td></tr></table></figure> 问题解决。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;arch蓝牙无法连接&quot;&gt;arch蓝牙无法连接&lt;/h1&gt;
&lt;p&gt;在arcchlinux成功安装并且已经安装蓝牙的相关包之后，在设置打开蓝牙发现需要先开启蓝牙。&lt;/p&gt;
&lt;p&gt;没啥好的解决办法，运行 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;ta</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>GPU部署llama-cpp-python(llama.cpp通用)</title>
    <link href="https://studyinglover.com/2023/08/06/GPU%E9%83%A8%E7%BD%B2llama-cpp-python(llama.cpp%E9%80%9A%E7%94%A8)/"/>
    <id>https://studyinglover.com/2023/08/06/GPU%E9%83%A8%E7%BD%B2llama-cpp-python(llama.cpp%E9%80%9A%E7%94%A8)/</id>
    <published>2023-08-06T23:01:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gpu部署llama-cpp-pythonllama.cpp通用">GPU部署llama-cpp-python(llama.cpp通用)</h1><h2 id="通用流程">通用流程</h2><p>我们的安装平台是Ubuntu20.04，Python 3.8.10，cuda 11.6。</p><p>首先确保自己是否已经安装了cuda,输入 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -V<br></code></pre></td></tr></table></figure></p><p>有类似下面的输出即可 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">nvcc</span>: NVIDIA (R) Cuda compiler driver<br><span class="hljs-attribute">Copyright</span> (c) <span class="hljs-number">2005</span>-<span class="hljs-number">2021</span> NVIDIA Corporation<br><span class="hljs-attribute">Built</span> <span class="hljs-literal">on</span> Fri_Dec_17_18:<span class="hljs-number">16</span>:<span class="hljs-number">03</span>_PST_2021<br><span class="hljs-attribute">Cuda</span> compilation tools, release <span class="hljs-number">11</span>.<span class="hljs-number">6</span>, V11.<span class="hljs-number">6</span>.<span class="hljs-number">55</span><br><span class="hljs-attribute">Build</span> cuda_11.<span class="hljs-number">6</span>.r11.<span class="hljs-number">6</span>/compiler.<span class="hljs-number">30794723</span>_0<br></code></pre></td></tr></table></figure></p><p>我们选用 <code>cuBLAS</code> 加速后端代理。直接按照下面命令安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> LLAMA_CUBLAS=1<br>CMAKE_ARGS=<span class="hljs-string">&quot;-DLLAMA_CUBLAS=on&quot;</span> FORCE_CMAKE=1 pip install llama-cpp-python<br></code></pre></td></tr></table></figure></p><p>不出意外的话就安装好了，但是你会出现很多意外，请你努力在一堆红色的报错中找出关键出错点，然后搜索，在最后我给出了几个我遇到的。</p><h2 id="运行">运行</h2><p>运行和CPU直接运行相似，只是需要加入几个参数. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model llama-2-70b-chat.ggmlv3.q5_K_M.bin --n_threads 30 --n_gpu_layers 200<br></code></pre></td></tr></table></figure></p><p><code>n_threads</code> 是一个CPU也有的参数，代表最多使用多少线程。</p><p><code>n_gpu_layers</code> 是一个GPU部署非常重要的一步，代表大语言模型有多少层在GPU运算，如果你的显存出现 <code>out of memory</code> 那就减小 <code>n_gpu_layers</code></p><h2 id="关于多卡">关于多卡</h2><p>亲测多卡没有遇到什么大坑，只要<code>torch.cuda.is_available()</code> 和<code>torch.cuda.device_count()</code>正常就可以跑起来。</p><p>两张 Tesla T4 的卡推理70B大概半分钟就可以出结果。</p><h2 id="报错解决">报错解决</h2><h3 id="check-for-working-cuda-compiler-usrlocalcudabinnvcc---skipped">Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped</h3><p>参考 https://github.com/ggerganov/llama.cpp/issues/1832 系统安装过程中没找到你的cuda在哪里，所以在pip安装之前先设置一个环境变量,<strong>把/usr/local/cuda-x.y改成你的cuda路径</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> CUDA_PATH=/usr/local/cuda-x.y<br></code></pre></td></tr></table></figure></p><h3 id="f16c-expected-a-number">'f16c': expected a number</h3><p>这是你的cuda版本太低了，升级到较新版本(11.6可用)。</p><p>或者参考 https://github.com/ggerganov/llama.cpp/issues/1467 和 https://github.com/marella/ctransformers/issues/53 中提到的命令和构建(我没有尝试，有谁试了可以请我结果)。</p><h3 id="value-sm_30-is-not-defined-for-option-gpu-name-tesla-t">Value 'sm_30' is not defined for option 'gpu-name' Tesla T</h3><p>先运行下面的命令 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-cache policy nvidia-cuda-toolkit<br></code></pre></td></tr></table></figure> 如果版本是<strong>1.0</strong> 那么请运行 <code>sudo apt remove nvidia-cuda-toolkit</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;gpu部署llama-cpp-pythonllama.cpp通用&quot;&gt;GPU部署llama-cpp-python(llama.cpp通用)&lt;/h1&gt;
&lt;h2 id=&quot;通用流程&quot;&gt;通用流程&lt;/h2&gt;
&lt;p&gt;我们的安装平台是Ubuntu20.04，Python 3.8.</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>花式求GCD</title>
    <link href="https://studyinglover.com/2023/08/02/%E8%8A%B1%E5%BC%8F%E6%B1%82GCD/"/>
    <id>https://studyinglover.com/2023/08/02/%E8%8A%B1%E5%BC%8F%E6%B1%82GCD/</id>
    <published>2023-08-02T18:46:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="花式求gcd">花式求GCD</h1><p>今天学校实验室纳新群有同学提到了<code>a^=b^=a^=b​</code> 交换两个数的操作，我突然想到之前在知乎看到通过异或实现gcd的方法，一番翻找后没啥结果，便去问了下认识的oi大佬有没有一行求gcd的算法。</p><p>大佬很快给出了一个函数<code>int gcd(int a,int b)&#123;return y?gcd(y,x%y):x;&#125;</code> 真的就是一行，完整的代码就是下面这个</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">gcd</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span> </span>&#123; <span class="hljs-keyword">return</span> y ? <span class="hljs-built_in">gcd</span>(y, x % y) : x; &#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-type">int</span> a,b;<br>a=<span class="hljs-number">10</span>;<br>b=<span class="hljs-number">20</span>;<br>a = <span class="hljs-built_in">gcd</span>(a,b);<br>cout&lt;&lt;a&lt;&lt;endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>但是我一像不对啊，我的异或呢？我又问了一下，大佬给了我一个截图 <img src="https://cdn.studyinglover.com/pic/2023/08/07b57e65da92d9c19bb82d740132f07c.png" /></p><p>就是这个神奇的写法</p><p>这段代码的实现方式是，使用异或运算符（^）和取模运算符（%）来交换变量a和b的值。具体来说，代码中的while循环会一直执行，直到b的值为0为止。在每次循环中，代码会先将a对b取模，然后将结果赋值给a，接着将b对a取模，然后将结果赋值给b，最后使用异或运算符交换a和b的值。这样，当循环结束时，a和b的值就被成功地交换了。(来自copilot chat)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-type">int</span> a,b;<br>a=<span class="hljs-number">10</span>;<br>b=<span class="hljs-number">20</span>;<br><span class="hljs-keyword">while</span>(b^=a^=b^=a%=b);<br>cout&lt;&lt;a&lt;&lt;endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;花式求gcd&quot;&gt;花式求GCD&lt;/h1&gt;
&lt;p&gt;今天学校实验室纳新群有同学提到了&lt;code&gt;a^=b^=a^=b​&lt;/code&gt; 交换两个数的操作，我突然想到之前在知乎看到通过异或实现gcd的方法，一番翻找后没啥结果，便去问了下认识的oi大佬有没有一行求gcd的算法</summary>
      
    
    
    
    
    <category term="算法" scheme="https://studyinglover.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>使用llama构建一个蜜罐(前端)</title>
    <link href="https://studyinglover.com/2023/08/01/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%89%8D%E7%AB%AF)/"/>
    <id>https://studyinglover.com/2023/08/01/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%89%8D%E7%AB%AF)/</id>
    <published>2023-08-01T00:12:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用llama构建一个蜜罐前端">使用llama构建一个蜜罐(前端)</h1><p><img src="https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png" /> 在<a href="https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/">使用llama构建一个蜜罐(后端)</a> 中我们通过llama和flask构建了一个蜜罐的后端，通过将shell命令作为字段的一部分，让llama假装执行命令来防止蜜罐被攻破。那有了后端我们还需要一个前端命令行来让用户登陆并执行命令。</p><p>完整项目开源在了<a href="https://github.com/StudyingLover/llama-honeypot-python">GitHub</a></p><p>接下来，让我们来实现一个模拟ssh服务器，或者说实现一个ssh mock 然后执行命令的时候不让他真的执行同时改一下输出。</p><p><strong>等等？我们真的需要一个ssh mock 吗？</strong> 还是说，我们需要的是一个<strong>跑在终端的，长得很像终端的，能输入输出的，一个可交互的代码？</strong></p><p>哦，好像我们需要的只是一个可交互的代码，难道攻击方ssh上来了还能验证一下这是不是真的是终端？(我用了三天才想通这个问题)</p><p>so,工作量一下子减少了太多了 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> requests<br>  <br><span class="hljs-comment"># 禁用 Ctrl Z stty susp undef</span><br><span class="hljs-comment"># 启用 Ctrl Z stty susp ^Z</span><br><br>admin_key = <span class="hljs-string">&quot;123456&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_responce</span>(<span class="hljs-params">command</span>):<br><span class="hljs-keyword">if</span> (command == admin_key):<br>exit()<br>output = requests.post(<span class="hljs-string">&quot;http://127.0.0.1:9000/admin/&quot;</span>+command).json()<br><span class="hljs-keyword">return</span> output[<span class="hljs-string">&quot;message&quot;</span>]<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">attact_warning</span>():<br><span class="hljs-keyword">pass</span><br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">anti_attact</span>():<br><span class="hljs-keyword">pass</span><br>  <br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>attact_warning()<br>anti_attact()<br><span class="hljs-keyword">try</span>:<br>command = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;[root@ubuntu ~]$ &quot;</span>)<br><span class="hljs-built_in">print</span>(get_responce(command))<br>  <br><span class="hljs-keyword">except</span> KeyboardInterrupt:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>这里有几个点需要注意 1. 代码<strong>不能直接用于生产环境！！！请先完善细节并大量测试。本项目仅为学习使用，未经过专业人员测试</strong> 1. <code>admin_key</code>，这个变量的作用是让管理员能用终端，<strong>记得修改</strong>。如果你认为这种方法太low了或者可能被作为突破口，请修改或PR。 2. 接口地址，我这里是<code>http://127.0.0.1:9000/admin/</code> ，这里需要改成你的，建议先用postman或者apifox或者啥的测一下。 3. 入侵检测和反击模块需要你<strong>自己实现</strong>，毕竟这只是一个让你的蜜罐更安全的项目。</p><p>在三个终端分别运行llama服务器(图右终端)，蜜罐后端(图左终端)和蜜罐前端(图中终端)</p><figure><img src="https://cdn.studyinglover.com/pic/2023/07/dd31f63365b8a8657b1459f7fe883a36.png" alt="" /><figcaption>image.png</figcaption></figure><p>项目还有很多改进之处，在后面我也会进一步优化prompt和模型来获得更好的终端对话体验。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用llama构建一个蜜罐前端&quot;&gt;使用llama构建一个蜜罐(前端)&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png&quot;</summary>
      
    
    
    
    
    <category term="网络安全" scheme="https://studyinglover.com/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>使用llama构建一个蜜罐(后端)</title>
    <link href="https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/"/>
    <id>https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/</id>
    <published>2023-07-29T17:52:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用llama构建一个蜜罐后端">使用llama构建一个蜜罐(后端)</h1><p><img src="https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png" /></p><p>完整项目开源在了<a href="https://github.com/StudyingLover/llama-honeypot-python">GitHub</a></p><p>众所周知，蜜罐是一个很有趣的东西，他是一种网络安全机制，旨在诱使攻击者攻击虚假的系统或应用程序，以便安全专业人员可以监视攻击者的行为并收集攻击者的信息。蜜罐通常是一台虚拟机或一台计算机，它看起来像一个真实的系统，但实际上是一个特意构建的系统，用于诱骗攻击者。攻击者在攻击蜜罐时，安全专业人员可以收集攻击者的信息，例如攻击者使用的工具、攻击者的IP地址、攻击者的攻击技术等等。这些信息可以帮助安全专业人员更好地了解攻击者的行为和意图，并采取相应的措施来保护真实的系统。</p><p>但是缺点很明显，不管我怎么做蜜罐终究是跑在真实的服务器上的，还是很可能被攻破，所以，我们能不能让ai模仿一个linux主机作为蜜罐？</p><p>今天早上看到了这个视频 https://b23.tv/pXiGNIK ， 他开源了一个使用chatGPT作为终端的代码，开源在<a href="gitee.com/cutecuteyu/chatgpt-honeypot">gitee</a> ，不幸的是我openai账户没钱了，但是，昨天我才写了<a href="https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/#%E6%90%AD%E5%BB%BA%E4%B8%8Eopenai%E6%8E%A5%E5%8F%A3%E5%85%BC%E5%AE%B9%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E5%8F%A3">搭建与openai接口兼容的服务器接口</a>, 那么我就可以改造一下他的代码，使用llama作为后端</p><p>首先clone他的仓库 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> gitee.com/cutecuteyu/chatgpt-honeypot<br><span class="hljs-built_in">cd</span> ./chatgpt-honeypot<br></code></pre></td></tr></table></figure></p><p>同时安装依赖 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install openai<br></code></pre></td></tr></table></figure></p><p>接下来我们在<code>chatgpt-honeypot</code>目录下创建一个 <code>.env</code> 文件，写上接口路径 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs .env">export OPENAI_API_BASE = http://localhost:8000/v1<br></code></pre></td></tr></table></figure></p><p>然后修改<code>myopenaiapikey.py</code> 文件，在第二行的<code>api=""</code> 中双引号随便填入一点东西。</p><p>下面修改<code>honeypot.py</code> ，因为我们的后端换成了llama,那么我们的prompt也需要更改,这里借鉴了<a href="https://github.com/Coldwave96/llama-honeypot">这个项目</a> ,将<code>chat2</code> 函数改成下面的内容 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">chat2</span>(<span class="hljs-params">query</span>):<br>response = openai.ChatCompletion.create(<br>model=<span class="hljs-string">&quot;gpt-3.5-turbo-0613&quot;</span>,<br>messages=[<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>,<br><span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">I want you to act as a Linux terminal. I will provide commands and history, then you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do no write explanations. Do not type commands unless I instruct you to do so.\n\n### Command:\n&#123;command&#125;\n\n### History:\n&#123;history&#125;\n### Response:\n</span><br><span class="hljs-string">&quot;&quot;&quot;</span>&#125;,<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: query&#125;],<br>)<br>message = response[<span class="hljs-string">&quot;choices&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;message&quot;</span>]<br><span class="hljs-keyword">return</span> message<br></code></pre></td></tr></table></figure></p><p>启动项目，正常IDE运行或者在命令行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3  honeypot.py<br></code></pre></td></tr></table></figure></p><p>启动llama后端,将/path/to改成你的路径 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model  /path/to/llama-2-13b-chat.ggmlv3.q4_1.bin<br></code></pre></td></tr></table></figure></p><p>在浏览器访问<code>http://127.0.0.1:9000/admin/ls</code>,看到浏览器显示<code>/home/user/Documents/project</code> 类似的内容说明运行成功。</p><p>项目当然还有很多可以改进的地方，例如使用更好的prompt,或者微调llama作为后端，留给大家继续探索。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用llama构建一个蜜罐后端&quot;&gt;使用llama构建一个蜜罐(后端)&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png&quot;</summary>
      
    
    
    
    
    <category term="网络安全" scheme="https://studyinglover.com/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>llama-cpp-python快速上手</title>
    <link href="https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <id>https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</id>
    <published>2023-07-28T17:23:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="llama-cpp-python快速上手">llama-cpp-python快速上手</h1><h2 id="搭建环境">搭建环境</h2><p>项目地址<a href="https://github.com/abetlen/llama-cpp-python">GitHub</a>,有能力的话可以直接阅读原始文档。</p><p>首先按照文档，安装llama-cpp-python <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install llama-cpp-python<br></code></pre></td></tr></table></figure></p><p>接下来，你可能缺一些依赖，这一点在文档中没有涉及但是我整理了我缺少的依赖，依次运行即可。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install uvicorn<br>pip install anyio<br>pip install starlette<br>pip install fastapi<br>pip install pydantic_settings<br>pip install sse_starlette<br></code></pre></td></tr></table></figure></p><h2 id="高级api和低级api">高级API和低级API</h2><h3 id="高级api">高级API</h3><p>高级 API 通过<code>Llama</code>类提供简单的托管接口。请将<code>./models/7B/ggml-model.bin</code> 换成你的模型的路径，下同。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_cpp <span class="hljs-keyword">import</span> Llama<br>llm = Llama(model_path=<span class="hljs-string">&quot;./models/7B/ggml-model.bin&quot;</span>)<br>output = llm(<span class="hljs-string">&quot;Q: Name the planets in the solar system? A: &quot;</span>, max_tokens=<span class="hljs-number">32</span>, stop=[<span class="hljs-string">&quot;Q:&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>], echo=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure> 返回值如下 <figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs arcade">&#123;<br>  <span class="hljs-string">&quot;id&quot;</span>: <span class="hljs-string">&quot;cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;</span>,<br>  <span class="hljs-string">&quot;object&quot;</span>: <span class="hljs-string">&quot;text_completion&quot;</span>,<br>  <span class="hljs-string">&quot;created&quot;</span>: <span class="hljs-number">1679561337</span>,<br>  <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;./models/7B/ggml-model.bin&quot;</span>,<br>  <span class="hljs-string">&quot;choices&quot;</span>: [<br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.&quot;</span>,<br>      <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-number">0</span>,<br>      <span class="hljs-string">&quot;logprobs&quot;</span>: <span class="hljs-built_in">None</span>,<br>      <span class="hljs-string">&quot;finish_reason&quot;</span>: <span class="hljs-string">&quot;stop&quot;</span><br>    &#125;<br>  ],<br>  <span class="hljs-string">&quot;usage&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;prompt_tokens&quot;</span>: <span class="hljs-number">14</span>,<br>    <span class="hljs-string">&quot;completion_tokens&quot;</span>: <span class="hljs-number">28</span>,<br>    <span class="hljs-string">&quot;total_tokens&quot;</span>: <span class="hljs-number">42</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></p><h3 id="低级api">低级API</h3><p>低级 API 直接<a href="https://docs.python.org/3/library/ctypes.html"><code>ctypes</code></a>绑定到<code>llama.cpp</code>. 整个低级 API 可以在<a href="https://github.com/abetlen/llama-cpp-python/blob/master/llama_cpp/llama_cpp.py">llama_cpp/llama_cpp.py</a>中找到，并直接镜像<a href="https://github.com/ggerganov/llama.cpp/blob/master/llama.h">llama.h</a>中的 C API 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> llama_cpp<br><span class="hljs-keyword">import</span> ctypes<br>params = llama_cpp.llama_context_default_params()<br><span class="hljs-comment"># use bytes for char * params</span><br>ctx = llama_cpp.llama_init_from_file(<span class="hljs-string">b&quot;./models/7b/ggml-model.bin&quot;</span>, params)<br>max_tokens = params.n_ctx<br><span class="hljs-comment"># use ctypes arrays for array params</span><br>tokens = (llama_cpp.llama_token * <span class="hljs-built_in">int</span>(max_tokens))()<br>n_tokens = llama_cpp.llama_tokenize(ctx, <span class="hljs-string">b&quot;Q: Name the planets in the solar system? A: &quot;</span>, tokens, max_tokens, add_bos=llama_cpp.c_bool(<span class="hljs-literal">True</span>))<br>llama_cpp.llama_free(ctx)<br></code></pre></td></tr></table></figure><h2 id="搭建与openai接口兼容的服务器接口">搭建与openai接口兼容的服务器接口</h2><p><code>llama-cpp-python</code>提供一个 Web 服务器，旨在作为 OpenAI API 的直接替代品。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model models/7B/ggml-model.bin<br></code></pre></td></tr></table></figure> 你可以在上面的命令运行成功后访问<a href="http://localhost:8000/docs">文档</a></p><p>文档是全英的，想要对话接口的话我用python写了个示例 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br>  <br>url = <span class="hljs-string">&#x27;http://localhost:8000/v1/chat/completions&#x27;</span><br>headers = &#123;<br><span class="hljs-string">&#x27;accept&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span>,<br><span class="hljs-string">&#x27;Content-Type&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span><br>&#125;<br>data = &#123;<br><span class="hljs-string">&#x27;messages&#x27;</span>: [<br>&#123;<br><span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;You are a helpful assistant.&#x27;</span>,<br><span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span><br>&#125;,<br>&#123;<br><span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;What is the capital of France?&#x27;</span>,<br><span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span><br>&#125;<br>]<br>&#125;<br>  <br>response = requests.post(url, headers=headers, json=data)<br><span class="hljs-built_in">print</span>(response.json())<br><span class="hljs-built_in">print</span>(response.json()[<span class="hljs-string">&#x27;choices&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>])<br></code></pre></td></tr></table></figure></p><p>如果你想自建一个接口，请在遵守相关法律法规的情况下，在自己的服务器上启动相关服务，并反向代理<code>http://localhost:8000</code> 地址。例如你反向代理到了<code>https://example.com</code>,那你的对话地址就是<code>https://example.com/v1/chat/completions</code>。当你想用gpt的时候就不用看openai的脸色了，直接部署一个自己的接口自己请求，或者调用openai库的时候apibase写自己的接口。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;llama-cpp-python快速上手&quot;&gt;llama-cpp-python快速上手&lt;/h1&gt;
&lt;h2 id=&quot;搭建环境&quot;&gt;搭建环境&lt;/h2&gt;
&lt;p&gt;项目地址&lt;a href=&quot;https://github.com/abetlen/llama-cpp-python&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>快速上手llama2.c(更新版)</title>
    <link href="https://studyinglover.com/2023/07/28/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c(%E6%9B%B4%E6%96%B0%E7%89%88)/"/>
    <id>https://studyinglover.com/2023/07/28/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c(%E6%9B%B4%E6%96%B0%E7%89%88)/</id>
    <published>2023-07-28T16:31:00.000Z</published>
    <updated>2023-09-07T07:50:45.743Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速上手llama2.c更新版">快速上手llama2.c(更新版)</h1><p>在上一次我同时在我的博客和知乎发布了<a href="https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/">快速上手llama2.c</a> 之后，我一个小透明也收获了不少收藏，并收到了人生中第一个这样的留言(其实我感觉是机器人)。 <img src="https://cdn.studyinglover.com/pic/2023/07/2eda3b2dcb8d68fc01169f5366c8157c.jpg" /></p><p>当然，之前的llama2.c也有一些不好的地方，例如不能添加自己的prompt,所以我提了这样的一个<a href="https://github.com/karpathy/llama2.c/issues/64">issue</a>,今天收到了贡献者的回复说是可以用了。那我们来看一下。</p><p>首先还是克隆整个仓库，编译并下载模型，这里以15m参数的模型作为示例 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/llama2.c.git<br><span class="hljs-built_in">cd</span> llama2.c<br>make run<br>wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin<br></code></pre></td></tr></table></figure></p><p>接下来我们就可以使用编译出来的<code>run</code> 运行了,要使用自己的prompt,需要指定温度和 步长，这里温度设置成1.0,步长设置256,prompt在双引号写，我这里写的是<code>One day morning , I don't want to go to school</code> . <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./run stories15M.bin 1.0 256 <span class="hljs-string">&quot;One day morning , I don&#x27;t want to go to school&quot;</span><br></code></pre></td></tr></table></figure></p><p>这里给出我的运行结果，也就3秒种不到 <figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs tp">&lt;s&gt;<br>One day morning , I don<span class="hljs-string">&#x27;t want to go to school, so he packed his trunk lid to pack. memorized his chores, he thought about what his mom would like him to stay home and not do all day. She wanted him to in a very competitive way.</span><br><span class="hljs-string">&quot;Come and play in the puddle, it&#x27;</span>ll be more fun<span class="hljs-comment">!&quot;He begged.</span><br><span class="hljs-comment">Mom shook her head. &quot;No, we haven&#x27;t seen coming for sure,&quot; she said thought. </span><br><span class="hljs-comment">Thumper and Mom just shrugged.</span><br><span class="hljs-comment">&quot;See,&quot; she said. &quot;Come on now. Let&#x27;s go and find some fun ways to clean the world!&quot;</span><br><span class="hljs-comment">The little boy was relieved and ran out to the yard. He had found a great idea to share his day with his mom instead. They scattered around the yard and had fun playing until their tired eyes were aching.</span><br><span class="hljs-comment">&lt;s&gt;</span><br><span class="hljs-comment">Once upon a time, there was a little boy named Tim. Tim was very excited because he was going on a trip with his family. He saw a big bus that helped them get off at their destination.</span><br><span class="hljs-comment">As the bus drove along, Tim noticed an unusual looking man sitting next to it. Tim asked the</span><br><span class="hljs-comment">achieved tok/s: 175.378267</span><br><span class="hljs-comment"></span><br></code></pre></td></tr></table></figure></p><p>当然为了获得更好的效果，我们可以使用更大模型</p><p>下载42m参数模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin<br></code></pre></td></tr></table></figure></p><p>下载110m参数模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;快速上手llama2.c更新版&quot;&gt;快速上手llama2.c(更新版)&lt;/h1&gt;
&lt;p&gt;在上一次我同时在我的博客和知乎发布了&lt;a href=&quot;https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
</feed>
