<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>plus studio</title>
  
  
  <link href="https://studyinglover.com/atom.xml" rel="self"/>
  
  <link href="https://studyinglover.com/"/>
  <updated>2023-08-22T14:11:42.152Z</updated>
  <id>https://studyinglover.com/</id>
  
  <author>
    <name>StudyingLover</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ViT在DDPM取代UNet(DiT)</title>
    <link href="https://studyinglover.com/2023/08/20/ViT%E5%9C%A8DDPM%E5%8F%96%E4%BB%A3UNet(DiT)/"/>
    <id>https://studyinglover.com/2023/08/20/ViT%E5%9C%A8DDPM%E5%8F%96%E4%BB%A3UNet(DiT)/</id>
    <published>2023-08-20T09:43:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="vit在ddpm取代unetdit">ViT在DDPM取代UNet(DiT)</h1><p><a href="https://www.wpeebles.com/DiT.html">项目主页</a></p><p>这篇论文主要是尝试使用ViT取代DDPM中的UNet，叫做Diffusion Transformer-DiT，作者训练了DiT-S、DiT-B、DiT-L 和 DiT-XL四种模型，每种模型的patch取8,4,2, 一共训练了12个模型。</p><p>作者探索的完整 DiT 设计空间是补丁大小、变压器块架构和模型大小。</p><p>模型第一层是对 sequences of patches 进行操作(就是ViT把图片看成<span class="math inline">\(16*16\)</span>的的单词之后单词构成的序列) 。 <img src="https://cdn.studyinglover.com/pic/2023/08/d9b9a168f177471d890c1bd3e3f2cc2d.png" alt="image.png" /></p><p>如图所示，给定的patch是<span class="math inline">\(p\times p\)</span> ,VAE采样出来的草绳大小是<span class="math inline">\(I\times I\times C\)</span> ,那么patches会变成长度为<span class="math inline">\(T=(I/\hat{p})^{2}\)</span> 的一个序列,每个patch维度是<span class="math inline">\(d\)</span> ,位置嵌入用的是sine-cosine。</p><p>接下来就是diffusion transformers的设计。 <img src="https://cdn.studyinglover.com/pic/2023/08/f68c4f271029a484e97822dbb9fb2569.png" alt="image.png" /></p><p>作者提到了一点，就是获取到path序列之后应该在后面加上去噪步数和类别标签，并在最后一个DiT块之后删掉。</p><p>在最终的 DiT 块之后，需要将输出解码为噪声预测和对角协方差预测。这两个输出的形状都等于整个模型的输入。作者使用标准线性解码器来做到这一点。如果使用 adaLN 自适应就应用最后一层范数，并将每个标记线性解码为 <span class="math inline">\(p\times p\times2C\)</span> 张量，其中 <span class="math inline">\(C\)</span> 是输入到DiT的空间大小。最后，将解码的token重新排列到其原始空间布局中，得到预测的噪声和协方差。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;vit在ddpm取代unetdit&quot;&gt;ViT在DDPM取代UNet(DiT)&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.wpeebles.com/DiT.html&quot;&gt;项目主页&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇论文主要是尝试使用ViT取代DDPM中的UNe</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/tags/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>arch4edu搞崩了我的flutter</title>
    <link href="https://studyinglover.com/2023/08/19/arch4edu%E6%90%9E%E5%B4%A9%E4%BA%86%E6%88%91%E7%9A%84flutter/"/>
    <id>https://studyinglover.com/2023/08/19/arch4edu%E6%90%9E%E5%B4%A9%E4%BA%86%E6%88%91%E7%9A%84flutter/</id>
    <published>2023-08-19T21:36:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="arch4edu搞崩了我的flutter">arch4edu搞崩了我的flutter</h1><p>今天是快乐的一天，适合滚包 <figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">yay</span><br></code></pre></td></tr></table></figure> 一切安好，arch4edu说我的flutter需要更新 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">==&gt; 要排除的包: (示例: <span class="hljs-string">&quot;1 2 3&quot;</span>, <span class="hljs-string">&quot;1-3&quot;</span>, <span class="hljs-string">&quot;^4&quot;</span> 或软件库名称)<br> -&gt; 排除软件包可能会导致不完整的升级并破坏系统<br>==&gt; <br><br></code></pre></td></tr></table></figure> 没什么需要排除的，接下来就是愉快的自动安装</p><p>突然我看到了这个</p><figure><img src="https://cdn.studyinglover.com/pic/2023/08/d257220b6c5bc01465f92fdd72320344.png" alt="" /><figcaption>image.png</figcaption></figure><p>警告啦，没啥好担心的啦，待会跑一下看好着没</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">flutter doctor                     <br>Found an existing Pub cache at /home/zjh/.pub-cache.<br>It can be repaired by running `dart pub cache repair`.<br>It can be reset by running `dart pub cache clean`.<br>Found an existing Dart Analysis Server cache at /home/zjh/.dartServer.<br>It can be reset by deleting /home/zjh/.dartServer.<br>Flutter failed to write to a file at <span class="hljs-string">&quot;/opt/flutter/packages/flutter_tools/.dart_tool/version&quot;</span>.<br>Please ensure that the SDK and/or project is installed <span class="hljs-keyword">in</span> a location that has <span class="hljs-built_in">read</span>/write<br>permissions <span class="hljs-keyword">for</span> the current user.<br>Try running:<br>  sudo <span class="hljs-built_in">chown</span> -R $(<span class="hljs-built_in">whoami</span>) /opt/flutter/packages/flutter_tools/.dart_tool/version<br><br></code></pre></td></tr></table></figure><p>好的他炸了</p><p>看着问题不大，就是读写权限的问题，的问题？鬼知道会有啥问题，我决定让arch4edu滚蛋</p><p>先<code>sudo pacman -Rns flutter</code>把arch4edu的flutter删掉，然后去<code>/etc/pacman.conf</code> 删除了arch4edu镜像，再<code>sudo pacman -Syu</code>滚一遍包，最后<code>yay flutter</code></p><p>中间会有一个问题 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">错误：无法提交处理 (有冲突的文件)<br>flutter: 文件系统中已存在 <span class="hljs-regexp">/opt/</span>flutter<span class="hljs-regexp">/bin/</span>cache/flutter_version_check.stamp <br>发生错误，没有软件包被更新。<br></code></pre></td></tr></table></figure> ok,sudo直接删就行，反正是cache</p><p>最后<code>flutter docker</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">Doctor summary (to see all details, run flutter doctor -v):<br>[✓] Flutter (Channel stable, 3.13.0, on Arch Linux 6.4.10-arch1-1, locale zh_CN.UTF-8)<br>[✓] Android toolchain - develop <span class="hljs-keyword">for</span> Android devices (Android SDK version 34.0.0)<br>[✓] Chrome - develop <span class="hljs-keyword">for</span> the web<br>[✓] Linux toolchain - develop <span class="hljs-keyword">for</span> Linux desktop<br>[✓] Android Studio (version 2022.2)<br>[✓] Connected device (2 available)<br>[✓] Network resources<br><br>• No issues found!<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;arch4edu搞崩了我的flutter&quot;&gt;arch4edu搞崩了我的flutter&lt;/h1&gt;
&lt;p&gt;今天是快乐的一天，适合滚包 &lt;figure class=&quot;highlight ebnf&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LISA(推理分割)笔记</title>
    <link href="https://studyinglover.com/2023/08/18/LISA(%E6%8E%A8%E7%90%86%E5%88%86%E5%89%B2)%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/08/18/LISA(%E6%8E%A8%E7%90%86%E5%88%86%E5%89%B2)%E7%AC%94%E8%AE%B0/</id>
    <published>2023-08-18T15:05:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lisa推理分割笔记">LISA(推理分割)笔记</h1><h2 id="简介">简介</h2><p>这篇论文题目中文翻译是 基于大型语言模型的语义分割， 提出了一个新任务-推理分割。大概就是给一张图和一段话，模型使用大语言模型分割出目标。作者给了一个例子，从图片中分割出富含维生素C的物品。</p><p>作者说这篇论文有三个贡献，提出了推理分割的任务，建立了一个推理分割基准，ReasonSeg， 还有训练了一个模型。</p><p>项目主页<a href="https://github.com/dvlab-research/LISA">GitHub</a></p><p>LISA可以完成四种任务 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation</p><h2 id="模型架构">模型架构</h2><h3 id="生成mask">生成mask</h3><p>这里作者提出了一些问题，就是大部分llm是不具备视觉能力，有视觉能力的泛化型不好还不好训练。相比之下，训练 LISA-7B 在 8 个 NVIDIA 24G 3090 GPU 上只需要 10,000 个训练步骤。(嗯8块3090)</p><p><img src="https://cdn.studyinglover.com/pic/2023/08/ded90e7e3f84739b187dd679c39bd8dd.png" alt="image.png" /> 模型结构就是上面这张图，右下角标了火花的就说明是需要训练或者微调的。首先扩充词表，加入<code>&lt;SEG&gt;</code> ,接下来给出一张图片<span class="math inline">\(x_{img}\)</span>和一段文本<span class="math inline">\(x_{txt}\)</span>, 将他们送入大语言模型<span class="math inline">\(\mathcal{F}\)</span> ,写成公式就是<span class="math display">\[\hat{\boldsymbol{y}}_{txt}=\mathcal{F}(x_{img},\boldsymbol{x}_{txt}).\]</span> 当LLM倾向于生成二进制分割掩码时，输出<span class="math inline">\(\hat{\boldsymbol{y}}_{txt}\)</span>应该包含一个<code>&lt;SEG&gt;</code>令牌。所以提取最后一层嵌入<span class="math inline">\(\hat{h}_{seg}\)</span> (因为他和<code>&lt;SEG&gt;</code> token 是相关的)， 并用一个MLP <span class="math inline">\(\gamma\)</span> 将其投影到<span class="math inline">\(h_{seg}\)</span>。</p><p>同时，视觉编码器<span class="math inline">\(\mathcal{F_{enc}}\)</span> 会从图片中提取出视觉特征<span class="math inline">\(\text{f}\)</span> 。</p><p>最后<span class="math inline">\(h_{seg}\)</span>和<span class="math inline">\(\text{f}\)</span> 会被送入一个和SAM有相同架构的解码器，获得最后的mask.</p><p>整个过程表示出来就是<span class="math display">\[\begin{gathered}\boldsymbol{h}_{seg}=\gamma(\hat{\boldsymbol{h}}_{seg}),\quad\boldsymbol{f}=\mathcal{F}_{enc}(\boldsymbol{x}_{img}),\\\hat{\boldsymbol{M}}=\mathcal{F}_{dec}(\boldsymbol{h}_{seg},\boldsymbol{f}).\end{gathered}\]</span> ### 训练目标 训练目标是文本生成损失 <span class="math inline">\(\mathcal{L}_{txt}\)</span> 和分割掩码损失 <span class="math inline">\(\mathcal{L}_{mask}\)</span> 进行端到端训练。总体目标 <span class="math inline">\(L\)</span> 是这些损失的加权和，由 <span class="math inline">\(\lambda_{txt}\)</span> 和 <span class="math inline">\(\lambda_{mask}\)</span> 确定<span class="math display">\[\mathcal{L}=\lambda_{txt}\mathcal{L}_{txt}+\lambda_{mask}\mathcal{L}_{mask}.\]</span> ## 训练 ### 数据集 训练数据由三部分组成，都是开源数据集 1. Semantic Segmentation Dataset 2. Vanilla Referring Segmentation Dataset 3. Visual Question Answering Dataset</p><p><strong>值得注意的是，LISA具有zero-shot能力，因为训练集不包含任何推理分割的内容。</strong></p><h3 id="需要训练的参数">需要训练的参数</h3><p>为了保持llm的泛化能力作者用了lora,解码器可以被微调，llm的词嵌入和投影最后一层潜入的mlp也可以微调</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;lisa推理分割笔记&quot;&gt;LISA(推理分割)笔记&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;这篇论文题目中文翻译是 基于大型语言模型的语义分割， 提出了一个新任务-推理分割。大概就是给一张图和一段话，模型使用大语言模型分割出目标。作者给了一个例子，从</summary>
      
    
    
    
    <category term="笔记" scheme="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="多模态" scheme="https://studyinglover.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>在终端绘制GPU显存使用曲线</title>
    <link href="https://studyinglover.com/2023/08/13/%E5%9C%A8%E7%BB%88%E7%AB%AF%E7%BB%98%E5%88%B6GPU%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E6%9B%B2%E7%BA%BF/"/>
    <id>https://studyinglover.com/2023/08/13/%E5%9C%A8%E7%BB%88%E7%AB%AF%E7%BB%98%E5%88%B6GPU%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E6%9B%B2%E7%BA%BF/</id>
    <published>2023-08-13T11:44:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在终端绘制gpu显存使用曲线">在终端绘制GPU显存使用曲线</h1><p>这个东西的灵感来自于写torch的时候想实时看到loss和gpu使用情况，突然想到可以在终端实时显示，经过与ai的一番激烈讨，最终有了这个代码。</p><p>我们首先要获取GPU的显存使用数据，先检查是否安装了<code>nvidia-smi</code>, 在终端输入有正常输出即可。</p><p>首先导入所有需要的库 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> asciichartpy<br><span class="hljs-keyword">import</span> platform<br></code></pre></td></tr></table></figure></p><p>通过<code>nvidia-smi</code> 的命令获取已经使用的显存和所有现存 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpu_used_memory</span>():<br>output = subprocess.check_output([<span class="hljs-string">&#x27;nvidia-smi&#x27;</span>, <span class="hljs-string">&#x27;--query-gpu=memory.used&#x27;</span>, <span class="hljs-string">&#x27;--format=csv,nounits&#x27;</span>])<br>output = output.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>lines = output.strip().split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>used_memory = <span class="hljs-built_in">int</span>(lines[<span class="hljs-number">1</span>])<br><span class="hljs-keyword">return</span> used_memory<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpu_total_memory</span>():<br>output = subprocess.check_output([<span class="hljs-string">&#x27;nvidia-smi&#x27;</span>, <span class="hljs-string">&#x27;--query-gpu=memory.total&#x27;</span>, <span class="hljs-string">&#x27;--format=csv,nounits&#x27;</span>])<br>output = output.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>lines = output.strip().split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>total_memory = <span class="hljs-built_in">int</span>(lines[<span class="hljs-number">1</span>])<br><span class="hljs-keyword">return</span> total_memory<br></code></pre></td></tr></table></figure></p><p><code>asciichartpy</code> 是一个 Python 库，用于在终端中绘制 ASCII 图表。我们用他来在终端绘制图标。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_gpu_memory</span>(<span class="hljs-params">gpu_memory_history</span>):<br>    used_memory = get_gpu_used_memory()<br>    total_memory = get_gpu_total_memory()<br><br>    used_percentage = used_memory / total_memory * <span class="hljs-number">100</span><br>    gpu_memory_history.append(used_percentage)<br><br>    <span class="hljs-comment"># 绘制字符图表</span><br>    chart = asciichartpy.plot(gpu_memory_history, &#123;<span class="hljs-string">&#x27;height&#x27;</span>: <span class="hljs-number">20</span>, <span class="hljs-string">&#x27;width&#x27;</span>: <span class="hljs-number">10</span>, <span class="hljs-string">&#x27;timestamp&#x27;</span>: <span class="hljs-literal">True</span>&#125;)<br>    <br>    <span class="hljs-comment"># 清空终端屏幕</span><br>    <span class="hljs-keyword">if</span> platform.system() == <span class="hljs-string">&#x27;Windows&#x27;</span>:<br>        subprocess.call(<span class="hljs-string">&#x27;cls&#x27;</span>, shell=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">else</span>:<br>        subprocess.call(<span class="hljs-string">&#x27;clear&#x27;</span>, shell=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-built_in">print</span>(chart)<br></code></pre></td></tr></table></figure></p><p>最后运行上面的代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    <span class="hljs-keyword">try</span>:<br>        gpu_memory_history = []<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            draw_gpu_memory(gpu_memory_history)<br>            time.sleep(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">except</span> KeyboardInterrupt:<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure> 运行效果 <img src="https://cdn.studyinglover.com/pic/2023/08/c320d69a8169e36fab4c82f1725c298b.png" alt="image.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;在终端绘制gpu显存使用曲线&quot;&gt;在终端绘制GPU显存使用曲线&lt;/h1&gt;
&lt;p&gt;这个东西的灵感来自于写torch的时候想实时看到loss和gpu使用情况，突然想到可以在终端实时显示，经过与ai的一番激烈讨，最终有了这个代码。&lt;/p&gt;
&lt;p&gt;我们首先要获取GPU的显存</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>GPTBot介绍</title>
    <link href="https://studyinglover.com/2023/08/11/GPTBot%E4%BB%8B%E7%BB%8D/"/>
    <id>https://studyinglover.com/2023/08/11/GPTBot%E4%BB%8B%E7%BB%8D/</id>
    <published>2023-08-11T20:58:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gptbot介绍">GPTBot介绍</h1><p>最近，openai公布了<a href="https://platform.openai.com/docs/gptbot/gptbot">GPTBot</a> 的相关信息，并给出了禁止GPTBot的方法。以下是全文翻译。</p><p>GPTBot是OpenAI的网络爬虫，可以通过以下User agent和字符串来识别。 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">User</span> agent token: GPTBot<br><span class="hljs-attribute">Full</span> user-agent string: Mozilla/<span class="hljs-number">5</span>.<span class="hljs-number">0</span> AppleWebKit/<span class="hljs-number">537</span>.<span class="hljs-number">36</span> (KHTML, like Gecko; compatible; GPTBot/<span class="hljs-number">1</span>.<span class="hljs-number">0</span>; +https://openai.com/gptbot)<br></code></pre></td></tr></table></figure></p><h2 id="使用">使用</h2><p>使用 GPTBot 用户代理爬取的网页可能会用于改进未来的模型，并且会过滤掉需要付费访问、已知收集个人身份信息（PII）或含有违反我们政策的文本的来源。允许 GPTBot 访问您的网站可以帮助 AI 模型变得更准确，提高它们的一般能力和安全性。在下面，我们还分享了如何禁止 GPTBot 访问您的网站。</p><h3 id="禁止-gptbot">禁止 GPTBot</h3><p>要禁止 GPTBot 访问您的网站，您可以将 GPTBot 添加到您网站的 robots.txt： <figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-attribute">User-agent</span><span class="hljs-punctuation">: </span>GPTBot<br><span class="hljs-attribute">Disallow</span><span class="hljs-punctuation">: </span>/<br></code></pre></td></tr></table></figure></p><h3 id="自定义-gptbot-访问">自定义 GPTBot 访问</h3><p>要允许 GPTBot 仅访问您网站的部分内容，您可以将 GPTBot 令牌添加到您网站的 robots.txt，如下所示： <figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">User-agent: GPTBot<br>Allow: <span class="hljs-regexp">/directory-1/</span><br>Disallow: <span class="hljs-regexp">/directory-2/</span><br></code></pre></td></tr></table></figure></p><h3 id="ip-出口范围">IP 出口范围</h3><p>对于 OpenAI 的爬虫，它会从 <a href="https://openai.com/gptbot-ranges.txt">OpenAI 网站</a>上记录的 IP 地址段向网站发出请求。</p><p>这里我给出IP 地址段 <figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">20.15.240.64</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.80</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.96</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.240.176</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.241.0</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.128</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.144</span>/<span class="hljs-number">28</span><br><span class="hljs-number">20.15.242.192</span>/<span class="hljs-number">28</span><br><span class="hljs-number">40.83.2.64</span>/<span class="hljs-number">28</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;gptbot介绍&quot;&gt;GPTBot介绍&lt;/h1&gt;
&lt;p&gt;最近，openai公布了&lt;a href=&quot;https://platform.openai.com/docs/gptbot/gptbot&quot;&gt;GPTBot&lt;/a&gt; 的相关信息，并给出了禁止GPTBot的方法。以下是</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>arch蓝牙无法连接</title>
    <link href="https://studyinglover.com/2023/08/10/arch%E8%93%9D%E7%89%99%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/"/>
    <id>https://studyinglover.com/2023/08/10/arch%E8%93%9D%E7%89%99%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/</id>
    <published>2023-08-10T17:18:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="arch蓝牙无法连接">arch蓝牙无法连接</h1><p>在arcchlinux成功安装并且已经安装蓝牙的相关包之后，在设置打开蓝牙发现需要先开启蓝牙。</p><p>没啥好的解决办法，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl <span class="hljs-built_in">enable</span>  --now bluetooth <br></code></pre></td></tr></table></figure> 问题解决。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;arch蓝牙无法连接&quot;&gt;arch蓝牙无法连接&lt;/h1&gt;
&lt;p&gt;在arcchlinux成功安装并且已经安装蓝牙的相关包之后，在设置打开蓝牙发现需要先开启蓝牙。&lt;/p&gt;
&lt;p&gt;没啥好的解决办法，运行 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;ta</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>GPU部署llama-cpp-python(llama.cpp通用)</title>
    <link href="https://studyinglover.com/2023/08/06/GPU%E9%83%A8%E7%BD%B2llama-cpp-python(llama.cpp%E9%80%9A%E7%94%A8)/"/>
    <id>https://studyinglover.com/2023/08/06/GPU%E9%83%A8%E7%BD%B2llama-cpp-python(llama.cpp%E9%80%9A%E7%94%A8)/</id>
    <published>2023-08-06T23:01:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gpu部署llama-cpp-pythonllama.cpp通用">GPU部署llama-cpp-python(llama.cpp通用)</h1><h2 id="通用流程">通用流程</h2><p>我们的安装平台是Ubuntu20.04，Python 3.8.10，cuda 11.6。</p><p>首先确保自己是否已经安装了cuda,输入 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -V<br></code></pre></td></tr></table></figure></p><p>有类似下面的输出即可 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">nvcc</span>: NVIDIA (R) Cuda compiler driver<br><span class="hljs-attribute">Copyright</span> (c) <span class="hljs-number">2005</span>-<span class="hljs-number">2021</span> NVIDIA Corporation<br><span class="hljs-attribute">Built</span> <span class="hljs-literal">on</span> Fri_Dec_17_18:<span class="hljs-number">16</span>:<span class="hljs-number">03</span>_PST_2021<br><span class="hljs-attribute">Cuda</span> compilation tools, release <span class="hljs-number">11</span>.<span class="hljs-number">6</span>, V11.<span class="hljs-number">6</span>.<span class="hljs-number">55</span><br><span class="hljs-attribute">Build</span> cuda_11.<span class="hljs-number">6</span>.r11.<span class="hljs-number">6</span>/compiler.<span class="hljs-number">30794723</span>_0<br></code></pre></td></tr></table></figure></p><p>我们选用 <code>cuBLAS</code> 加速后端代理。直接按照下面命令安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> LLAMA_CUBLAS=1<br>CMAKE_ARGS=<span class="hljs-string">&quot;-DLLAMA_CUBLAS=on&quot;</span> FORCE_CMAKE=1 pip install llama-cpp-python<br></code></pre></td></tr></table></figure></p><p>不出意外的话就安装好了，但是你会出现很多意外，请你努力在一堆红色的报错中找出关键出错点，然后搜索，在最后我给出了几个我遇到的。</p><h2 id="运行">运行</h2><p>运行和CPU直接运行相似，只是需要加入几个参数. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model llama-2-70b-chat.ggmlv3.q5_K_M.bin --n_threads 30 --n_gpu_layers 200<br></code></pre></td></tr></table></figure></p><p><code>n_threads</code> 是一个CPU也有的参数，代表最多使用多少线程。</p><p><code>n_gpu_layers</code> 是一个GPU部署非常重要的一步，代表大语言模型有多少层在GPU运算，如果你的显存出现 <code>out of memory</code> 那就减小 <code>n_gpu_layers</code></p><h2 id="关于多卡">关于多卡</h2><p>亲测多卡没有遇到什么大坑，只要<code>torch.cuda.is_available()</code> 和<code>torch.cuda.device_count()</code>正常就可以跑起来。</p><p>两张 Tesla T4 的卡推理70B大概半分钟就可以出结果。</p><h2 id="报错解决">报错解决</h2><h3 id="check-for-working-cuda-compiler-usrlocalcudabinnvcc---skipped">Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped</h3><p>参考 https://github.com/ggerganov/llama.cpp/issues/1832 系统安装过程中没找到你的cuda在哪里，所以在pip安装之前先设置一个环境变量,<strong>把/usr/local/cuda-x.y改成你的cuda路径</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> CUDA_PATH=/usr/local/cuda-x.y<br></code></pre></td></tr></table></figure></p><h3 id="f16c-expected-a-number">'f16c': expected a number</h3><p>这是你的cuda版本太低了，升级到较新版本(11.6可用)。</p><p>或者参考 https://github.com/ggerganov/llama.cpp/issues/1467 和 https://github.com/marella/ctransformers/issues/53 中提到的命令和构建(我没有尝试，有谁试了可以请我结果)。</p><h3 id="value-sm_30-is-not-defined-for-option-gpu-name-tesla-t">Value 'sm_30' is not defined for option 'gpu-name' Tesla T</h3><p>先运行下面的命令 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-cache policy nvidia-cuda-toolkit<br></code></pre></td></tr></table></figure> 如果版本是<strong>1.0</strong> 那么请运行 <code>sudo apt remove nvidia-cuda-toolkit</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;gpu部署llama-cpp-pythonllama.cpp通用&quot;&gt;GPU部署llama-cpp-python(llama.cpp通用)&lt;/h1&gt;
&lt;h2 id=&quot;通用流程&quot;&gt;通用流程&lt;/h2&gt;
&lt;p&gt;我们的安装平台是Ubuntu20.04，Python 3.8.</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>花式求GCD</title>
    <link href="https://studyinglover.com/2023/08/02/%E8%8A%B1%E5%BC%8F%E6%B1%82GCD/"/>
    <id>https://studyinglover.com/2023/08/02/%E8%8A%B1%E5%BC%8F%E6%B1%82GCD/</id>
    <published>2023-08-02T18:46:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="花式求gcd">花式求GCD</h1><p>今天学校实验室纳新群有同学提到了<code>a^=b^=a^=b​</code> 交换两个数的操作，我突然想到之前在知乎看到通过异或实现gcd的方法，一番翻找后没啥结果，便去问了下认识的oi大佬有没有一行求gcd的算法。</p><p>大佬很快给出了一个函数<code>int gcd(int a,int b)&#123;return y?gcd(y,x%y):x;&#125;</code> 真的就是一行，完整的代码就是下面这个</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">gcd</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span> </span>&#123; <span class="hljs-keyword">return</span> y ? <span class="hljs-built_in">gcd</span>(y, x % y) : x; &#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-type">int</span> a,b;<br>a=<span class="hljs-number">10</span>;<br>b=<span class="hljs-number">20</span>;<br>a = <span class="hljs-built_in">gcd</span>(a,b);<br>cout&lt;&lt;a&lt;&lt;endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>但是我一像不对啊，我的异或呢？我又问了一下，大佬给了我一个截图 <img src="https://cdn.studyinglover.com/pic/2023/08/07b57e65da92d9c19bb82d740132f07c.png" /></p><p>就是这个神奇的写法</p><p>这段代码的实现方式是，使用异或运算符（^）和取模运算符（%）来交换变量a和b的值。具体来说，代码中的while循环会一直执行，直到b的值为0为止。在每次循环中，代码会先将a对b取模，然后将结果赋值给a，接着将b对a取模，然后将结果赋值给b，最后使用异或运算符交换a和b的值。这样，当循环结束时，a和b的值就被成功地交换了。(来自copilot chat)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-type">int</span> a,b;<br>a=<span class="hljs-number">10</span>;<br>b=<span class="hljs-number">20</span>;<br><span class="hljs-keyword">while</span>(b^=a^=b^=a%=b);<br>cout&lt;&lt;a&lt;&lt;endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;花式求gcd&quot;&gt;花式求GCD&lt;/h1&gt;
&lt;p&gt;今天学校实验室纳新群有同学提到了&lt;code&gt;a^=b^=a^=b​&lt;/code&gt; 交换两个数的操作，我突然想到之前在知乎看到通过异或实现gcd的方法，一番翻找后没啥结果，便去问了下认识的oi大佬有没有一行求gcd的算法</summary>
      
    
    
    
    
    <category term="算法" scheme="https://studyinglover.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>使用llama构建一个蜜罐(前端)</title>
    <link href="https://studyinglover.com/2023/08/01/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%89%8D%E7%AB%AF)/"/>
    <id>https://studyinglover.com/2023/08/01/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%89%8D%E7%AB%AF)/</id>
    <published>2023-08-01T00:12:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用llama构建一个蜜罐前端">使用llama构建一个蜜罐(前端)</h1><p><img src="https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png" /> 在<a href="https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/">使用llama构建一个蜜罐(后端)</a> 中我们通过llama和flask构建了一个蜜罐的后端，通过将shell命令作为字段的一部分，让llama假装执行命令来防止蜜罐被攻破。那有了后端我们还需要一个前端命令行来让用户登陆并执行命令。</p><p>完整项目开源在了<a href="https://github.com/StudyingLover/llama-honeypot-python">GitHub</a></p><p>接下来，让我们来实现一个模拟ssh服务器，或者说实现一个ssh mock 然后执行命令的时候不让他真的执行同时改一下输出。</p><p><strong>等等？我们真的需要一个ssh mock 吗？</strong> 还是说，我们需要的是一个<strong>跑在终端的，长得很像终端的，能输入输出的，一个可交互的代码？</strong></p><p>哦，好像我们需要的只是一个可交互的代码，难道攻击方ssh上来了还能验证一下这是不是真的是终端？(我用了三天才想通这个问题)</p><p>so,工作量一下子减少了太多了 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> requests<br>  <br><span class="hljs-comment"># 禁用 Ctrl Z stty susp undef</span><br><span class="hljs-comment"># 启用 Ctrl Z stty susp ^Z</span><br><br>admin_key = <span class="hljs-string">&quot;123456&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_responce</span>(<span class="hljs-params">command</span>):<br><span class="hljs-keyword">if</span> (command == admin_key):<br>exit()<br>output = requests.post(<span class="hljs-string">&quot;http://127.0.0.1:9000/admin/&quot;</span>+command).json()<br><span class="hljs-keyword">return</span> output[<span class="hljs-string">&quot;message&quot;</span>]<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">attact_warning</span>():<br><span class="hljs-keyword">pass</span><br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">anti_attact</span>():<br><span class="hljs-keyword">pass</span><br>  <br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>attact_warning()<br>anti_attact()<br><span class="hljs-keyword">try</span>:<br>command = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;[root@ubuntu ~]$ &quot;</span>)<br><span class="hljs-built_in">print</span>(get_responce(command))<br>  <br><span class="hljs-keyword">except</span> KeyboardInterrupt:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>这里有几个点需要注意 1. 代码<strong>不能直接用于生产环境！！！请先完善细节并大量测试。本项目仅为学习使用，未经过专业人员测试</strong> 1. <code>admin_key</code>，这个变量的作用是让管理员能用终端，<strong>记得修改</strong>。如果你认为这种方法太low了或者可能被作为突破口，请修改或PR。 2. 接口地址，我这里是<code>http://127.0.0.1:9000/admin/</code> ，这里需要改成你的，建议先用postman或者apifox或者啥的测一下。 3. 入侵检测和反击模块需要你<strong>自己实现</strong>，毕竟这只是一个让你的蜜罐更安全的项目。</p><p>在三个终端分别运行llama服务器(图右终端)，蜜罐后端(图左终端)和蜜罐前端(图中终端)</p><figure><img src="https://cdn.studyinglover.com/pic/2023/07/dd31f63365b8a8657b1459f7fe883a36.png" alt="" /><figcaption>image.png</figcaption></figure><p>项目还有很多改进之处，在后面我也会进一步优化prompt和模型来获得更好的终端对话体验。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用llama构建一个蜜罐前端&quot;&gt;使用llama构建一个蜜罐(前端)&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png&quot;</summary>
      
    
    
    
    
    <category term="网络安全" scheme="https://studyinglover.com/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>使用llama构建一个蜜罐(后端)</title>
    <link href="https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/"/>
    <id>https://studyinglover.com/2023/07/29/%E4%BD%BF%E7%94%A8llama%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%9C%9C%E7%BD%90(%E5%90%8E%E7%AB%AF)/</id>
    <published>2023-07-29T17:52:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用llama构建一个蜜罐后端">使用llama构建一个蜜罐(后端)</h1><p><img src="https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png" /></p><p>完整项目开源在了<a href="https://github.com/StudyingLover/llama-honeypot-python">GitHub</a></p><p>众所周知，蜜罐是一个很有趣的东西，他是一种网络安全机制，旨在诱使攻击者攻击虚假的系统或应用程序，以便安全专业人员可以监视攻击者的行为并收集攻击者的信息。蜜罐通常是一台虚拟机或一台计算机，它看起来像一个真实的系统，但实际上是一个特意构建的系统，用于诱骗攻击者。攻击者在攻击蜜罐时，安全专业人员可以收集攻击者的信息，例如攻击者使用的工具、攻击者的IP地址、攻击者的攻击技术等等。这些信息可以帮助安全专业人员更好地了解攻击者的行为和意图，并采取相应的措施来保护真实的系统。</p><p>但是缺点很明显，不管我怎么做蜜罐终究是跑在真实的服务器上的，还是很可能被攻破，所以，我们能不能让ai模仿一个linux主机作为蜜罐？</p><p>今天早上看到了这个视频 https://b23.tv/pXiGNIK ， 他开源了一个使用chatGPT作为终端的代码，开源在<a href="gitee.com/cutecuteyu/chatgpt-honeypot">gitee</a> ，不幸的是我openai账户没钱了，但是，昨天我才写了<a href="https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/#%E6%90%AD%E5%BB%BA%E4%B8%8Eopenai%E6%8E%A5%E5%8F%A3%E5%85%BC%E5%AE%B9%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A5%E5%8F%A3">搭建与openai接口兼容的服务器接口</a>, 那么我就可以改造一下他的代码，使用llama作为后端</p><p>首先clone他的仓库 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> gitee.com/cutecuteyu/chatgpt-honeypot<br><span class="hljs-built_in">cd</span> ./chatgpt-honeypot<br></code></pre></td></tr></table></figure></p><p>同时安装依赖 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install openai<br></code></pre></td></tr></table></figure></p><p>接下来我们在<code>chatgpt-honeypot</code>目录下创建一个 <code>.env</code> 文件，写上接口路径 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs .env">export OPENAI_API_BASE = http://localhost:8000/v1<br></code></pre></td></tr></table></figure></p><p>然后修改<code>myopenaiapikey.py</code> 文件，在第二行的<code>api=""</code> 中双引号随便填入一点东西。</p><p>下面修改<code>honeypot.py</code> ，因为我们的后端换成了llama,那么我们的prompt也需要更改,这里借鉴了<a href="https://github.com/Coldwave96/llama-honeypot">这个项目</a> ,将<code>chat2</code> 函数改成下面的内容 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">chat2</span>(<span class="hljs-params">query</span>):<br>response = openai.ChatCompletion.create(<br>model=<span class="hljs-string">&quot;gpt-3.5-turbo-0613&quot;</span>,<br>messages=[<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>,<br><span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">I want you to act as a Linux terminal. I will provide commands and history, then you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do no write explanations. Do not type commands unless I instruct you to do so.\n\n### Command:\n&#123;command&#125;\n\n### History:\n&#123;history&#125;\n### Response:\n</span><br><span class="hljs-string">&quot;&quot;&quot;</span>&#125;,<br>&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: query&#125;],<br>)<br>message = response[<span class="hljs-string">&quot;choices&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;message&quot;</span>]<br><span class="hljs-keyword">return</span> message<br></code></pre></td></tr></table></figure></p><p>启动项目，正常IDE运行或者在命令行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3  honeypot.py<br></code></pre></td></tr></table></figure></p><p>启动llama后端,将/path/to改成你的路径 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model  /path/to/llama-2-13b-chat.ggmlv3.q4_1.bin<br></code></pre></td></tr></table></figure></p><p>在浏览器访问<code>http://127.0.0.1:9000/admin/ls</code>,看到浏览器显示<code>/home/user/Documents/project</code> 类似的内容说明运行成功。</p><p>项目当然还有很多可以改进的地方，例如使用更好的prompt,或者微调llama作为后端，留给大家继续探索。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用llama构建一个蜜罐后端&quot;&gt;使用llama构建一个蜜罐(后端)&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.studyinglover.com/pic/2023/07/e9a49d4a404ed9bc4b0f119249194e3d.png&quot;</summary>
      
    
    
    
    
    <category term="网络安全" scheme="https://studyinglover.com/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>llama-cpp-python快速上手</title>
    <link href="https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <id>https://studyinglover.com/2023/07/28/llama-cpp-python%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</id>
    <published>2023-07-28T17:23:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="llama-cpp-python快速上手">llama-cpp-python快速上手</h1><h2 id="搭建环境">搭建环境</h2><p>项目地址<a href="https://github.com/abetlen/llama-cpp-python">GitHub</a>,有能力的话可以直接阅读原始文档。</p><p>首先按照文档，安装llama-cpp-python <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install llama-cpp-python<br></code></pre></td></tr></table></figure></p><p>接下来，你可能缺一些依赖，这一点在文档中没有涉及但是我整理了我缺少的依赖，依次运行即可。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install uvicorn<br>pip install anyio<br>pip install starlette<br>pip install fastapi<br>pip install pydantic_settings<br>pip install sse_starlette<br></code></pre></td></tr></table></figure></p><h2 id="高级api和低级api">高级API和低级API</h2><h3 id="高级api">高级API</h3><p>高级 API 通过<code>Llama</code>类提供简单的托管接口。请将<code>./models/7B/ggml-model.bin</code> 换成你的模型的路径，下同。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_cpp <span class="hljs-keyword">import</span> Llama<br>llm = Llama(model_path=<span class="hljs-string">&quot;./models/7B/ggml-model.bin&quot;</span>)<br>output = llm(<span class="hljs-string">&quot;Q: Name the planets in the solar system? A: &quot;</span>, max_tokens=<span class="hljs-number">32</span>, stop=[<span class="hljs-string">&quot;Q:&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>], echo=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure> 返回值如下 <figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs arcade">&#123;<br>  <span class="hljs-string">&quot;id&quot;</span>: <span class="hljs-string">&quot;cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;</span>,<br>  <span class="hljs-string">&quot;object&quot;</span>: <span class="hljs-string">&quot;text_completion&quot;</span>,<br>  <span class="hljs-string">&quot;created&quot;</span>: <span class="hljs-number">1679561337</span>,<br>  <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;./models/7B/ggml-model.bin&quot;</span>,<br>  <span class="hljs-string">&quot;choices&quot;</span>: [<br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.&quot;</span>,<br>      <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-number">0</span>,<br>      <span class="hljs-string">&quot;logprobs&quot;</span>: <span class="hljs-built_in">None</span>,<br>      <span class="hljs-string">&quot;finish_reason&quot;</span>: <span class="hljs-string">&quot;stop&quot;</span><br>    &#125;<br>  ],<br>  <span class="hljs-string">&quot;usage&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;prompt_tokens&quot;</span>: <span class="hljs-number">14</span>,<br>    <span class="hljs-string">&quot;completion_tokens&quot;</span>: <span class="hljs-number">28</span>,<br>    <span class="hljs-string">&quot;total_tokens&quot;</span>: <span class="hljs-number">42</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></p><h3 id="低级api">低级API</h3><p>低级 API 直接<a href="https://docs.python.org/3/library/ctypes.html"><code>ctypes</code></a>绑定到<code>llama.cpp</code>. 整个低级 API 可以在<a href="https://github.com/abetlen/llama-cpp-python/blob/master/llama_cpp/llama_cpp.py">llama_cpp/llama_cpp.py</a>中找到，并直接镜像<a href="https://github.com/ggerganov/llama.cpp/blob/master/llama.h">llama.h</a>中的 C API 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> llama_cpp<br><span class="hljs-keyword">import</span> ctypes<br>params = llama_cpp.llama_context_default_params()<br><span class="hljs-comment"># use bytes for char * params</span><br>ctx = llama_cpp.llama_init_from_file(<span class="hljs-string">b&quot;./models/7b/ggml-model.bin&quot;</span>, params)<br>max_tokens = params.n_ctx<br><span class="hljs-comment"># use ctypes arrays for array params</span><br>tokens = (llama_cpp.llama_token * <span class="hljs-built_in">int</span>(max_tokens))()<br>n_tokens = llama_cpp.llama_tokenize(ctx, <span class="hljs-string">b&quot;Q: Name the planets in the solar system? A: &quot;</span>, tokens, max_tokens, add_bos=llama_cpp.c_bool(<span class="hljs-literal">True</span>))<br>llama_cpp.llama_free(ctx)<br></code></pre></td></tr></table></figure><h2 id="搭建与openai接口兼容的服务器接口">搭建与openai接口兼容的服务器接口</h2><p><code>llama-cpp-python</code>提供一个 Web 服务器，旨在作为 OpenAI API 的直接替代品。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 -m llama_cpp.server --model models/7B/ggml-model.bin<br></code></pre></td></tr></table></figure> 你可以在上面的命令运行成功后访问<a href="http://localhost:8000/docs">文档</a></p><p>文档是全英的，想要对话接口的话我用python写了个示例 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br>  <br>url = <span class="hljs-string">&#x27;http://localhost:8000/v1/chat/completions&#x27;</span><br>headers = &#123;<br><span class="hljs-string">&#x27;accept&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span>,<br><span class="hljs-string">&#x27;Content-Type&#x27;</span>: <span class="hljs-string">&#x27;application/json&#x27;</span><br>&#125;<br>data = &#123;<br><span class="hljs-string">&#x27;messages&#x27;</span>: [<br>&#123;<br><span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;You are a helpful assistant.&#x27;</span>,<br><span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span><br>&#125;,<br>&#123;<br><span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;What is the capital of France?&#x27;</span>,<br><span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span><br>&#125;<br>]<br>&#125;<br>  <br>response = requests.post(url, headers=headers, json=data)<br><span class="hljs-built_in">print</span>(response.json())<br><span class="hljs-built_in">print</span>(response.json()[<span class="hljs-string">&#x27;choices&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>])<br></code></pre></td></tr></table></figure></p><p>如果你想自建一个接口，请在遵守相关法律法规的情况下，在自己的服务器上启动相关服务，并反向代理<code>http://localhost:8000</code> 地址。例如你反向代理到了<code>https://example.com</code>,那你的对话地址就是<code>https://example.com/v1/chat/completions</code>。当你想用gpt的时候就不用看openai的脸色了，直接部署一个自己的接口自己请求，或者调用openai库的时候apibase写自己的接口。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;llama-cpp-python快速上手&quot;&gt;llama-cpp-python快速上手&lt;/h1&gt;
&lt;h2 id=&quot;搭建环境&quot;&gt;搭建环境&lt;/h2&gt;
&lt;p&gt;项目地址&lt;a href=&quot;https://github.com/abetlen/llama-cpp-python&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>快速上手llama2.c(更新版)</title>
    <link href="https://studyinglover.com/2023/07/28/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c(%E6%9B%B4%E6%96%B0%E7%89%88)/"/>
    <id>https://studyinglover.com/2023/07/28/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c(%E6%9B%B4%E6%96%B0%E7%89%88)/</id>
    <published>2023-07-28T16:31:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速上手llama2.c更新版">快速上手llama2.c(更新版)</h1><p>在上一次我同时在我的博客和知乎发布了<a href="https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/">快速上手llama2.c</a> 之后，我一个小透明也收获了不少收藏，并收到了人生中第一个这样的留言(其实我感觉是机器人)。 <img src="https://cdn.studyinglover.com/pic/2023/07/2eda3b2dcb8d68fc01169f5366c8157c.jpg" /></p><p>当然，之前的llama2.c也有一些不好的地方，例如不能添加自己的prompt,所以我提了这样的一个<a href="https://github.com/karpathy/llama2.c/issues/64">issue</a>,今天收到了贡献者的回复说是可以用了。那我们来看一下。</p><p>首先还是克隆整个仓库，编译并下载模型，这里以15m参数的模型作为示例 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/llama2.c.git<br><span class="hljs-built_in">cd</span> llama2.c<br>make run<br>wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin<br></code></pre></td></tr></table></figure></p><p>接下来我们就可以使用编译出来的<code>run</code> 运行了,要使用自己的prompt,需要指定温度和 步长，这里温度设置成1.0,步长设置256,prompt在双引号写，我这里写的是<code>One day morning , I don't want to go to school</code> . <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./run stories15M.bin 1.0 256 <span class="hljs-string">&quot;One day morning , I don&#x27;t want to go to school&quot;</span><br></code></pre></td></tr></table></figure></p><p>这里给出我的运行结果，也就3秒种不到 <figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs tp">&lt;s&gt;<br>One day morning , I don<span class="hljs-string">&#x27;t want to go to school, so he packed his trunk lid to pack. memorized his chores, he thought about what his mom would like him to stay home and not do all day. She wanted him to in a very competitive way.</span><br><span class="hljs-string">&quot;Come and play in the puddle, it&#x27;</span>ll be more fun<span class="hljs-comment">!&quot;He begged.</span><br><span class="hljs-comment">Mom shook her head. &quot;No, we haven&#x27;t seen coming for sure,&quot; she said thought. </span><br><span class="hljs-comment">Thumper and Mom just shrugged.</span><br><span class="hljs-comment">&quot;See,&quot; she said. &quot;Come on now. Let&#x27;s go and find some fun ways to clean the world!&quot;</span><br><span class="hljs-comment">The little boy was relieved and ran out to the yard. He had found a great idea to share his day with his mom instead. They scattered around the yard and had fun playing until their tired eyes were aching.</span><br><span class="hljs-comment">&lt;s&gt;</span><br><span class="hljs-comment">Once upon a time, there was a little boy named Tim. Tim was very excited because he was going on a trip with his family. He saw a big bus that helped them get off at their destination.</span><br><span class="hljs-comment">As the bus drove along, Tim noticed an unusual looking man sitting next to it. Tim asked the</span><br><span class="hljs-comment">achieved tok/s: 175.378267</span><br><span class="hljs-comment"></span><br></code></pre></td></tr></table></figure></p><p>当然为了获得更好的效果，我们可以使用更大模型</p><p>下载42m参数模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin<br></code></pre></td></tr></table></figure></p><p>下载110m参数模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;快速上手llama2.c更新版&quot;&gt;快速上手llama2.c(更新版)&lt;/h1&gt;
&lt;p&gt;在上一次我同时在我的博客和知乎发布了&lt;a href=&quot;https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%</summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>Paper Gestalt笔记</title>
    <link href="https://studyinglover.com/2023/07/27/Paper%20Gestalt%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/27/Paper%20Gestalt%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-27T10:57:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="paper-gestalt笔记">Paper Gestalt笔记</h1><p>最近读到了一篇CVPR2010非常优秀的论文，叫做<a href="https://bbabenko.github.io/assets/papers/paper_gestalt.pdf">Paper Gestalt</a> ,他考虑到近年来(2010年的近年来)CVPR的投稿两出现了大量增长，但是作者很可能接触到一个不优秀的审稿人，所以训练了一个视觉分类器来判断一篇CVPR的论文是否应该被接受来辅助审稿。当然模型效果非常优秀了，在误分类15%的goog paper (应该被接受)的情况下可以筛选掉50% bad paper。</p><p>在这项工作中，作者构建了一种简单的直觉，即一篇论文的质量可以通过浏览总体的视觉效果来估计，并使用这种直觉来构建一个系统，该系统使用基本的计算机视觉技术来预测论文是否应该被接受或拒绝。这个任务中具有判别能力的视觉特征集就被称为Paper Gestalt。</p><p>最有意思的一点是，作者训练出来的默认为认为他的论文有88.4%的可能被接受。</p><p>作者将这个任务认为是一个二分类任务<span class="math inline">\(\{(x_1,y_1),(x_2,y_2),...(x_n,y_n)\}\)</span> ,其中<span class="math inline">\(x_i\)</span> 是一个图片的视觉特征，<span class="math inline">\(y_i\)</span> 则是对论文的一个标签。</p><p>给定一篇论文的图像，需要计算可插入分类系统的视觉特征的数量。作者选择了一些标准的计算机视觉特征来捕捉渐变、纹理、颜色和纹理信息。特别是作者是基于LUV直方图、直方图的定向梯度和梯度幅度来计算特征。</p><p>作者选用了AdaBoost作为分类器，公式是<span class="math display">\[h(x)=\sum_{t=1}^T\alpha_th_t(x)\]</span> <span class="math inline">\(h_t\)</span>就是一个弱分类器，这里选用的是决策树<span class="math inline">\(h_t(x)=\mathbf{1}[f_t(x)&gt;\theta]\)</span> ,<span class="math inline">\(\theta\)</span> 是阈值，<span class="math inline">\(f_t\)</span> 是图像特征，整体的训练流程如图所示。(实话实话，对于我这种2020年才接触深度学习的人来说AdaBoost真的是老古董技术了(ง •̀_•́)ง，只在计算机视觉课上听过这种技术用于人脸检测) <img src="https://cdn.studyinglover.com/pic/2023/07/7230c1fa1d43d4fb676127135aef728f.png" alt="image.png" /></p><p>AdaBoost有许多吸引人的理论特性。例如，众所周知，经验误差是有界的<span class="math display">\[\epsilon(h)\leq\prod_{t=1}^T2\sqrt{\epsilon_t(1-\epsilon_t)}\]</span> 虽然这个公式摆在这没有任何用，但是作者发现数学公式多了有利于论文被接受，所以他又摆上了 Maxwell’s equations <span class="math display">\[\begin{array}{rcl}\oint\vec{E}\cdot d\vec{A}&amp;=&amp;\frac{Q_{enc}}{\epsilon_0}\\&amp;&amp;\\\oint\vec{B}\cdot d\vec{A}&amp;=&amp;0\\&amp;&amp;&amp;\\\oint\vec{E}\cdot d\vec{s}&amp;=&amp;-\frac{d\phi_B}{dt}\\\oint\vec{B}\cdot d\vec{s}&amp;=&amp;\mu_0\epsilon_0\frac{d\phi_E}{dt}+\mu_0i_{enc}\end{array}\]</span> 哦你问视觉分类器跟Maxwell’s equations 到底有啥关系？这就是这篇论文的结论部分了，作者使用了一些论文作为例子分析了效果。 <img src="https://cdn.studyinglover.com/pic/2023/07/c29f925390f8307701c7206b71e177bb.png" alt="image.png" /> <img src="https://cdn.studyinglover.com/pic/2023/07/698b7a4ae9b5fa5751a2b562f4bad18a.png" alt="image.png" /> 我们从作者给出的图可以发现，一篇被接受的论文有数学公式，有图表还有图像，而被拒的论文有令人困惑的大表格，缺少页数还有缺少五颜六色的图片。</p><p>说到令人困惑的大表格不知道你有没有想到一篇论文，对就是我们巨有钱的OPENAI做的CLIP。这表格属实看的人眼睛疼，被显卡的钱亮瞎了狗眼。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420145907.png" alt="image.png" /></p><p> 作者还不忘了夸一下他的论文，说他的固然存在缺页/空白页的问题，但其色彩斑斓的图表和令人印象深刻的数学公式构成非常漂亮。问题是你这图也不对呀，有的图片位置都和最终论文不一样。  <img src="https://cdn.studyinglover.com/pic/2023/07/86016048d0e76fde6f121419d1a3f0a4.png" alt="image.png" /></p><p>还有一点需要指出的是，作者的模型分析一篇论文只需要0.5秒。</p><p>在我找原文的时候，我发现arXiv上挂了一篇18年的文章<a href="https://arxiv.org/abs/1812.08775">Deep Paper Gestalt</a> ,据说他训练的模型把自己拒掉了。按照这个趋势我是不是可以搞一篇论文叫做Paper Gestalt with Latent Space?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;paper-gestalt笔记&quot;&gt;Paper Gestalt笔记&lt;/h1&gt;
&lt;p&gt;最近读到了一篇CVPR2010非常优秀的论文，叫做&lt;a href=&quot;https://bbabenko.github.io/assets/papers/paper_gestalt.pd</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>DINO-v2笔记</title>
    <link href="https://studyinglover.com/2023/07/27/DINO-v2%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/27/DINO-v2%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-27T00:04:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dino-v2笔记">DINO-v2笔记</h1><p>DINO-v2一种无监督学习的预训练方法，可以生成具有强大泛化能力的视觉特征，适用于各种图像分布和任务，而无需进行微调。这篇论文重点介绍了数据和模型规模方面的技术贡献，包括自动构建一个多样化和精心筛选的图像数据集、在多个层级上进行训练、使用Sinkhorn-Knopp居中方法和KoLeo正则化等。实验结果表明，该方法在多个图像理解任务上的表现超过了目前公开的最佳无监督和半监督方法。</p><p>作者实际上花了大量的篇幅减少了数据如何创建，如何进行预训练和如何优化训练过程。</p><p><a href="https://dinov2.metademolab.com/">项目主页</a>,项目开源在<a href="https://github.com/facebookresearch/dinov2">GitHub</a></p><h2 id="数据集准备">数据集准备</h2><p>作者通过从一个大型未筛选数据池中检索与几个精选数据集中的图像接近的图像来组装他们的LVD-142M数据集。作者描述了数据处理流程的主要组成部分，包括精选/未筛选数据源，图像去重步骤和检索系统。整个流程不需要任何元数据或文本，直接使用图像。</p><p>整个处理分布在一个由20个节点组成的计算集群上，该集群配备了8个V100-32GB GPU，生成LVD-142M数据集不到两天。</p><h3 id="数据来源">数据来源</h3><p>作者在包含 ImageNet-22k，ImageNet-1k、Google Landmarks 和几个细粒度数据集的的数据集进行选择。对于不安全的数据源，爬取公开可用的网络数据存储库中收集了原始未过滤的图像数据集。从存储库中的每个网页中，作者从<img>标签中提取图像的 URL 链接。作者在构建数据集过程中丢弃了不受域限制或限制的 URL，并对下载的图像（PCA 哈希重复数据删除、NSFW 过滤和模糊可识别人脸）进行后处理。这导致 1.2B 个独特的图像。</p><h3 id="消除重复数据">消除重复数据</h3><p>作者将使用了A Self-Supervised Descriptor for Image Copy Detection 这篇论文中的方法来处理未经处理的数据，并去除接近重复的图像。这减少了冗余并增加了图像之间的多样性。此外还删除了这个工作中使用的任何基准测试或验证集中包含的几乎重复的图像。</p><h3 id="自监督的图像检索">自监督的图像检索</h3><p>首先使用在ImageNet-22k上预训练的自监督ViT-H/16网络来计算图像嵌入，并使用余弦相似性作为图像之间的距离度量。接下来对未分级的数据进行k-means聚类。给定要检索的查询数据集，如果它足够大，那么就为每个查询图像检索N个（通常是4个）最近的邻居。如果它很小，就从与每个查询图像相对应的聚类中采样M个图像。可以通过目视检查检索结果来调整N和M。</p><h2 id="判别式自监督的预培训">判别式自监督的预培训</h2><h3 id="图像级目标">图像级目标</h3><p>同一图像的不同裁剪中获得不同的部分，使用ViT进行编码，用过去迭代的指数移动平均值构建教师模型，从学生和教师网络中提取的特征之间的交叉熵损失学习学生模型的参数</p><h3 id="patch级目标">patch级目标</h3><p>随即屏蔽给学生的一些输入补丁，但不屏蔽给老师的。然后，我们在每个屏蔽补丁上的两个网络的补丁特征之间添加交叉熵损失。这种损失与图像级别的损失相结合。</p><h3 id="解绑两个目标的权重联系">解绑两个目标的权重联系</h3><p>将上面两个目标相关的权重捆绑在一起会使模型在patch上欠拟合，而在图像级别上过拟合。解开这些权重可以解决这个问题，并提高两个目标的性能。</p><h3 id="sinkhorn-knopp-centering">Sinkhorn-Knopp centering</h3><p>这是一种替代DINO和iBot模型中的teacher softmax-centering步骤的方法，即使用SwAV模型的Sinkhorn-Knopp（SK）批量归一化。作者在这个方法中运行了3次Sinkhorn-Knopp算法步骤，并对学生应用softmax归一化。这个方法的目的是提高自监督学习模型的性能。</p><h3 id="koleo-regularizer">KoLeo regularizer</h3><p>KoLeo正则化器源自Kozachenko-Leonenko差分熵估计器，它鼓励批处理中特征的均匀跨度。给定一组n个向量(x1, . . . , xn)，它被定义为<span class="math inline">\(\mathcal{L}_\text{koleo}=-\frac1n\sum_{i=1}^n\log(d_{n,i})\)</span> ，其中<span class="math inline">\(d_{n,i}=\min_{j\neq i}\left\|x_i-x_j\right\|\)</span>是<span class="math inline">\(x_i\)</span>和批处理中任何其他点之间的最小距离。在计算这个正则化器之前，我们还要对特征进行L2-归一化。</p><h3 id="adapting-the-resolution">Adapting the resolution</h3><p>在像素级别的下游任务中，如分割或检测，提高图像分辨率是非常重要的，因为低分辨率下小物体容易消失。但是高分辨率的训练需要更多的时间和内存，所以作者提出了一种方法，在预训练的最后一段时间内将图像的分辨率提高到518×518。</p><h2 id="有效的实施">有效的实施</h2><p>作者对于训练大规模模型的几个改进措施，包括使用A100 GPU和PyTorch 2.0进行训练，提供代码和预训练模型，并在附录的Table 17中详细描述了模型的细节。</p><p>另外，与iBOT实现相比，DINOv2的代码在相同硬件条件下，运行速度提高了2倍，内存使用量减少了三分之一。</p><h3 id="快速高效的注意力">快速高效的注意力</h3><p>作者自己实现了一个fastattention,需要注意的是<strong>作者的ViT-g架构略有不同，采用1536的嵌入维度和24个头（每个头64维），而不是1408的嵌入维度和16个头（每个头88维），以最大化计算效率</strong>。</p><h3 id="自注意中的嵌套张量">自注意中的嵌套张量</h3><p>作者使用了一种新的技术，可以在同一个正向传递中运行全局裁剪和局部裁剪（具有不同数量的补丁标记），与之前的实现相比，可以获得显着的计算效率提升。此外，作者提到他们使用的基础组件已经在xFormers库中提供。</p><h3 id="有效的随机深度">有效的随机深度</h3><p>作者使用了一种改进的随机深度（stochastic depth）方法，相比于传统的掩码方法，该方法跳过了被丢弃的残差计算，从而在一定程度上节省了内存和计算资源。在本次实验中，使用高丢弃率（d=40%）时，这种方法使计算效率和内存使用效率得到了显著提高。具体实现方法是通过在批处理维度上随机重新排列B个样本，并在块计算中仅对前<span class="math inline">\((1-d)×B\)</span>个样本进行计算。</p><h3 id="完全共享数据并行fsdp">完全共享数据并行（FSDP）</h3><p>通过将模型副本分配到多个GPU中，可以将模型大小限制在GPU节点总内存的范围内。此外，FSDP的实现方式可以将权重片段存储为float32，但在传播权重和梯度时使用float16，从而降低跨GPU通信成本。相较于DistributedDataParallel（DDP）中使用的float32梯度all-reduce操作，使用Pytorch-FSDP混合精度训练的通信成本减少了约50％，在扩展GPU节点数量时训练过程更加高效。总的来说，Pytorch-FSDP混合精度训练在几乎所有情况下都优于使用autocast的DDP。</p><h3 id="模型蒸馏">模型蒸馏</h3><p>作者发现即使对于一个规模较大的ViT-L模型，他们的预训练方法也能够取得比从头开始训练更好的性能。此外，他们还提出了一种知识蒸馏方法，与A simple recipe for competitive low-compute self supervised vision models. arXiv preprint arXiv:2301.09451 所描述的方法相似，但没有修改蒸馏的损失项，并评估了学生模型的指数移动平均值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;dino-v2笔记&quot;&gt;DINO-v2笔记&lt;/h1&gt;
&lt;p&gt;DINO-v2一种无监督学习的预训练方法，可以生成具有强大泛化能力的视觉特征，适用于各种图像分布和任务，而无需进行微调。这篇论文重点介绍了数据和模型规模方面的技术贡献，包括自动构建一个多样化和精心筛选的图像</summary>
      
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/categories/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
    
  </entry>
  
  <entry>
    <title>快速上手llama2.c</title>
    <link href="https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/"/>
    <id>https://studyinglover.com/2023/07/25/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Bllama2.c/</id>
    <published>2023-07-25T16:19:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速上手llama2.c">快速上手llama2.c</h1><p><a href="https://github.com/karpathy/llama2.c.git">llama2.c</a>一个完整的解决方案，可以使用PyTorch从头开始训练的Llama 2 LLM（Lightweight Language Model）模型，并将权重导出为二进制文件，然后加载到一个简单的500行C文件（run.c）中进行推理。另外，你也可以加载、微调和推理Meta的Llama 2模型（但这部分仍在积极开发中）。因此，这个仓库提供了一个"全栈"的训练和推理方案，专注于极简和简洁性。你可能会认为只有拥有数十亿参数的LLM才能实现有用的功能，但事实上，如果领域足够狭窄，非常小的LLM也可以表现出惊人的性能。建议参考TinyStories论文以获得灵感。</p><p>需要注意的是，这个项目最初只是一个有趣的周末项目：作者在之前的nanoGPT基础上进行了调整，实现了Llama-2架构而不是GPT-2，并且主要的工作是编写了C推理引擎（run.c）。因此，这个项目还比较年轻，并且在快速发展中。特别感谢llama.cpp项目为此项目提供了灵感。作者希望保持超级简洁，所以选择了硬编码Llama 2架构，采用fp32精度，并仅使用纯C编写一个没有依赖项的推理文件。</p><p>首先clone整个仓库并编译 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/karpathy/llama2.c.git<br><span class="hljs-built_in">cd</span> llama.c<br>gcc -O3 -o run run.c -lm<br></code></pre></td></tr></table></figure></p><p>接下来下载模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://karpathy.ai/llama2c/model.bin -P out<br></code></pre></td></tr></table></figure></p><p>或者下载更大的一个模型 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://karpathy.ai/llama2c/model44m.bin -P out44m<br></code></pre></td></tr></table></figure></p><p>接下来进行推理 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./run out/model.bin<br></code></pre></td></tr></table></figure></p><p>我们将会看到这样一段输出就代表运行成功 <figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">&lt;s&gt;<br> One day, <span class="hljs-keyword">a</span> little otter named Ollie went <span class="hljs-built_in">to</span> play <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> river. Ollie was very compassionate. He loved <span class="hljs-built_in">to</span> help his friends <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> town.<br>While playing, Ollie saw <span class="hljs-keyword">a</span> big fish. The fish was stuck <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> mud. <span class="hljs-string">&quot;Help me, please!&quot;</span> said <span class="hljs-keyword">the</span> fish. Ollie wanted <span class="hljs-built_in">to</span> help <span class="hljs-keyword">the</span> fish. He swam away, looking <span class="hljs-keyword">for</span> something <span class="hljs-built_in">to</span> break <span class="hljs-keyword">the</span> mud.<br>Ollie found <span class="hljs-keyword">a</span> small stick. He used <span class="hljs-keyword">the</span> stick <span class="hljs-built_in">to</span> break <span class="hljs-keyword">the</span> mud. The fish was free! <span class="hljs-string">&quot;Thank you, Ollie!&quot;</span> <span class="hljs-keyword">the</span> fish said. The fish was happy <span class="hljs-keyword">and</span> swam away.<br>Ollie felt good <span class="hljs-keyword">for</span> helping <span class="hljs-keyword">the</span> fish. He went back <span class="hljs-built_in">to</span> play <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> river. Ollie knew that helping others made him feel good. And <span class="hljs-built_in">from</span> that day, Ollie was always compassionate <span class="hljs-built_in">to</span> everyone.<br>&lt;s&gt;<br> Tom was <span class="hljs-keyword">a</span> big boy who liked <span class="hljs-built_in">to</span> help his mom. He saw his mom doing laundry <span class="hljs-keyword">and</span> asked <span class="hljs-keyword">if</span> he could join. His mom said yes, but he had <span class="hljs-built_in">to</span> be careful <span class="hljs-keyword">with</span> <span class="hljs-keyword">the</span> iron. The iron was hot <span class="hljs-keyword">and</span> had <span class="hljs-keyword">a</span> button <span class="hljs-keyword">on</span> <span class="hljs-title">it</span>.<br>Tom took <span class="hljs-keyword">the</span> iron <span class="hljs-keyword">and</span> ran <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> house. He wanted <span class="hljs-built_in">to</span> iron his shirt<br>achieved tok/s: <span class="hljs-number">178.148921</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;快速上手llama2.c&quot;&gt;快速上手llama2.c&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/karpathy/llama2.c.git&quot;&gt;llama2.c&lt;/a&gt;一个完整的解决方案，可以使用PyTorch从头开始训练的Llama </summary>
      
    
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>AnyDoor笔记</title>
    <link href="https://studyinglover.com/2023/07/24/AnyDoor%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/24/AnyDoor%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-24T19:39:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="anydoor笔记">AnyDoor笔记</h1><p>在这项工作中，香港大学，阿里联合提出了提出了 AnyDoor，这是一种基于扩散的生成器，可以进行对象隐形传态。这项研究的核心贡献是使用判别 ID 提取器和频率感知细节提取器来表征目标对象。在视频和图像数据的不同组合上进行训练，我们在场景图像的特定位置合成对象。AnyDoor 为一般区域到区域的映射任务提供了通用解决方案，并且可以为各种应用有利可图。</p><p><a href="https://damo-vilab.github.io/AnyDoor-Page/">项目地址</a></p><p>AnyDoor的模型架构图如下图所示，看起来还是比较清晰的，我们一部分一部分来看 <img src="https://cdn.studyinglover.com/pic/2023/07/84e8fadaba321d5eb9be47f710d22997.png" alt="image.png" /></p><h2 id="id特征提取器">ID特征提取器</h2><p>一般都选择CLIP的图像编码器编码一个图像对象。但是CLIP 是由粗略描述的文本对训练而来的，所以CLIP 只能给出一些语义上的描述，但是很难对每个物体的特征做出分辨。所以作者做出了两点更新，背景移除和自监督的表示。</p><p>对应到整个pipeline就是这部分 <img src="https://cdn.studyinglover.com/pic/2023/07/e22b104405f6f2e4ace7a680c6d44e23.png" alt="image.png" /></p><h3 id="背景移除">背景移除</h3><p>背景移除就是使用一个分割模型将背景删除，然后将目标物体和背景中心对齐。可以使用一些自动的模型(例如Segment Anything),和可交互式的模型</p><h3 id="自监督的表示">自监督的表示</h3><p>作者说在这个工作中他们发现自监督的模型可以很好的保留物体的判别特征。在大规模数据集上进行预训练，自监督模型自然配备了实例检索能力，可以将对象投影到增强不变的特征空间(augmentation-invariant feature space，经过抱大佬大腿，这个特征空间是说经过图像增强语义不变)中。作者采用了DINO2作为编码器，得到了一个全局的特征<span class="math inline">\(\mathbf{T}_{\mathrm{g}}^{1 \times 1536}\)</span> 和一个局部的特征<span class="math inline">\(\mathbf{T}_{\mathrm{p}}^{256 \times 1536}\)</span> ,使用一个线性层将这两个向量投影到UNet需要的维度，然后合并俩个向量使用，最后的向量是<span class="math inline">\(\mathbf{T}_{\text {ID }}^{257 \times 1024}\)</span></p><h2 id="细节特征提取">细节特征提取</h2><p>作者认为，由于 ID 令牌会丢失空间分辨率，因此它们很难充分保持目标对象的精细细节。所以使用互补的细节作为生成过程中额外的指导。</p><p>使用拼贴作为控件可以提供强大的先验，作者尝试将“背景移除的对象”缝合到场景图像的给定位置。通过这个拼贴，可以观察到生成保真度的显着改进，但生成的结果与缺乏多样性的给定目标过于相似。面对这个问题，作者探索设置信息瓶颈以防止拼贴给出太多外观约束。实际上就是设计了一个高频映射来表示对象，它可以保持精细细节，但允许通用的局部变体，如手势、照明、方向等。</p><p>对应pipeline的这部分， <img src="https://cdn.studyinglover.com/pic/2023/07/02e880c65b826610ff0afc47e939fc40.png" alt="image.png" /></p><p>作者使用了这样一个公式来提取高频图<span class="math display">\[\mathbf{I}_h=\left(\mathbf{I} \otimes \mathbf{K}_h+\mathbf{I} \otimes \mathbf{K}_v\right) \odot \mathbf{I} \odot \mathbf{M}_{\text {erode }}\]</span> <span class="math inline">\(\mathbf{I}\)</span> 是一张RGB的图像(上一步背景移除得到的图片),<span class="math inline">\(\mathbf{K}_h\)</span> 和<span class="math inline">\(\mathbf{K}_v\)</span> 是水平和垂直Sobel kernel，这里被用作高频滤波器，<span class="math inline">\(\otimes\)</span>代表卷积,<span class="math inline">\(\odot\)</span>代表逐元素乘法。侵蚀掩码 <span class="math inline">\(\mathbf{M}_{\text {erode }}\)</span> 来过滤目标对象外部轮廓附近的信息。</p><p>在得到高频图后，根据给定的位置将其拼接到场景图像上，然后将拼贴传递给细节提取器。细节提取器是一个 ControlNet 中的UNet 编码器，它生成一系列具有分层分辨率的细节图。</p><h2 id="特征注入">特征注入</h2><p>在获得 ID 标记和细节图后，将它们注入到预训练的文本到图像扩散模型中以指导生成。作者选择了stable diffusion，它将图像投影到潜在空间中，并使用UNet进行概率采样。我们注意到预训练的 UNet 为 <span class="math inline">\(\hat{\mathbf{x}}_\theta\)</span> ，它从初始潜在噪声 <span class="math inline">\(\epsilon \sim \mathcal{U}([0,1])\)</span>开始去噪，并将文本嵌入 c 作为生成新图像潜在 的条件。训练监督是均方误差损失为<span class="math display">\[\mathbb{E}_{\mathbf{x},\mathbf{c},\epsilon,t}(\|\hat{\mathbf{x}}_\theta(\alpha_t\mathbf{x}+\sigma_t\epsilon,\mathbf{c})-\mathbf{x}\|_2^2)\]</span> <span class="math inline">\(\mathbf{x}\)</span> 是ground-truth,t是反向过程的步数,<span class="math inline">\(\alpha_t\)</span> <span class="math inline">\(\sigma_t\)</span> 是去噪的超参数。</p><p>在这项工作中，文本嵌入 c 被替换为前面的 ID 标记，这些标记通过交叉注意注入到每个 UNet 层。对于细节图，将它们与每个分辨率的 UNet 解码器特征连接起来。在训练期间，模型冻结 UNet 编码器的预训练参数以保留先验并调整 UNet 解码器以适应我们的新任务。</p><h2 id="训练策略">训练策略</h2><h3 id="图像文本对">图像文本对</h3><p>理想的训练样本是“不同场景中同一对象”的图像对，但是这些数据集不能直接由现有数据集提供。作为替代方案，以前的工作利用单个图像并应用旋转、翻转和弹性变换等增强。然而，这些幼稚的增强不能很好地代表姿势和视图的真实变体。</p><p>为了解决这个问题，在这项工作中，作者使用视频数据集来捕获包含相同对象的不同帧。</p><h3 id="自适应的训练步长">自适应的训练步长</h3><p>虽然视频数据有利于学习外观变化，但由于分辨率低或运动模糊，帧质量通常不能令人满意。相比之下，图像可以提供高质量的细节和通用的场景，但缺乏外观变化。</p><p>为了利用视频数据和图像数据，作者开发了自适应时间步采样，使不同模态的数据有利于去噪训练的不同阶段。stable dissusion为每个训练数据均匀地采样时间步长 (T)。然而，观察到初始去噪步骤主要集中在生成整体结构、姿势和视图；后面的步骤涵盖了纹理和颜色等精细细节 。因此，对于视频数据，可以增加了在训练期间采样早期去噪步骤（大 T）以更好地学习外观变化的可能性。对于图像，增加了后期步骤（小 T）的概率来学习如何覆盖精细细节。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;anydoor笔记&quot;&gt;AnyDoor笔记&lt;/h1&gt;
&lt;p&gt;在这项工作中，香港大学，阿里联合提出了提出了 AnyDoor，这是一种基于扩散的生成器，可以进行对象隐形传态。这项研究的核心贡献是使用判别 ID 提取器和频率感知细节提取器来表征目标对象。在视频和图像数据的</summary>
      
    
    
    
    <category term="文字生成图片" scheme="https://studyinglover.com/categories/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/"/>
    
    
  </entry>
  
  <entry>
    <title>Archlinux安装scrcpy加载共享库出错 error while loading shared libraries:libusb-1.0.so.0:wrong ELF class:ELFCLASS32</title>
    <link href="https://studyinglover.com/2023/07/21/Archlinux%E5%AE%89%E8%A3%85scrcpy%E5%8A%A0%E8%BD%BD%E5%85%B1%E4%BA%AB%E5%BA%93%E5%87%BA%E9%94%99/"/>
    <id>https://studyinglover.com/2023/07/21/Archlinux%E5%AE%89%E8%A3%85scrcpy%E5%8A%A0%E8%BD%BD%E5%85%B1%E4%BA%AB%E5%BA%93%E5%87%BA%E9%94%99/</id>
    <published>2023-07-21T16:13:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="archlinux安装scrcpy加载共享库出错">Archlinux安装scrcpy加载共享库出错</h1><p>在安装scrcpy时通过<code>sudo pacman -S scrcpy</code>顺利安装,但是运行报错 <figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">scrcpy:</span> <span class="hljs-keyword">error</span> <span class="hljs-keyword">while</span> loading <span class="hljs-keyword">shared</span> libraries: libusb-<span class="hljs-number">1.0</span>.so.<span class="hljs-number">0</span>: wrong ELF <span class="hljs-keyword">class</span>: ELFCLASS32<br></code></pre></td></tr></table></figure></p><p>这是在64位系统上运行32位库出错，我发现了这个10年的issue https://github.com/Rouji/Ergodone-Setup/issues/1 也就是说我们只需要运行<code>sudo pacman -S libusb-compat</code></p><p>但是运行之后出现了新的问题 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span> <br><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">libusb</span>-compat: 文件系统中已存在 /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br></code></pre></td></tr></table></figure></p><p>一般来说已经有的库就不要动它了，运行<code>sudo pacman -Syu</code> 没有解决，会报同样的错误，说明libusb这个文件不是包管理器提供的，那就删掉现有的库然后让pacman帮我们安装</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> rm -f /usr/lib/libusb-<span class="hljs-number">0</span>.<span class="hljs-number">1</span>.so.<span class="hljs-number">4</span>.<span class="hljs-number">4</span>.<span class="hljs-number">4</span> <br><span class="hljs-attribute">sudo</span> pacman -S libusb-compat<br></code></pre></td></tr></table></figure><p>插上手机，运行<code>scrcpy</code>,成功运行</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;archlinux安装scrcpy加载共享库出错&quot;&gt;Archlinux安装scrcpy加载共享库出错&lt;/h1&gt;
&lt;p&gt;在安装scrcpy时通过&lt;code&gt;sudo pacman -S scrcpy&lt;/code&gt;顺利安装,但是运行报错 &lt;figure class=&quot;</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
    <category term="ArchLinux" scheme="https://studyinglover.com/tags/ArchLinux/"/>
    
  </entry>
  
  <entry>
    <title>npc_gzip笔记</title>
    <link href="https://studyinglover.com/2023/07/18/npc_gzip%E7%AC%94%E8%AE%B0/"/>
    <id>https://studyinglover.com/2023/07/18/npc_gzip%E7%AC%94%E8%AE%B0/</id>
    <published>2023-07-18T16:57:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="npc_gzip笔记">npc_gzip笔记</h1><h2 id="论文笔记">论文笔记</h2><p>npc_gzip 的论文名叫做 "Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors ,意为不需要参数，使用压缩器的文本分类方法。论文的代码也只有仅仅的十四行，就在部分数据集上取得了超越 <strong>bert</strong> 的效果。</p><p>npc_gzip由一个无损压缩器，一个基于距离的度量函数和K近邻算法组成。</p><p>使用压缩器进行分类的直觉是有两方面 1. 压缩器擅长捕捉规律性；<br />2. 来自同一类别的对象比不同类别的对象具有更多的规律性。</p><p>假设<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> 属于相同的类别，<span class="math inline">\(x_3\)</span> 属于不同的类别，用<span class="math inline">\(C(\cdot)\)</span> 代表压缩器， 我们会发现<span class="math inline">\(C\left(x_1 x_2\right)-C\left(x_1\right)&lt;C\left(x_1 x_3\right)-C\left(x_1\right)\)</span> , <span class="math inline">\(C\left(x_1 x_2\right)\)</span> 代表 x1 和 x2 的串联的压缩长度。换句话说<span class="math inline">\(C\left(x_1 x_2\right)\)</span> 可以解释为我们仍然需要根据 x1 的信息对 x2 进行编码多少字节： <figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs erlang">x1 = Japan&#x27;s Seiko Epson Corp. has developed a <span class="hljs-number">12</span>-gram flying microrobot.<br><br>x2 = The latest tiny flying robot has been unveiled in Japan.<br><br>x3 = Michael Phelps won the gold medal in the <span class="hljs-number">400</span> individual medley.<br></code></pre></td></tr></table></figure></p><p>这种直觉可以形式化为源自 Kolmogorov 复杂度的距离度量。Kolmogorov 复杂度 K(x) 表征了可以生成 x 的最短二进制程序的长度。K(x) 理论上是信息测量的最终下限。</p><p><span class="math display">\[\begin{aligned} E(x, y) &amp; =\max \{K(x \mid y), K(y \mid x)\} \\ &amp; =K(x y)-\min \{K(x), K(y)\}\end{aligned}\]</span></p><p>由于 Kolmogorov 复杂度的可计算性质使得 E(x,y) 不可计算，所以可以使用归一化压缩距离 (NCD)，利用压缩长度 C(x) 来近似 Kolmogorov 复杂度 K(x)。形式上是<span class="math display">\[N C D(x, y)=\frac{C(x y)-\min \{C(x), C(y)\}}{\max \{C(x), C(y)\}}\]</span> 使用压缩长度背后的直觉是压缩器最大压缩的 x 的长度接近 K(x)。一般来说，压缩比越高，C(x)越接近K(x)。</p><p>实验的结果使用 gzip 作为压缩器，这里的<span class="math inline">\(C(x)\)</span> 表示 gzip 压缩后 x 的长度。<span class="math inline">\(C(xy)\)</span> 是 x 和 y 的串联的压缩长度。NCD 提供距离矩阵使用 k-最近邻来执行分类。</p><p>核心代码真的真的就非常简单了 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gzip2 <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">for</span> ( x1 , _ ) <span class="hljs-keyword">in</span> test_set :<br>Cx1 = <span class="hljs-built_in">len</span> ( gzip . compress ( x1 . encode () ) )<br>distance_from_x1 = []<br><span class="hljs-keyword">for</span> ( x2 , _ ) <span class="hljs-keyword">in</span> training_set :<br>Cx2 = <span class="hljs-built_in">len</span> ( gzip . compress ( x2 . encode () )<br>x1x2 = <span class="hljs-string">&quot; &quot;</span> . join ([ x1 , x2 ])<br>Cx1x2 = <span class="hljs-built_in">len</span> ( gzip . compress ( x1x2 . encode () )<br>ncd = ( Cx1x2 - <span class="hljs-built_in">min</span> ( Cx1 , Cx2 )) / <span class="hljs-built_in">max</span> ( Cx1 , Cx2 )<br>distance_from_x1 . append ( ncd )<br>sorted_idx = np . argsort ( np . array ( distance_from_x1 ) )<br>top_k_class = training_set [ sorted_idx [: k ] , <span class="hljs-number">1</span>]<br>predict_class = <span class="hljs-built_in">max</span> ( <span class="hljs-built_in">set</span> ( top_k_class ) , key = top_k_class . count )<br></code></pre></td></tr></table></figure></p><p>这种方法是 DNN 的简单、轻量级和通用的替代方案。很简单，因为它不需要任何预处理或训练。它的轻量级在于它不需要参数或 GPU 资源进行分类。由于压缩器是数据类型不可知的，非参数方法不会带来潜在的假设。</p><h2 id="代码实践">代码实践</h2><p>作者在GitHub上开源了他的代码 <a href="https://github.com/bazingagin/npc_gzip">npc_gzip</a> .我们先把代码拉到本地 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/bazingagin/npc_gzip<br></code></pre></td></tr></table></figure> 接下来安装依赖项，有条件的话创建一个虚拟环境 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ./npc_gzip<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure></p><p>安装完了之后运行<code>main_text.py</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python main_text.py<br></code></pre></td></tr></table></figure></p><p>注意，如果你遇到了这个问题 <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">Traceback (most recent <span class="hljs-keyword">call</span> last):<br>  File &quot;main_text.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">2</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> data <span class="hljs-keyword">import</span> *<br>  File &quot;/home/npc_gzip/data.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">12</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/datasets/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">43</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> .arrow_dataset <span class="hljs-keyword">import</span> Dataset<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/datasets/arrow_dataset.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">59</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> HfApi, HfFolder<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/huggingface_hub/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">322</span>, <span class="hljs-keyword">in</span> __getattr__<br>    submod = importlib.import_module(submod_path)<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/importlib/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">127</span>, <span class="hljs-keyword">in</span> import_module<br>    <span class="hljs-keyword">return</span> _bootstrap._gcd_import(<span class="hljs-type">name</span>[<span class="hljs-keyword">level</span>:], package, <span class="hljs-keyword">level</span>)<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/huggingface_hub/hf_api.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">32</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">import</span> requests<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/requests/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">43</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    <span class="hljs-keyword">import</span> urllib3<br>  File &quot;/home/.conda/envs/npc_gzip/lib/python3.7/site-packages/urllib3/__init__.py&quot;, <span class="hljs-type">line</span> <span class="hljs-number">42</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    &quot;urllib3 v2.0 only supports OpenSSL 1.1.1+, currently &quot;<br>ImportError: urllib3 v2<span class="hljs-number">.0</span> <span class="hljs-keyword">only</span> supports OpenSSL <span class="hljs-number">1.1</span><span class="hljs-number">.1</span>+, currently the <span class="hljs-string">&#x27;ssl&#x27;</span> module <span class="hljs-keyword">is</span> compiled <span class="hljs-keyword">with</span> <span class="hljs-string">&#x27;OpenSSL 1.0.2u  20 Dec 2019&#x27;</span>. See: https://github.com/urllib3/urllib3/issues/<span class="hljs-number">2168</span><br></code></pre></td></tr></table></figure></p><p>urllib3 v2.0（您安装的版本）需要 OpenSSL 1.1.1+ 才能正常工作，因为它依赖于 OpenSSL 1.1 的一些新功能.</p><p>安装旧版本即可解决 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install urllib3==1.26.6 <br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;npc_gzip笔记&quot;&gt;npc_gzip笔记&lt;/h1&gt;
&lt;h2 id=&quot;论文笔记&quot;&gt;论文笔记&lt;/h2&gt;
&lt;p&gt;npc_gzip 的论文名叫做 &quot;Low-Resource&quot; Text Classification: A Parameter-Free Classifi</summary>
      
    
    
    
    <category term="自然语言处理" scheme="https://studyinglover.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>python调用c++函数</title>
    <link href="https://studyinglover.com/2023/07/15/python%E8%B0%83%E7%94%A8c++%E5%87%BD%E6%95%B0/"/>
    <id>https://studyinglover.com/2023/07/15/python%E8%B0%83%E7%94%A8c++%E5%87%BD%E6%95%B0/</id>
    <published>2023-07-15T09:30:00.000Z</published>
    <updated>2023-08-22T14:11:42.156Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python调用c函数">python调用c++函数</h1><p>当我们需要在Python中使用C++编写的函数时，可以将C++代码编译成共享库文件（.so文件），然后来调用这些函数。这里介绍两种方法。</p><h2 id="使用python的api">使用python的api</h2><p>首先要安装安装<code>python-dev</code> 和<code>cmake</code></p><p>在Archlinux下<code>yay python-dev</code> ，<code>yay cmake</code>即可。其他平台需要自行搜索</p><p>首先创建一个C++文件 <code>main.cpp</code> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;Python.h&gt;</span></span><br>  <br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span> </span>&#123;<br><span class="hljs-keyword">return</span> a + b;<br>&#125;<br>  <br><span class="hljs-function"><span class="hljs-type">static</span> PyObject* <span class="hljs-title">py_add</span><span class="hljs-params">(PyObject* self, PyObject* args)</span> </span>&#123;<br><span class="hljs-type">int</span> a, b;<br><span class="hljs-keyword">if</span> (!<span class="hljs-built_in">PyArg_ParseTuple</span>(args, <span class="hljs-string">&quot;ii&quot;</span>, &amp;a, &amp;b)) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;<br>&#125;<br><span class="hljs-type">int</span> result = <span class="hljs-built_in">add</span>(a, b);<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">PyLong_FromLong</span>(result);<br>&#125;<br>  <br><span class="hljs-type">static</span> PyMethodDef module_methods[] = &#123;<br>&#123;<span class="hljs-string">&quot;add&quot;</span>, py_add, METH_VARARGS, <span class="hljs-string">&quot;Add two integers.&quot;</span>&#125;,<br>&#123;<span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, <span class="hljs-literal">NULL</span>&#125;<br>&#125;;<br>  <br><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> <span class="hljs-title class_">PyModuleDef</span> module_def = &#123;<br>PyModuleDef_HEAD_INIT,<br><span class="hljs-string">&quot;my_module&quot;</span>,<br><span class="hljs-string">&quot;My custom module.&quot;</span>,<br><span class="hljs-number">-1</span>,<br>module_methods<br>&#125;;<br>  <br><span class="hljs-function">PyMODINIT_FUNC <span class="hljs-title">PyInit_my_module</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span> </span>&#123;<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">PyModule_Create</span>(&amp;module_def);<br>&#125;<br></code></pre></td></tr></table></figure></p><p>接着用cmake构建<code>.so</code>文件，<code>CMakeLists.txt</code> 内容如下 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs CMakeLists">cmake_minimum_required(VERSION 3.0)<br>  <br>project(my_module)<br>  <br>find_package(Python REQUIRED COMPONENTS Interpreter Development)<br>  <br>add_library(my_module SHARED main.cpp)<br>  <br>target_include_directories(my_module PRIVATE $&#123;Python_INCLUDE_DIRS&#125;)<br>target_link_libraries(my_module PRIVATE $&#123;Python_LIBRARIES&#125;)<br>  <br>set_target_properties(my_module PROPERTIES PREFIX &quot;&quot;)<br>set_target_properties(my_module PROPERTIES SUFFIX &quot;.so&quot;)<br></code></pre></td></tr></table></figure></p><p>构建完成后会有一个名为<code>my_module.so</code> 的文件</p><p>接下来使用python调用,注意将python文件和<code>my_module.so</code> 放到同一个目录下 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> my_module<br>  <br>result = my_module.add(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure></p><h2 id="按照-c-语言的规则来编译和链接">按照 C 语言的规则来编译和链接</h2><p>首先，我们需要编写一个C++文件<code>mylib.cpp</code> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">extern</span> <span class="hljs-string">&quot;C&quot;</span> <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> a + b;<br>&#125;<br></code></pre></td></tr></table></figure></p><p>接下来，编译<code>mylib.cpp</code> 为一个<code>.so</code>文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">g++ -shared -o mylib.so -fPIC mylib.cpp<br></code></pre></td></tr></table></figure></p><p>最后使用python加载<code>mylib.so</code> 文件并调用 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> ctypes<br><br><span class="hljs-comment"># 加载共享库文件</span><br>mylib = ctypes.cdll.LoadLibrary(<span class="hljs-string">&#x27;./mylib.so&#x27;</span>)<br><br>result = mylib.add(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python调用c函数&quot;&gt;python调用c++函数&lt;/h1&gt;
&lt;p&gt;当我们需要在Python中使用C++编写的函数时，可以将C++代码编译成共享库文件（.so文件），然后来调用这些函数。这里介绍两种方法。&lt;/p&gt;
&lt;h2 id=&quot;使用python的api&quot;&gt;使用</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>Filesystem type ntfs3,ntfs not configured in kernel</title>
    <link href="https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/"/>
    <id>https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/</id>
    <published>2023-07-14T09:35:00.000Z</published>
    <updated>2023-08-22T14:11:42.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="filesystem-type-ntfs3ntfs-not-configured-in-kernel">Filesystem type ntfs3,ntfs not configured in kernel</h1><p>昨天卸载硬盘的时候卡住了，然后我就直接拔下了硬盘，再插上就出现了这个问题 <img src="https://cdn.studyinglover.com/pic/2023/07/7da166adca81943084fbc25dae0a3e16.png" alt="image.png" /></p><p>我先用备份恢复了一下，但是重新插上硬盘问题依然存在。接下来google了一下，Archwiki中有提到<a href="https://wiki.archlinux.org/title/NTFS">这个问题</a>，但是标记这个问题是已经过时的，所描述的问题已得到解决。从内核版本6.2开始，ntfs3支持<code>windows_names</code>选项。我就先按照文档说的做了，但是问题依然没有解决。 <img src="https://cdn.studyinglover.com/pic/2023/07/92f0be4c455602d2eda6b9ecd6229969.png" alt="image.png" /></p><p>接下来翻了下reddit，发现有人存在类似的问题 https://www.reddit.com/r/archlinux/comments/s3w6uu/cannot_mount_ntfs_drives_on_516/ ， 有人提到需要安装<code>ntfs-3g</code> ,那么就是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yay ntfs-3g<br></code></pre></td></tr></table></figure></p><p>问题解决</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;filesystem-type-ntfs3ntfs-not-configured-in-kernel&quot;&gt;Filesystem type ntfs3,ntfs not configured in kernel&lt;/h1&gt;
&lt;p&gt;昨天卸载硬盘的时候卡住了，然后我就直接拔</summary>
      
    
    
    
    <category term="踩坑" scheme="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
</feed>
