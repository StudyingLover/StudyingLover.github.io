<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>plus studio</title>
    <link>https://studyinglover.com/</link>
    
    <atom:link href="https://studyinglover.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 14 Jul 2023 01:42:06 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Filesystem type ntfs3,ntfs not configured in kernel</title>
      <link>https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/</link>
      <guid>https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/</guid>
      <pubDate>Fri, 14 Jul 2023 09:35:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;filesystem-type-ntfs3ntfs-not-configured-in-kernel&quot;&gt;Filesystem type ntfs3,ntfs not configured in kernel&lt;/h1&gt;
&lt;p&gt;昨天卸载硬盘的时候卡住了，然后我就直接拔</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="filesystem-type-ntfs3ntfs-not-configured-in-kernel">Filesystem type ntfs3,ntfs not configured in kernel</h1><p>昨天卸载硬盘的时候卡住了，然后我就直接拔下了硬盘，再插上就出现了这个问题 <img src="https://cdn.studyinglover.com/pic/2023/07/7da166adca81943084fbc25dae0a3e16.png" alt="image.png" /></p><p>我先用备份恢复了一下，但是重新插上硬盘问题依然存在。接下来google了一下，Archwiki中有提到<a href="https://wiki.archlinux.org/title/NTFS">这个问题</a>，但是标记这个问题是已经过时的，所描述的问题已得到解决。从内核版本6.2开始，ntfs3支持<code>windows_names</code>选项。我就先按照文档说的做了，但是问题依然没有解决。 <img src="https://cdn.studyinglover.com/pic/2023/07/92f0be4c455602d2eda6b9ecd6229969.png" alt="image.png" /></p><p>接下来翻了下reddit，发现有人存在类似的问题 https://www.reddit.com/r/archlinux/comments/s3w6uu/cannot_mount_ntfs_drives_on_516/ ， 有人提到需要安装<code>ntfs-3g</code> ,那么就是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yay ntfs-3g<br></code></pre></td></tr></table></figure></p><p>问题解决</p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/07/14/Filesystem%20type%20ntfs3,ntfs%20not%20configured%20in%20kernel/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>open_clip编码图像和文本</title>
      <link>https://studyinglover.com/2023/07/13/open_clip%E7%BC%96%E7%A0%81%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC/</link>
      <guid>https://studyinglover.com/2023/07/13/open_clip%E7%BC%96%E7%A0%81%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC/</guid>
      <pubDate>Thu, 13 Jul 2023 23:14:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;open_clip是CLIP的开源实现版本，只训练了CLIP效果最好的几个模型。&lt;/p&gt;
&lt;p&gt;安装是 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>open_clip是CLIP的开源实现版本，只训练了CLIP效果最好的几个模型。</p><p>安装是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install open_clip_torch<br></code></pre></td></tr></table></figure></p><p>首先导入 open_clip，并创建相关模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> open_clip<br><span class="hljs-keyword">import</span> torch<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>clip_model_name = <span class="hljs-string">&quot;ViT-L-14&quot;</span><br>clip_model,_,clip_preprocess = open_clip.create_model_and_transforms(clip_model_name<br>clip_model_name,pretrained = <span class="hljs-string">&quot;openai&quot;</span>,precision=<span class="hljs-string">&#x27;fp16&#x27;</span> <span class="hljs-keyword">if</span> device == <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;fp32&#x27;</span>,device=device,<br>)<br><br>tokenize = open_clip.get_tokenizer(clip_model_name)<br></code></pre></td></tr></table></figure></p><p><code>tokenize</code> 是分词器，所有的文本都要先经过分析器才能放入模型进行推理。</p><h4 id="编码图像">编码图像</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">image_to_features</span>(<span class="hljs-params">image: Image.Image</span>) -&gt; torch.Tensor:<br>images = clip_preprocess(image).unsqueeze(<span class="hljs-number">0</span>).to(device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>image_features = clip_model.encode_image(images)<br><span class="hljs-keyword">return</span> image_features<br>  <br>img = cv.imread(<span class="hljs-string">&quot;/path/to/example.png&quot;</span>)<br>img = Image.fromarray(img)<br><br>image_feature = image_to_features(img)<br></code></pre></td></tr></table></figure><p><code>/path/to/example.png</code> 替换成自己图片的路径</p><p><code>image_to_features</code> 函数是一个封装过的将图像转成文本的函数，传入的参数是一个<code>image_to_features</code>格式的图片。</p><p><code>image_feature</code> 就是经过CLIP的编码器得到的特征</p><h4 id="编码文本">编码文本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;a photo of a cat&quot;</span><br>text_tokens = tokenize([prompt]).to(device)<br>text_features = clip_model.encode_text(text_tokens)<br></code></pre></td></tr></table></figure><p><code>text_features</code> 就是得到的特征。</p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E5%A4%9A%E6%A8%A1%E6%80%81/">多模态</category>
      
      
      
      <comments>https://studyinglover.com/2023/07/13/open_clip%E7%BC%96%E7%A0%81%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PicGo配置CloudflareR2图片储存</title>
      <link>https://studyinglover.com/2023/07/09/PicGo%E9%85%8D%E7%BD%AECloudflareR2%E5%9B%BE%E7%89%87%E5%82%A8%E5%AD%98/</link>
      <guid>https://studyinglover.com/2023/07/09/PicGo%E9%85%8D%E7%BD%AECloudflareR2%E5%9B%BE%E7%89%87%E5%82%A8%E5%AD%98/</guid>
      <pubDate>Sun, 09 Jul 2023 20:24:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;picgo配置cloudflarer2图片储存&quot;&gt;PicGo配置CloudflareR2图片储存&lt;/h1&gt;
&lt;p&gt;首先需要安装PicGo,并购买CloudFlare R2。CloudFlare R2选择免费计划即可，只是需要绑定银行卡或者paypal(淘宝两块钱解</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="picgo配置cloudflarer2图片储存">PicGo配置CloudflareR2图片储存</h1><p>首先需要安装PicGo,并购买CloudFlare R2。CloudFlare R2选择免费计划即可，只是需要绑定银行卡或者paypal(淘宝两块钱解君忧)。</p><p>在R2的管理界面选择管理R2 API Tokens， <img src="https://cdn.studyinglover.com/pic/2023/07/c5fa048794dc5eab45d6e83efef1df8e.png" alt="image.png" /></p><p>创建一个API Token <img src="https://cdn.studyinglover.com/pic/2023/07/eed2d1b23fb75a7abc5ac334688baba7.png" alt="image.png" /> 注意选择权限为edit <img src="https://cdn.studyinglover.com/pic/2023/07/0a7ece4445c1a2f190adc2dd82351f62.png" alt="image.png" /></p><p>创建API Token之后，保存Access Key ID和Secret Access Key。</p><p>接下来返回R2的管理界面，创建一个储存桶 <img src="https://cdn.studyinglover.com/pic/2023/07/6de3892cb5f8a0bf1c53bb83d2070ca6.png" alt="image.png" /> 填入名字并创建桶，点击进入储存桶的管理界面，进入setting界面。 <img src="https://cdn.studyinglover.com/pic/2023/07/674ad9e98a4d4c064cd135353f967fce.png" alt="image.png" /></p><p>自定义自己的域名并允许公开访问，选择Connect Domain绑定到自己的域名，选择AllowAccess允许公开访问。 <img src="https://cdn.studyinglover.com/pic/2023/07/135bb11e6b475ed4d7acdf491003cf52.png" alt="image.png" /></p><p>接下来打开PicGo,安装s3插件 <img src="https://cdn.studyinglover.com/pic/2023/07/bc0d82dee02bc1a2b114477b827b125c.png" alt="image.png" /></p><p>应用密钥ID和应用密钥填入在API Token获取的Access Key ID和Secret Access Key，桶名填入创建的桶的名称，自定义节点填入储存桶管理界面中途中对应的路径。自定义域名填入前面绑定的自己的域名。 <img src="https://cdn.studyinglover.com/pic/2023/07/0c0cc997c92cd807ecb48c3b2b08e394.png" alt="image.png" /></p><p>尝试上传不出意外就上传成功了。</p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/07/09/PicGo%E9%85%8D%E7%BD%AECloudflareR2%E5%9B%BE%E7%89%87%E5%82%A8%E5%AD%98/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ArchlinuxGnome快捷键打开终端</title>
      <link>https://studyinglover.com/2023/06/28/ArchlinuxGnome%E9%85%8D%E7%BD%AE%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF/</link>
      <guid>https://studyinglover.com/2023/06/28/ArchlinuxGnome%E9%85%8D%E7%BD%AE%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF/</guid>
      <pubDate>Wed, 28 Jun 2023 19:30:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;网上大量教程说命令打开终端的命令是&lt;code&gt;gnome-terminal&lt;/code&gt; ， 然而&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://proxy.thisis.plus/202306281925975.png&quot; alt=&quot;&quot; /&gt;&lt;figcapt</description>
        
      
      
      
      <content:encoded><![CDATA[<p>网上大量教程说命令打开终端的命令是<code>gnome-terminal</code> ， 然而</p><figure><img src="https://proxy.thisis.plus/202306281925975.png" alt="" /><figcaption>image.png</figcaption></figure><p>经过一番搜索，我发现 https://www.omglinux.com/gnome-console-tab-overview/ <img src="https://proxy.thisis.plus/202306281927089.png" alt="image.png" /></p><p>emmmmm,意思是终端命令是<code>kgx</code> ?</p><p>果然，又被一些教程坑了 <img src="https://proxy.thisis.plus/202306281928138.png" alt="image.png" /></p><p>最后配置如图 <img src="https://proxy.thisis.plus/202306211810384.png" alt="image.png" /></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/28/ArchlinuxGnome%E9%85%8D%E7%BD%AE%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>clip-interrogator代码解析</title>
      <link>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <guid>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <pubDate>Fri, 23 Jun 2023 22:59:40 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;clip-interrogator代码解析&quot;&gt;clip-interrogator代码解析&lt;/h1&gt;
&lt;p&gt;clip-interrogator 的的主要代码在仓库的&lt;code&gt;./clip-interrogator&lt;/code&gt; 文件夹下 &lt;figure class</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="clip-interrogator代码解析">clip-interrogator代码解析</h1><p>clip-interrogator 的的主要代码在仓库的<code>./clip-interrogator</code> 文件夹下 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">.<br>├── clip_interrogator<span class="hljs-selector-class">.py</span><br>├── data<br>│   ├── artists<span class="hljs-selector-class">.txt</span><br>│   ├── flavors<span class="hljs-selector-class">.txt</span><br>│   ├── mediums<span class="hljs-selector-class">.txt</span><br>│   ├── movements<span class="hljs-selector-class">.txt</span><br>│   └── negative<span class="hljs-selector-class">.txt</span><br>└── __init__<span class="hljs-selector-class">.py</span><br><br></code></pre></td></tr></table></figure></p><p>这里主要解析<code>clip-interrogator.py</code> 文件。</p><h2 id="init.py"><strong>init</strong>.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> .clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator, LabelTable, list_caption_models, list_clip_models, load_list<br><br>__version__ = <span class="hljs-string">&#x27;0.6.0&#x27;</span><br>__author__ = <span class="hljs-string">&#x27;pharmapsychotic&#x27;</span><br></code></pre></td></tr></table></figure><p>这个 <code>__init__.py</code> 文件的作用是在包被导入时执行初始化操作，并提供了版本号和作者信息。</p><h2 id="clip_interrogator.py">clip_interrogator.py</h2><p>文件的大致结构是这样的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> 需要的库<br><br>CAPTION_MODELS = &#123;<br><span class="hljs-string">&#x27;blip-base&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-base&#x27;</span>, <span class="hljs-comment"># 990MB</span><br><span class="hljs-string">&#x27;blip-large&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-large&#x27;</span>, <span class="hljs-comment"># 1.9GB</span><br><span class="hljs-string">&#x27;blip2-2.7b&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-opt-2.7b&#x27;</span>, <span class="hljs-comment"># 15.5GB</span><br><span class="hljs-string">&#x27;blip2-flan-t5-xl&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-flan-t5-xl&#x27;</span>, <span class="hljs-comment"># 15.77GB</span><br><span class="hljs-string">&#x27;git-large-coco&#x27;</span>: <span class="hljs-string">&#x27;microsoft/git-large-coco&#x27;</span>, <span class="hljs-comment"># 1.58GB</span><br>&#125;<br><br>CACHE_URL_BASE = <span class="hljs-string">&#x27;https://huggingface.co/pharma/ci-preprocess/resolve/main/&#x27;</span><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Config</span>:<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Interrogator</span>():<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LabelTable</span>():<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_download_file</span>(<span class="hljs-params">url: <span class="hljs-built_in">str</span>, filepath: <span class="hljs-built_in">str</span>, chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">4</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>, quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge_tables</span>(<span class="hljs-params">tables: <span class="hljs-type">List</span>[LabelTable], ci: Interrogator</span>) -&gt; LabelTable:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prompt_at_max_len</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_truncate_to_fit</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">str</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_caption_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_clip_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_list</span>(<span class="hljs-params">data_path: <span class="hljs-built_in">str</span>, filename: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br></code></pre></td></tr></table></figure></p><p><code>CAPTION_MODELS</code> 定义了各个所需要的模型在huggingface 地址。<code>CACHE_URL_BASE</code> 是缓存地址</p><h3 id="config-class">Config class</h3><p>首先定义了CLIP和BILP模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">caption_model = <span class="hljs-literal">None</span><br>caption_processor = <span class="hljs-literal">None</span><br>clip_model = <span class="hljs-literal">None</span><br>clip_preprocess = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure></p><p>接下来对BLIP和CLIP进行了详细的设置2 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># blip settings</span><br>caption_max_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span><br>caption_model_name: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-string">&#x27;blip-large&#x27;</span> <span class="hljs-comment"># use a key from CAPTION_MODELS or None</span><br>caption_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br>  <br><span class="hljs-comment"># clip settings</span><br>clip_model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;ViT-L-14/openai&#x27;</span><br>clip_model_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span><br>clip_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure></p><p>这段代码是Config类中与Interrogator类相关的配置参数。</p><p>接下来定义了interrogator的相关设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">cache_path: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;cache&#x27;</span> <span class="hljs-comment"># 存储缓存的文本嵌入的路径</span><br>download_cache: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span> <span class="hljs-comment"># 是否从huggingface下载缓存的嵌入向量</span><br>chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span> <span class="hljs-comment"># CLIP的批处理大小</span><br>data_path: <span class="hljs-built_in">str</span> = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data&#x27;</span>)<span class="hljs-comment"># 数据文件的路径</span><br>device: <span class="hljs-built_in">str</span> = (<span class="hljs-string">&quot;mps&quot;</span> <span class="hljs-keyword">if</span> torch.backends.mps.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>flavor_intermediate_count: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span><br>quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span> <span class="hljs-comment"># 是否显示进度条</span><br></code></pre></td></tr></table></figure></p><p><code>apply_low_vram_defaults</code>方法，用于将配置参数设置为适合低显存设备的默认值。在该方法中，将一些参数设置为较小的值，以减少显存的使用。</p><h3 id="interrogator-class">Interrogator class</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Config</span>):<br>self.config = config<br>self.device = config.device<br>self.dtype = torch.float16 <span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">else</span> torch.float32<br>self.caption_offloaded = <span class="hljs-literal">True</span><br>self.clip_offloaded = <span class="hljs-literal">True</span><br>self.load_caption_model()<br>self.load_clip_model()<br></code></pre></td></tr></table></figure><p>继承了<code>Config</code> 类中的一些配置。</p><h4 id="load_caption_model">load_caption_model</h4><p>这个方法用于加载图像描述模型。首先判断配置中是否直接传入了图像描述模型对象，并且是否指定了图像描述模型名称。如果没有直接传入模型对象并且指定了模型名称，则根据模型名称加载对应的模型。加载过程中根据模型名称的不同选择不同的加载方式。加载完成后，将模型设置为eval模式，并根据配置决定是否将模型移动到指定的设备上</p><h4 id="load_clip_model">load_clip_model</h4><p>这个方法用于加载CLIP模型。首先根据配置中指定的CLIP模型名称解析出模型名称和预训练模型名称。然后判断配置中是否直接传入了CLIP模型对象。如果没有直接传入模型对象，则根据模型名称和预训练模型名称加载模型。加载过程中会调用<code>open_clip.create_model_and_transforms()</code>方法创建模型和预处理函数，并设置模型为eval模式。加载完成后，将模型和预处理函数保存到对应的属性中。</p><p>接下来，根据配置中的数据路径加载一些标签数据，并创建<code>LabelTable</code>对象。<code>LabelTable</code>类用于管理标签和对应的嵌入向量。这里创建了artists、flavors、mediums、movements、trendings和negative等LabelTable对象。</p><p>最后，打印加载CLIP模型和数据所花费的时间。</p><h4 id="chain">chain</h4><p>这个方法用于它用于在一组短语中选择最佳的短语，以构建一个完整的提示。</p><p>首先调用_prepare_clip()方法，准备CLIP模型。</p><p>然后，将短语列表转换为一个集合，方便操作。如果没有指定最佳提示，则通过调用rank_top()方法选择当前短语列表中与图像特征最相似的短语作为最佳提示，并计算其相似度。然后从短语集合中移除最佳提示。</p><p>接下来，使用curr_prompt和curr_sim变量保存当前的提示和相似度。</p><p>定义了一个名为check的内部函数，用于检查给定的附加短语是否应该成为当前提示的一部分。该函数会根据相似度比较结果更新最佳提示和最佳相似度，并判断是否需要更新当前提示。</p><p>使用一个循环遍历max_count次，每次迭代中选择当前短语列表中与当前提示加上附加短语后最相似的短语作为最佳短语。然后将该短语的一部分（从curr_prompt的长度加2开始）作为附加短语。调用check()函数进行相似度比较和更新。</p><p>在循环过程中，如果当前提示已经达到了最大长度，则停止迭代。最后，返回最佳提示。</p><h4 id="generate_caption">generate_caption</h4><p>使用BILP生成图像的描述。它首先对图像进行预处理，然后使用图像描述模型生成描述的tokens，最后将tokens解码为文本描述。</p><h4 id="image_to_features">image_to_features</h4><p>使用CLIP的图像编码器将图片转换成torch格式的特征</p><h4 id="interrogate">interrogate</h4><p><code>interrogate_classic</code> 首先生成一个标准格式的提示，描述图像，然后列出艺术家、趋势、风格和口味等文本修饰符。它使用了mediums、artists、trendings、movements和flavors等LabelTable对象来选择相应的修饰符。</p><p><code>interrogate_fast</code> 在生成的描述后面简单地添加排名靠前的词语。它通常比经典模式产生更好的生成提示和图像之间的相似度，但提示的可读性较差。它使用了artists、flavors、mediums、movements和trendings等LabelTable对象来选择排名靠前的词语。</p><p><code>interrogate_negative</code> 主要生成负面词汇，将与图像最不相似的词语连接在一起。它可以用于构建与正面提示相对应的负面提示，并且通常可以改善生成图像的结果，特别是在使用稳定扩散2（Stable Diffusion 2）时。它使用了flavors和negative等LabelTable对象来选择最不相似的词语。</p><p><code>interrogate</code> 会生成一个完整的提示。首先生成一个基于图像的描述，然后根据图像特征和LabelTable对象生成一组修饰符。然后使用chain方法选择最佳的修饰符，并根据相似度和一些条件选择最佳提示。最后，根据生成的多个提示的相似度，选择最终的生成提示。</p><h4 id="prepare_caption">_prepare_caption</h4><p>用于加载BLIP模型。</p><h4 id="prepare_clip">_prepare_clip</h4><p>用于加载CLIP模型。</p><h4 id="rank_top">rank_top</h4><p>这个方法用于对文本进行排名，并返回排名最高的文本。</p><p>首先加载CLIP模型。使用tokenize方法将文本数组转换为文本tokens，并将其移动到设备上。</p><p>然后，使用<code>clip_model</code>的<code>encode_text</code>方法对文本tokens进行编码，得到文本的特征向量。对特征向量进行归一化处理，使其长度为1。接着，计算文本特征向量与图像特征向量之间的相似度。通过计算特征向量的点积得到相似度。如果<code>reverse</code>为<code>True</code>，则将相似度取负，以实现按相似度降序排列。最后，返回排名最高的文本，即相似度最大的文本。</p><h4 id="similarity和similarities">similarity和similarities</h4><p>通过计算点积的方式计算了相似度</p><h3 id="labeltable-class">LabelTable class</h3><p>这个类创建标签，并对标签进行排名</p><h4 id="init"><strong>init</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, labels:<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], desc:<span class="hljs-built_in">str</span>, ci: Interrogator</span>):<br>clip_model, config = ci.clip_model, ci.config<br>self.chunk_size = config.chunk_size<br>self.config = config<br>self.device = config.device<br>self.embeds = []<br>self.labels = labels<br>self.tokenize = ci.tokenize<br>  <br><span class="hljs-built_in">hash</span> = hashlib.sha256(<span class="hljs-string">&quot;,&quot;</span>.join(labels).encode()).hexdigest()<br>sanitized_name = self.config.clip_model_name.replace(<span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>).replace(<span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>)<br>self._load_cached(desc, <span class="hljs-built_in">hash</span>, sanitized_name)<br>  <br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.labels) != <span class="hljs-built_in">len</span>(self.embeds):<br>self.embeds = []<br>chunks = np.array_split(self.labels, <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(self.labels)/config.chunk_size))<br><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> tqdm(chunks, desc=<span class="hljs-string">f&quot;Preprocessing <span class="hljs-subst">&#123;desc&#125;</span>&quot;</span> <span class="hljs-keyword">if</span> desc <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>, disable=self.config.quiet):<br>text_tokens = self.tokenize(chunk).to(self.device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>text_features = clip_model.encode_text(text_tokens)<br>text_features /= text_features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>text_features = text_features.half().cpu().numpy()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(text_features.shape[<span class="hljs-number">0</span>]):<br>self.embeds.append(text_features[i])<br>  <br><span class="hljs-keyword">if</span> desc <span class="hljs-keyword">and</span> self.config.cache_path:<br>os.makedirs(self.config.cache_path, exist_ok=<span class="hljs-literal">True</span>)<br>cache_filepath = os.path.join(self.config.cache_path, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sanitized_name&#125;</span>_<span class="hljs-subst">&#123;desc&#125;</span>.safetensors&quot;</span>)<br>tensors = &#123;<br><span class="hljs-string">&quot;embeds&quot;</span>: np.stack(self.embeds),<br><span class="hljs-string">&quot;hash&quot;</span>: np.array([<span class="hljs-built_in">ord</span>(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">hash</span>], dtype=np.int8)<br>&#125;<br>save_file(tensors, cache_filepath)<br>  <br><span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cpu&#x27;</span> <span class="hljs-keyword">or</span> self.device == torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>):<br>self.embeds = [e.astype(np.float32) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> self.embeds]<br></code></pre></td></tr></table></figure><p>继承了<code>Interrogator</code> 中的一些内容，同时对embeds 做了预处理。</p><h4 id="load_cached">_load_cached</h4><p>用于加载缓存的嵌入向量。</p><h4 id="rank和rank">_rank和rank</h4><p>用于对图像特征和文本嵌入向量进行排名。<code>_rank</code>方法计算图像特征与文本嵌入向量之间的相似度，并返回排名最高的文本索引。<code>rank</code>方法根据<code>chunk_size</code>的大小，将文本嵌入向量分成多个批次进行排名，然后返回排名最高的文本标签。</p><h2 id="data">data</h2><p>存储了常用的文字生成图片的prompt</p><h2 id="clip-interrogator究竟做了什么">clip-interrogator究竟做了什么</h2><p>首先，clip-interrogator会使用BILP生成一段对图片的自然语言描述。</p><p>接下来会根据四种模式，从data文件夹下的txt文件中组合出文字生成图片常用的prompt,通过CLIP进行编码，然后将图片也用CLIP进行编码，计算出相似度最大的一组prompt,和BILP生成的prompt拼接到一起，就得到了一组prompt。</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>GroundingDINO安装报错解决</title>
      <link>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</link>
      <guid>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</guid>
      <pubDate>Wed, 21 Jun 2023 17:25:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;groundingdino安装报错解决&quot;&gt;GroundingDINO安装报错解决&lt;/h1&gt;
&lt;p&gt;在安装会遇到这个错误 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="groundingdino安装报错解决">GroundingDINO安装报错解决</h1><p>在安装会遇到这个错误 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs bash">  ERROR: Command errored out with <span class="hljs-built_in">exit</span> status 1:<br>   <span class="hljs-built_in">command</span>: /usr/bin/python3 /tmp/tmpmhvo4wyp build_wheel /tmp/tmp3a4xwmi4<br>       cwd: /tmp/pip-install-x0mg8qpf/pycocotools<br>  Complete output (77 lines):<br>  running bdist_wheel<br>  running build<br>  running build_py<br>  creating build<br>  creating build/lib.linux-x86_64-cpython-38<br>  creating build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/coco.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/mask.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/cocoeval.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/__init__.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  running build_ext<br>  cythoning pycocotools/_mask.pyx to pycocotools/_mask.c<br>  building <span class="hljs-string">&#x27;pycocotools._mask&#x27;</span> extension<br>  creating build/temp.linux-x86_64-cpython-38<br>  creating build/temp.linux-x86_64-cpython-38/common<br>  creating build/temp.linux-x86_64-cpython-38/pycocotools<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c ./common/maskApi.c -o build/temp.linux-x86_64-cpython-38/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToBbox’:<br>  ./common/maskApi.c:151:32: warning: unused variable ‘xp’ [-Wunused-variable]<br>    151 |     uint h, w, xs, ys, xe, ye, xp, cc; siz j, m;<br>        |                                ^~<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrPoly’:<br>  ./common/maskApi.c:197:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |   ^~~<br>  ./common/maskApi.c:197:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |                                                      ^<br>  ./common/maskApi.c:198:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |   ^~~<br>  ./common/maskApi.c:198:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |                                                      ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToString’:<br>  ./common/maskApi.c:243:7: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |       ^~<br>  ./common/maskApi.c:243:27: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |                           ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrString’:<br>  ./common/maskApi.c:251:3: warning: this ‘<span class="hljs-keyword">while</span>’ clause does not guard... [-Wmisleading-indentation]<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |   ^~~~~<br>  ./common/maskApi.c:251:22: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">while</span>’<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |                      ^~~~<br>  ./common/maskApi.c:259:5: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |     ^~<br>  ./common/maskApi.c:259:34: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |                                  ^~~~<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-cpython-38/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99<br>  pycocotools/_mask.c:6:10: fatal error: Python.h: No such file or directory<br>      6 | <span class="hljs-comment">#include &quot;Python.h&quot;</span><br>        |          ^~~~~~~~~~<br>  compilation terminated.<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/setuptools/dist.py:745: SetuptoolsDeprecationWarning: Invalid dash-separated options<br>  !!<br>  <br>          ********************************************************************************<br>          Usage of dash-separated <span class="hljs-string">&#x27;index-url&#x27;</span> will not be supported <span class="hljs-keyword">in</span> future<br>          versions. Please use the underscore name <span class="hljs-string">&#x27;index_url&#x27;</span> instead.<br>  <br>          By 2023-Sep-26, you need to update your project and remove deprecated calls<br>          or your builds will no longer be supported.<br>  <br>          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html <span class="hljs-keyword">for</span> details.<br>          ********************************************************************************<br>  <br>  !!<br>    opt = self.warn_dash_deprecation(opt, section)<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive <span class="hljs-string">&#x27;language_level&#x27;</span> not <span class="hljs-built_in">set</span>, using 2 <span class="hljs-keyword">for</span> now (Py2). This will change <span class="hljs-keyword">in</span> a later release! File: /tmp/pip-install-x0mg8qpf/pycocotools/pycocotools/_mask.pyx<br>    tree = Parsing.p_module(s, pxd, full_module_name)<br>  error: <span class="hljs-built_in">command</span> <span class="hljs-string">&#x27;/usr/bin/x86_64-linux-gnu-gcc&#x27;</span> failed with <span class="hljs-built_in">exit</span> code 1<br>  ----------------------------------------<br>  ERROR: Failed building wheel <span class="hljs-keyword">for</span> pycocotools<br>Failed to build pycocotools<br>ERROR: Could not build wheels <span class="hljs-keyword">for</span> pycocotools <span class="hljs-built_in">which</span> use PEP 517 and cannot be installed directly<br></code></pre></td></tr></table></figure> 细读报错，我们会发现是编译过程中少了一个<code>Python.h</code> 的头文件导致编译pycocotools失败。</p><p>我们尝试直接安装<code>pycocotools</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install pycocotools<br></code></pre></td></tr></table></figure><p>会出现和上面一样的错误。</p><p>google一番,提示说<code>sudo apt-get install libsuitesparse-dev</code></p><p>受到报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) ... error<br> error: subprocess-exited-with-error<br> <br> × Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) did not run successfully.<br> │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">1</span><br> ╰─&gt; [<span class="hljs-number">77</span> lines of output]<br></code></pre></td></tr></table></figure></p><p>最后的结果依然是 <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs subunit">  note: This error originates from a subprocess, and is likely not a problem with pip.<br>  ERROR: Failed building wheel for pycocotools<br>Failed to build pycocotools<br><span class="hljs-keyword">ERROR: </span>Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects<br></code></pre></td></tr></table></figure></p><p>尝试通过安装<code>pip install "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI"</code> 解决</p><p>获得报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs awk">fatal: unable to access <span class="hljs-string">&#x27;https://github.com/philferriere/cocoapi.git/&#x27;</span>: GnuTLS recv error (-<span class="hljs-number">110</span>): The TLS connection was non-properly terminated.<br>  error: subprocess-exited-with-error<br>  <br>  × git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>  │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>  ╰─&gt; See above <span class="hljs-keyword">for</span> output.<br>  <br>  note: This error originates from a subprocess, and is likely not a problem with pip.<br>error: subprocess-exited-with-error<br><br>× git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>│ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>╰─&gt; See above <span class="hljs-keyword">for</span> output.<br></code></pre></td></tr></table></figure></p><p>运行<code>sudo apt install python3.8-dev</code></p><p>然后<code>git clone https://github.com/cocodataset/cocoapi.git</code> , <code>cd ./cocoapi/PythonAPI</code> ,接下来 <code>make</code></p><p>运行<code>pip install -e .</code> ,成功安装<code>pycocotools</code> .</p><p>再次运行<code>pip install GroundingDINO</code> , 成功。</p><figure><img src="https://proxy.thisis.plus/202306211724652.png" alt="" /><figcaption>image.png</figcaption></figure>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</title>
      <link>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/</link>
      <guid>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/</guid>
      <pubDate>Mon, 19 Jun 2023 23:06:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&quot;&gt;2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&lt;/h1&gt;
&lt;h2 id=&quot;鲲鹏活动&quot;&gt;鲲鹏活动&lt;/h2&gt;
&lt;p&gt;我是白吃白喝来的你真以为我是来学技术的？&lt;/p&gt;
&lt;h3 id=&quot;上午场&quot;&gt;上午场&lt;/h3</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评">2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</h1><h2 id="鲲鹏活动">鲲鹏活动</h2><p>我是白吃白喝来的你真以为我是来学技术的？</p><h3 id="上午场">上午场</h3><p><img src="https://proxy.thisis.plus/202306190655993.jpg" /></p><p>院士发言，讲了从教多年开设公共课的历程，还有将核心技术掌握在自己手里的重要性，举了上世纪欧洲软件和20年哈工大哈工程被禁用matlab的例子。 <img src="https://proxy.thisis.plus/202306190655529.jpg" /></p><p>然后我就牙疼的受不了看牙去了……</p><h3 id="下午场">下午场</h3><p>下午场有三部分，星享会，鲲鹏训练营还有人才发展论坛。星享会是关于互联网+产业命题赛道和鲲鹏应用创新大赛的分享。鲲鹏训练营没有参与，我猜是用类似华为云的沙盒做实验。人才发展论坛是大佬们发言讲自己做的一些研究和人才培养模式 <img src="https://proxy.thisis.plus/202306190703495.jpg" alt="IMG_20230617_135653.jpg" /></p><figure><img src="https://proxy.thisis.plus/202306190704505.jpg" alt="" /><figcaption>IMG_20230617_173651.jpg</figcaption></figure><figure><img src="https://proxy.thisis.plus/202306190704514.jpg" alt="" /><figcaption>IMG_20230617_165145.jpg</figcaption></figure><h2 id="零食午饭测评">零食午饭测评</h2><p>因为牙疼刚做了根管的缘故没有吃的太全，所以只能聊一聊自己吃了的部分</p><h3 id="午饭">午饭</h3><p><img src="https://proxy.thisis.plus/202306190642829.jpg" /></p><p>左边粥是皮蛋瘦肉粥，绝对好评，好喝还适合我这种牙刚做了手术的人，我喝了三碗。</p><p>右边的甜点里我们最远的那一排左边的是蒸饺，正常。右边的不知道叫什么，夹心，正常水平。</p><p>中间的一排虽然长得不一样，都是千层饼。有一点咸味，有点硬不适合那天的牙。</p><p>离我们最近的一排最左边的小蛋糕，我吃了两个，第一个没啥味道，第二个有苦味。中间的豆沙。最右边的，流心绿豆糕，非常好吃，很软口感很好，很适合我的牙，吃了六个吧。</p><h3 id="零食">零食</h3><p><img src="https://proxy.thisis.plus/202306190642391.jpg" /></p><p>右边的饮料据工作人员说是他们自己调的，气泡莫吉托，挺好喝的，有碳酸饮料的感觉 <img src="https://proxy.thisis.plus/202306190642335.jpg" /></p><p>左边盘子里左上是芒果蛋糕，右上草莓蛋糕。芒果蛋糕整个是芒果和奶油，草莓蛋糕正常。左下不好吃，很硬没啥味道。右下核桃芯还是很好吃的。</p><h2 id="收获">收获</h2><p>一本基于鲲鹏的大数据挖掘de书，两个水杯，一个肩带，一个文化衫</p>]]></content:encoded>
      
      
      
      
      <comments>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RoboMaster开源仓库汇总(长期更新)</title>
      <link>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/</link>
      <guid>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/</guid>
      <pubDate>Sun, 18 Jun 2023 22:40:00 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;视觉&quot;&gt;视觉&lt;/h2&gt;
&lt;h3 id=&quot;陈君&quot;&gt;陈君&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://gitlab.com/rm_vision&quot;&gt;陈君视觉1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/rm-vision-arc</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="视觉">视觉</h2><h3 id="陈君">陈君</h3><p><a href="https://gitlab.com/rm_vision">陈君视觉1</a></p><p><a href="https://github.com/rm-vision-archive">陈君视觉2</a></p><h3 id="沈航">沈航</h3><ol type="1"><li><a href="https://github.com/tup-robomaster/TUP-NN-Train-2">TUP-NN-Train-2:项目结构修改，增加几种新网络结构</a></li><li><a href="https://github.com/tup-robomaster/TUP2023-Sentry-Framework">TUP2023-Sentry-Framework:哨兵框架开源，包括全向感知，决策，导航 提供VIO和LIO方案</a></li><li><a href="https://github.com/tup-robomaster/RM_Radar2023">RM_Radar2023:沈阳航空航天大学2023年雷达程序</a></li><li><a href="https://github.com/tup-robomaster/TRTInferenceForYolo">TRTInferenceForYoloX:YOLOXTensorRT推理</a></li></ol><h3 id="西浦">西浦</h3><p><a href="https://github.com/zRzRzRzRzRzRzR/YOLO-of-RoboMaster-Keypoints-Detection-2023">四点识别模型</a></p><h2 id="工具">工具</h2><p><a href="http://shenyibo.me/RM-labeling-tool/">装甲板数据集制作</a></p><p><a href="https://github.com/tup-robomaster/AutoLabel">AutoLabel:自动标注pipeline，集成角点坐标分布分析,图像去重等工具</a></p><p><a href="https://github.com/ifr-cv/rm_part_visual_tag_identify">RoboMaster视觉标签识别器</a></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/RoboMaster/">RoboMaster</category>
      
      
      <comments>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>没有手都可以在腾讯云创建镜像</title>
      <link>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</link>
      <guid>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</guid>
      <pubDate>Fri, 16 Jun 2023 21:15:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。&lt;/p&gt;
&lt;h2 id=&quot;创建&quot;&gt;创建&lt;/h2&gt;
&lt;p&gt;首先登陆腾讯云的账号，进入控制台界面&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;ht</description>
        
      
      
      
      <content:encoded><![CDATA[<p>腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。</p><h2 id="创建">创建</h2><p>首先登陆腾讯云的账号，进入控制台界面</p><figure><img src="https://proxy.thisis.plus/202306162104428.png" alt="" /><figcaption>image.png</figcaption></figure><p>选择实例的的更多选项 <img src="https://proxy.thisis.plus/202306162106168.png" alt="image.png" /></p><p>选择制作镜像 <img src="https://proxy.thisis.plus/202306162107031.png" alt="image.png" /></p><p>在弹出的窗口填入镜像名称，标签和备注即可 <img src="https://proxy.thisis.plus/202306162108164.png" alt="image.png" /></p><p>选择制作镜像后就会进入制作界面，稍等片刻我们就可以看到制作的镜像了 <img src="https://proxy.thisis.plus/202306162111218.png" alt="image.png" /></p><h2 id="应用">应用</h2><p>想要使用创建好的实例也很简单，在左侧选择镜像 <img src="https://proxy.thisis.plus/202306162113093.png" alt="image.png" /></p><p>然后点击创建镜像即可 <img src="https://proxy.thisis.plus/202306162113912.png" alt="image.png" /></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Arch下PicGo不能从剪切板上传图片</title>
      <link>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</link>
      <guid>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</guid>
      <pubDate>Tue, 13 Jun 2023 00:12:40 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://proxy.thisis.plus/202306162118377.p</description>
        
      
      
      
      <content:encoded><![CDATA[<p>作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错</p><figure><img src="https://proxy.thisis.plus/202306162118377.png" alt="" /><figcaption>image.png</figcaption></figure><p>解决方法很简单啦，进入PicGo设置，打开最下面的使用内置剪贴板上传。 <img src="https://proxy.thisis.plus/202306162122805.png" alt="image.png" /></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>LoRA 笔记</title>
      <link>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 13 Jun 2023 00:12:40 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;lora-笔记&quot;&gt;LoRA 笔记&lt;/h1&gt;
&lt;p&gt;自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA&lt;sup id=&quot;fnref:1&quot; class=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="lora-笔记">LoRA 笔记</h1><p>自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="LoRA: Low-Rank Adaptation of Large Language Models. (n.d.).">[1]</span></a></sup>冻结预训练模型权重并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数的数量减少了 10,000 倍，GPU 内存需求减少了 3 倍。</p><h2 id="什么是low-rank">什么是low-rank</h2><p>首先需要明确一些什么什么是矩阵的秩，rank</p><p>在国内的本科线性代数课程中我们是这样定义矩阵的秩的</p><blockquote><p>设在矩阵<span class="math inline">\(A\)</span> 中有一个有一个不等于<span class="math inline">\(0\)</span> 的<span class="math inline">\(r\)</span> 阶子式<span class="math inline">\(D\)</span> ,且所有<span class="math inline">\(r+1\)</span> 阶子式(如果存在的话)都等于<span class="math inline">\(0\)</span> ，那么<span class="math inline">\(D\)</span> 称为矩阵<span class="math inline">\(A\)</span> 的最高阶非零子式，数<span class="math inline">\(r\)</span> 成为矩阵的秩，记为<span class="math inline">\(R(A)\)</span> 。并规定零矩阵的秩为0。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="_同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解_. (2015).">[2]</span></a></sup></p></blockquote><p>怎么求矩阵的秩呢，很简单啦就是把一个矩阵化成RREF(课本上管这个叫行最简行矩阵)然后数一下每一行第一个非零元素所在列为单位向量的个数就可以了。</p><p>好的，发生了什么？好像并没有解释清楚秩到底是什么。</p><p>实际上啊，秩反映了矩阵里列向量线性相关的程度，意思就是你矩阵里的那几个向量能“支”出来几维，假如说我有一个矩阵里面有五个向量，但是他的矩阵秩是3,这就说明五个向量只能撑起一个3维空间，剩下两个向量可以被三个不能被互相表示的向量表示(课本上管这个叫线性相关和线性无关)，用李宏毅的话说就是这里有两个向量在"耍废"。</p><blockquote><p>推荐一下3Blue1Brown的视频https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0，线性代数讲的很清楚。</p></blockquote><p>该清楚了秩是什么，低秩是什么就很好理解了，就是有个矩阵他的秩很低，小于矩阵里面向量的个数(向量组线性相关/有向量在"耍废")。</p><p>你可能会想问，LoRA作为一个微调大语言模型和图文大模型的方法，关矩阵的秩什么事？在2020年，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568">[3]</span></a></sup> 指出大模型的训练实际发生在low-rank空间上的,所以说我们只需要构造一个低秩空间下的训练方法就可以了。</p><h2 id="为什么需要lora">为什么需要LoRA</h2><p>LoRA并不是第一个进行微调大模型的，从迁移学习开始有很多的尝试，以语言建模为例，在有效适应方面有两种突出的策略：添加适配器层或优化某种形式的输入层激活。然而，这两种策略都有其局限性，尤其是在大规模和延迟敏感的生产场景中。 ### 添加适配器层(引入推理延迟) 适配层(Adapter) 实际上就是在原本的架构上添加一些层，让他学到新的东西。例如<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning.">[4]</span></a></sup> <img src="https://proxy.thisis.plus/202306132022661.png" /> 左侧为每个 Transformer 层添加适配器模块两次：在多头注意力的投影和两个前馈层之后。右侧适配器由一个瓶颈组成，该瓶颈包含相对于原始模型中的注意力层和前馈层的参数很少。适配器还包含跳过连接。在适配器调整期间，绿色层在下游数据上进行训练，这包括适配器、层归一化参数和最终分类层（图中未显示）。</p><p>虽然可以通过修剪层或利用多任务设置来减少整体延迟，但没有直接的方法绕过适配器层中的额外计算。在单个 GPU 上对 GPT-2介质运行推理，我们看到在使用适配器时延迟显着增加，即使瓶颈维度非常小。</p><h3 id="优化某种形式的输入层激活很难进行">优化某种形式的输入层激活(很难进行)</h3><p>作者观察到前缀调整很难优化，并且它的性能在可训练参数中非单调地变化，证实了原始论文中的类似观察结果。更根本的是，保留序列长度的一部分进行适应必然会降低可用于处理下游任务的序列长度，所以作者怀疑与其他方法相比，调整提示的性能较低。</p><h2 id="lora到底怎么工作">LoRA到底怎么工作</h2><p>神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。对于预训练的权重矩阵 <span class="math inline">\(W_0 ∈ R^{d×k}\)</span>，我们通过使用低秩分解 <span class="math inline">\(W_0 + ΔW = W_0 + BA\)</span> 表示后者来约束其更新，其中 <span class="math inline">\(B ∈ R^{d×r} , A ∈ R^{r×k}\)</span>，秩<span class="math inline">\(r\)</span> 为 <span class="math inline">\(min(d, k)\)</span>。在训练期间，<span class="math inline">\(W_0\)</span> 被冻结并且不接收梯度更新，而 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 包含可训练的参数。注意 <span class="math inline">\(W_0\)</span> 和 <span class="math inline">\(ΔW = BA\)</span> 都乘以相同的输入，它们各自的输出向量按坐标求和。对于 <span class="math inline">\(h = W_0x\)</span>，我们修改后的前向传递产生：<span class="math display">\[h=W_0x+\Delta Wx=W_0x+BAx\]</span> 参数初始化时，我们对 A 使用随机高斯初始化，B 使用零，因此 ΔW = BA 在训练开始时为零。所以 <span class="math inline">\(\Delta W = BA\)</span> 在训练开始时为零.用<span class="math inline">\(\frac{\alpha}{r}\)</span> 缩放 <span class="math inline">\(ΔWx\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是 <span class="math inline">\(r\)</span> 中的一个常数。在使用 Adam 进行优化时，如果我们适当地缩放初始化，调整 <span class="math inline">\(\alpha\)</span> 与调整学习率大致相同。因此，我们只需将 <span class="math inline">\(\alpha\)</span> 设置为我们尝试的第一个 r，而不对其进行调整。当我们改变时，这种缩放有助于减少重新调整超参数的需要</p><p>这种微调方式有两个好处</p><ol type="1"><li>完全泛化的微调方式</li><li>不会引入推理延迟</li></ol><p>在推理的时候，只需要把<span class="math inline">\(B\)</span>和<span class="math inline">\(A\)</span> 两个矩阵乘起来然后加回到原先的参数矩阵就完成了参数的更新</p><p><img src="https://proxy.thisis.plus/202306132038132.png" /></p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>LoRA: Low-Rank Adaptation of Large Language Models. (n.d.). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><em>同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解</em>. (2015). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Diffusers去除NSFW限制</title>
      <link>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/</link>
      <guid>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/</guid>
      <pubDate>Sun, 11 Jun 2023 00:02:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;众所周知，&lt;del&gt;涩涩是图像生成技术发展的重大推动力&lt;/del&gt; . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(&lt;strong&gt;not safe for work&lt;/stro</description>
        
      
      
      
      <content:encoded><![CDATA[<p>众所周知，<del>涩涩是图像生成技术发展的重大推动力</del> . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(<strong>not safe for work</strong>)的内容，<del>这就给我们涩涩带来了不必要的麻烦</del>。所以我将介绍如何去除限制</p><p>该方法来自网友，<a href="https://www.reddit.com/r/StableDiffusion/comments/wxba44/disable_hugging_face_nsfw_filter_in_three_step/">原链接</a></p><p>先给一段示例代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure><p>我们只需要设置<code>StableDiffusionPipeline</code> 这个类的<code>safety_checker</code>函数，更改之后的代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dummy</span>(<span class="hljs-params">images, **kwargs</span>): <br><span class="hljs-keyword">return</span> images, <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>pipe.safety_checker = dummy<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure></p><p>成功实现<del>涩涩自由</del></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>StableDiffusion笔记</title>
      <link>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Mon, 29 May 2023 15:36:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;Stable Diffusion 是一个图像生成方法，由 &lt;em&gt;&lt;a href=&quot;https://stability.ai/&quot;&gt;Stability AI&lt;/a&gt; and &lt;a href=&quot;https://runwayml.com/&quot;&gt;Runway&lt;/a&gt;&lt;/em&gt; 在LD</description>
        
      
      
      
      <content:encoded><![CDATA[<p>Stable Diffusion 是一个图像生成方法，由 <em><a href="https://stability.ai/">Stability AI</a> and <a href="https://runwayml.com/">Runway</a></em> 在LDM<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[1]</span></a></sup> 的基础上提出。在GitHub有很多他的实现和应用<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="AUTOMATIC1111. (n.d.). _GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI_. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui">[4]</span></a></sup> ,其中<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup> 是最早的实现版本，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup> 是V2版本，由 Stability AI 完成。</p><h2 id="整体结构">整体结构</h2><pre><code class=" mermaid">flowchart TDsubgraph Input-noisyRandom-seed --&gt; latent-Gaussian-noise endsubgraph Input-promptprompt --&gt; TextEncoder --&gt; TextEmbaddingsendlatent-Gaussian-noise --&gt;Unet&#123;Unet-with-MultiAttention&#125;TextEmbaddings--&gt;UnetUnet --&gt; predict-noisy --sampling-steps--&gt;Unetpredict-noisy --&gt; Decoder --&gt; Image </code></pre><p>在一开始，StableDiffusion会通过一个随机数种子生成一张在隐空间下的随机噪声，同时通过一个文本编码器对输入的prompt进行编码，生成一个文本向量。随机噪声和文本向量会一块送入Unet，经过DDPM的步骤得到一张隐空间下的图片，通过一个解码器得到完整的图片。这里的Unet做出了改进，中间加入了交叉注意力机制。</p><h3 id="unet-with-multiattention">Unet-with-MultiAttention</h3><p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IRTbG2rYv0IUH8HHAxWRrQ.png" alt="Unet-with-MultiAttention 图源medium.com" /> 图中Switch用于在不同的输入之间调整。</p><ul><li>文本数据通过一个文本编码器(一般是CLIP的文本编码器)将文本转换为向量，投影到Unet上</li><li>图像，语义图，表示等直接送入Unet</li></ul><p>反向扩散过程中输入的文本向量和隐空间下的噪声图片需要经过 <span class="math inline">\(t\)</span>轮的Unet网络，每一轮预测一个噪声，噪声图减去这个噪声，得到的图片继续送入Unet进行下一轮</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>CompVis. (n.d.). <em>GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model</em>. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Stability-AI. (n.d.). <em>GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models</em>. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>AUTOMATIC1111. (n.d.). <em>GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI</em>. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Steins. (2023, January 2). Stable diffusion clearly explained! - Steins. <em>Medium</em>. https://medium.com/<span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e"><span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e">@steinsfu/stable-diffusion-clearly-explained-ed008044e07e</span></span> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/">笔记</category>
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>huggingface 和相关库</title>
      <link>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/</link>
      <guid>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/</guid>
      <pubDate>Tue, 09 May 2023 12:35:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;huggingface-和相关库&quot;&gt;huggingface 和相关库&lt;/h1&gt;
&lt;h2 id=&quot;huggingface&quot;&gt;huggingface&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;是</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="huggingface-和相关库">huggingface 和相关库</h1><h2 id="huggingface">huggingface</h2><p><a href="https://huggingface.co/">Hugging Face</a>是一个专注于自然语言处理（NLP）的开源平台，它旨在让NLP变得更加易用和普及。Hugging Face推出了多个库，例如Transformers，Datasets，Tokenizers和Accelerate，它们分别提供了预训练的模型，大规模的数据集，高效的分词器和分布式训练的工具。Hugging Face还拥有一个活跃的社区，其中有数千名研究人员，开发者和爱好者共同交流和贡献NLP的最新进展。 <img src="https://proxy.thisis.plus/202305092233241.png" alt="image.png" /></p><p>Hugging Face的名字来源于一个可爱的表情符号，它代表了平台的愿景：让人类和机器之间的交流更加自然和亲密。Hugging Face的核心产品是Transformers库，它包含了超过10000个预训练的模型，涵盖了各种NLP任务，如文本分类，问答，文本生成，情感分析等。Transformers库支持多种深度学习框架，如PyTorch，TensorFlow，JAX和Flax，并且可以轻松地在不同的设备上运行，如CPU，GPU和TPU。Hugging Face还提供了一个在线平台，Spaces，它可以让用户快速地部署和分享他们的模型和应用。 <img src="https://proxy.thisis.plus/202305092235498.png" alt="image.png" /></p><p>近年来，Hugging Face托管的模型已经不局限于NLP领域，而是涉及到了更多的领域，如计算机视觉（CV），语音识别（ASR），音乐生成（MG）等。这些模型都可以在Hugging Face的网站上找到，并且可以通过Transformers库或者其他的库来使用。Hugging Face还提供了一个数据集库，叫做Datasets，它包含了超过1000个数据集，覆盖了各种领域和语言。Datasets库可以帮助用户快速地加载，处理和缓存数据，以及进行数据分析和可视化。</p><h2 id="accelerate">Accelerate</h2><p>Accelerate 是一个可以让训练变得更加简单的库，它可以通过几行代码来在分布式设备上运行相同的pytorch代码</p><p>可以通过pypi 和 conda安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install accelerate<br>conda install -c conda-forge accelerate<br></code></pre></td></tr></table></figure></p><p>你可能会遇到这种报错 <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure></p><p>这里的依赖已经安装成功了，只是被安装到了未被添加到PATH的目录，接下来运行的时候只需要指明目录即可。例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p><p>通过<code>accelerate config</code> 命令可以配置当前文件夹启用。(如果啥都不知道就全部选No)</p><h2 id="transformers">Transformers</h2><p><a href="https://huggingface.co/docs/transformers/index">Transformers</a> 收集了所有的SOTA的NLP研究方法，并提供了对应的预训练模型和接口。Transformers 支持 PyTorch、TensorFlow 和 JAX 之间的框架互操作性。这提供了在模型生命周期的每个阶段使用不同框架的灵活性；在一个框架中用三行代码训练一个模型，然后将其加载到另一个框架中进行推理。模型还可以导出为 ONNX 和 TorchScript 等格式，以便在生产环境中部署。</p><p>安装非常简单 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers[torch]&#x27;</span><br></code></pre></td></tr></table></figure> 这回安装torch对应的api，当然也可以安装完整版 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers&#x27;</span> <br></code></pre></td></tr></table></figure></p><p>21年冬天在家上网课的时候，我看到了这样的一个教程<a href="https://zhuanlan.zhihu.com/p/421642560">这篇文章是我用AI生成出来的</a> ,他就是使用了transformers 库构建了一个生成式语言模型。</p><h2 id="diffusers">Diffusers</h2><p><a href="https://huggingface.co/docs/diffusers/index">Diffusers</a> 收集了所有SOTA的扩散模型，用于生成图像、音频，甚至分子的 3D 结构。diffusers提供了扩散模型的完整pipeline ，包括DDPM，DDIM，stable_diffusion_2，VAE，controlnet等等，可以使用简单的几行代码完成推理。</p><p>安装和上面一样 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers[<span class="hljs-string">&quot;torch&quot;</span>]<br></code></pre></td></tr></table></figure> 或者 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers <br></code></pre></td></tr></table></figure></p><h3 id="pipeline">pipeline</h3><p>pipeline 是diffusers 甚至huggingface各个库的一个重要概念，他封装了各个模型加载权重，构建网络结构，推理和训练的全部过程。</p><p>这里以stable diffusion 1.5为例，首先创建pipeline，并指明stable diffusion 的版本 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DiffusionPipeline<br><br>model_id = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br>pipeline = DiffusionPipeline.from_pretrained(model_id)<br></code></pre></td></tr></table></figure></p><p>接下来给出提示(prompt) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;portrait photo of a old warrior chief&quot;</span><br></code></pre></td></tr></table></figure></p><p>为了加速推理，我们可以把数据放到gpu上 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>设置生成器，并生成图像 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><p>当然，huggingface推荐我们在float16上做推理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)<br>pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br>generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><h3 id="lora">LoRA</h3><p><a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation of Large Language Models (LoRA)</a>是一种训练方法，可以加速大型模型的训练，同时消耗更少的内存，最有用的例子莫过于生成人脸了。Diffusers 现在支持使用 LoRA 进行<a href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-lora">文本到图像生成</a>和<a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora">DreamBooth</a>微调。</p><p><a href="https://arxiv.org/abs/2208.12242">DreamBooth</a>是Google提出的微调技术，用于个性化文本到图像模型（如 Stable Diffusion），可以以在给定几张主题图像的情况下生成不同背景下主题的逼真图像。</p><p>在<a href="https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py">这里</a> 你可以找到完整的代码，在<a href="https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ">Google Drive</a> 下载完整的图像用于训练</p><p>先设置基本信息，分别是模型名，示例图片和模型输出文件夹 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MODEL_NAME=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br><span class="hljs-built_in">export</span> INSTANCE_DIR=<span class="hljs-string">&quot;path-to-instance-images&quot;</span><br><span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;path-to-save-model&quot;</span><br></code></pre></td></tr></table></figure></p><p>接下来运行代码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate launch train_dreambooth_lora.py \<br>  --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_NAME</span>  \<br>  --instance_data_dir=<span class="hljs-variable">$INSTANCE_DIR</span> \<br>  --output_dir=<span class="hljs-variable">$OUTPUT_DIR</span> \<br>  --instance_prompt=<span class="hljs-string">&quot;a photo of sks dog&quot;</span> \<br>  --resolution=512 \<br>  --train_batch_size=1 \<br>  --gradient_accumulation_steps=1 \<br>  --checkpointing_steps=100 \<br>  --learning_rate=1e-4 \<br>  --report_to=<span class="hljs-string">&quot;wandb&quot;</span> \<br>  --lr_scheduler=<span class="hljs-string">&quot;constant&quot;</span> \<br>  --lr_warmup_steps=0 \<br>  --max_train_steps=500 \<br>  --validation_prompt=<span class="hljs-string">&quot;A photo of sks dog in a bucket&quot;</span> \<br>  --validation_epochs=50 \<br>  --seed=<span class="hljs-string">&quot;0&quot;</span> \<br>  --push_to_hub<br></code></pre></td></tr></table></figure></p><p>推理也是使用起来很简单的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><br>pipe.unet.load_attn_procs(lora_model_path)<br>pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>image = pipe(<span class="hljs-string">&quot;A picture of a sks dog in a bucket.&quot;</span>, num_inference_steps=<span class="hljs-number">25</span>, guidance_scale=<span class="hljs-number">7.5</span>).images[<span class="hljs-number">0</span>]<br>image.save(<span class="hljs-string">&quot;bucket-dog.png&quot;</span>)<br></code></pre></td></tr></table></figure></p><h3 id="controlnet">[[ControlNet]]</h3><p>可以看之前的文章<a href="https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/">ControlNet训练自己数据集</a></p><h2 id="gradio">Gradio</h2><p>gradio 是一个可以快速构建交互式网页的工具，Webui就是用它做出来的，使用他的核心代码就是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">demo = gradio.Interface(fn, inputs, outputs, ···)<br>demo.launch()<br></code></pre></td></tr></table></figure> 传入一个函数和参数，获取返回值</p><p>剩下的就是你写好fn，设计一个好看的界面，然后launch就可以了。</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Multidiffusion代码分析</title>
      <link>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <guid>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <pubDate>Tue, 09 May 2023 12:35:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;multidiffusion代码分析&quot;&gt;Multidiffusion代码分析&lt;/h1&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;当我们使用计算机生成图像时，经常会遇到一些困难，例如如何生成高质量、高分辨率的图像，如何控制图像的风格和内容等。近年来，深度学习技</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="multidiffusion代码分析">Multidiffusion代码分析</h1><h2 id="前言">前言</h2><p>当我们使用计算机生成图像时，经常会遇到一些困难，例如如何生成高质量、高分辨率的图像，如何控制图像的风格和内容等。近年来，深度学习技术在图像生成领域取得了很大的进展，其中一种流行的方法是使用变分自编码器（VAE）和生成对抗网络（GAN）等模型。然而，这些方法通常需要大量的训练数据和计算资源，而且生成的图像可能会出现一些问题，例如模糊、失真和不连续等。</p><p>为了解决这些问题，一些研究人员提出了一种新的合成全景图的方法，称为MultiDiffusion。该方法使用了一种多步推理的策略，将全景图像的生成过程分解成多个步骤，并在每个步骤中对潜变量向量进行微调，从而生成高质量、高分辨率的全景图像。MultiDiffusion方法不需要大量的训练数据和计算资源，而且能够生成具有良好视觉效果的全景图像。本文将介绍MultiDiffusion方法的实现细节，并提供相应的代码和解释。(chatgpt写的，大家凑活着看)</p><p><a href="https://multidiffusion.github.io/">官方主页</a> <a href="https://github.com/omerbt/MultiDiffusion">代码</a> <a href="https://huggingface.co/spaces/weizmannscience/MultiDiffusion">在线体验</a></p><h2 id="分析">分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTextModel, CLIPTokenizer, logging<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoencoderKL, UNet2DConditionModel, DDIMScheduler<br><span class="hljs-comment"># suppress partial model loading warning</span><br>logging.set_verbosity_error()<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><span class="hljs-keyword">import</span> argparse<br></code></pre></td></tr></table></figure><p>这里导入了所有的库，包括huggingface推出的transformers 和 diffusers。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seed_everything</span>(<span class="hljs-params">seed</span>):<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed(seed)<br>    <span class="hljs-comment"># torch.backends.cudnn.deterministic = True</span><br>    <span class="hljs-comment"># torch.backends.cudnn.benchmark = True</span><br></code></pre></td></tr></table></figure><p>常规操作，设置随机数，实际上还有另一种写法<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="关注 R. 却没能成为自己​. (n.d.). _pytorch如何确保 可重复性/每次训练结果相同(固定了随机种子，为什么还不行)？_. 知乎. Retrieved May 9, 2023, from http://zhihu.com/question/345043149/answer/2940838756">[1]</span></a></sup> . 这里是设置了torch 在CPU 和 GPU 的随机数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seed_torch</span>(<span class="hljs-params">seed=<span class="hljs-number">1029</span></span>):<br>    random.seed(seed)   <span class="hljs-comment"># Python的随机性</span><br>    os.environ[<span class="hljs-string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="hljs-built_in">str</span>(seed)    <span class="hljs-comment"># 设置Python哈希种子，为了禁止hash随机化，使得实验可复现</span><br>    np.random.seed(seed)   <span class="hljs-comment"># numpy的随机性</span><br>    torch.manual_seed(seed)   <span class="hljs-comment"># torch的CPU随机性，为CPU设置随机种子</span><br>    torch.cuda.manual_seed(seed)   <span class="hljs-comment"># torch的GPU随机性，为当前GPU设置随机种子</span><br>    torch.cuda.manual_seed_all(seed)  <span class="hljs-comment"># if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span>   <span class="hljs-comment"># if benchmark=True, deterministic will be False</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span>   <span class="hljs-comment"># 选择确定性算法</span><br></code></pre></td></tr></table></figure> 事实上，涉及到一些类似upsample 的层，因为原子加操作带来的浮点误差，永远也对不齐。 <code>a + b） + c != a + (b + c)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_views</span>(<span class="hljs-params">panorama_height, panorama_width, window_size=<span class="hljs-number">64</span>, stride=<span class="hljs-number">8</span></span>):<br>    panorama_height /= <span class="hljs-number">8</span><br>    panorama_width /= <span class="hljs-number">8</span><br>    num_blocks_height = (panorama_height - window_size) // stride + <span class="hljs-number">1</span><br>    num_blocks_width = (panorama_width - window_size) // stride + <span class="hljs-number">1</span><br>    total_num_blocks = <span class="hljs-built_in">int</span>(num_blocks_height * num_blocks_width)<br>    views = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_num_blocks):<br>        h_start = <span class="hljs-built_in">int</span>((i // num_blocks_width) * stride)<br>        h_end = h_start + window_size<br>        w_start = <span class="hljs-built_in">int</span>((i % num_blocks_width) * stride)<br>        w_end = w_start + window_size<br>        views.append((h_start, h_end, w_start, w_end))<br>    <span class="hljs-keyword">return</span> views<br></code></pre></td></tr></table></figure><p>这段代码的作用是将一个全景图像分成多个小块，每个块的大小为<span class="math inline">\(window_{size} * window_{size}\)</span>，步长为<span class="math inline">\(stride\)</span>，返回每个小块的位置信息。</p><p>下面类定义了整个multidiffusion的所有操作 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self.device = device<br>self.sd_version = sd_version<br></code></pre></td></tr></table></figure> 定义了设备(CPU/GPU)和stable diffusion的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] loading stable diffusion...&#x27;</span>)<br><br><span class="hljs-keyword">if</span> hf_key <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] using hugging face custom model key: <span class="hljs-subst">&#123;hf_key&#125;</span>&#x27;</span>)<br>model_key = hf_key<br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;2.1&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;stabilityai/stable-diffusion-2-1-base&quot;</span><br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;2.0&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;stabilityai/stable-diffusion-2-base&quot;</span><br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;1.5&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br><span class="hljs-keyword">else</span>:<br><span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&#x27;Stable-diffusion version <span class="hljs-subst">&#123;self.sd_version&#125;</span> not supported.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>加载了stable diffusion的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create model</span><br>self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;vae&quot;</span>).to(self.device)<br>self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;tokenizer&quot;</span>)<br>self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;text_encoder&quot;</span>).to(self.device)<br>self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;unet&quot;</span>).to(self.device)<br>self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;scheduler&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] loaded stable diffusion!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这里是从预训练模型加载并创建模型，分别加载了VAE，tokenizer，text_encoder</p><table><thead><tr class="header"><th>模型</th><th>内容</th></tr></thead><tbody><tr class="odd"><td>VAE</td><td>变分自动编码器</td></tr><tr class="even"><td>tokenizer</td><td>分词器,负责将一句话分割成一个一个词，这里是CLIPTokenizer</td></tr><tr class="odd"><td>text_encoder</td><td>文本编码器</td></tr><tr class="even"><td>UNet2DConditionModel</td><td>Unet，负责重建和预测</td></tr><tr class="odd"><td>DDIMScheduler</td><td>DDIM采样器</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_embeds</span>(<span class="hljs-params">self, prompt, negative_prompt</span>):<br><span class="hljs-comment"># prompt, negative_prompt: [str]</span><br><span class="hljs-comment"># Tokenize text and get embeddings</span><br>text_input = self.tokenizer(prompt, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=self.tokenizer.model_max_length,<br>truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># Do the same for unconditional embeddings</span><br>uncond_input = self.tokenizer(negative_prompt, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=self.tokenizer.model_max_length, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># Cat for final embeddings</span><br>text_embeddings = torch.cat([uncond_embeddings, text_embeddings])<br><span class="hljs-keyword">return</span> text_embeddings<br></code></pre></td></tr></table></figure><p>这里是将提示(prompt) 转换成了text_embeddings、</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_latents</span>(<span class="hljs-params">self, latents</span>):<br>        latents = <span class="hljs-number">1</span> / <span class="hljs-number">0.18215</span> * latents<br>        imgs = self.vae.decode(latents).sample<br>        imgs = (imgs / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>).clamp(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> imgs<br></code></pre></td></tr></table></figure><p>这段代码作用是将一个向量从latent space 解码成一个图像。</p><p>它接收一个潜变量向量集合作为输入，并使用变分自编码器（VAE）将其解码成图像。他将输入的潜变量向量集合除以0.18215进行缩放(魔数，不知原因)，然后调用VAE的decode方法来生成一组图像同时使用sample方法产生一些随机性，从而增加输出图像的多样性。最后缩放到<span class="math inline">\([0,1]\)</span> 范围内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">text2panorama</span>(<span class="hljs-params">self, prompts, negative_prompts=<span class="hljs-string">&#x27;&#x27;</span>, height=<span class="hljs-number">512</span>, width=<span class="hljs-number">2048</span>, num_inference_steps=<span class="hljs-number">50</span>, guidance_scale=<span class="hljs-number">7.5</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(prompts, <span class="hljs-built_in">str</span>):<br>            prompts = [prompts]<br>  <br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(negative_prompts, <span class="hljs-built_in">str</span>):<br>            negative_prompts = [negative_prompts]<br>  <br>        <span class="hljs-comment"># Prompts -&gt; text embeds</span><br>        text_embeds = self.get_text_embeds(prompts, negative_prompts)  <span class="hljs-comment"># [2, 77, 768]</span><br>  <br>        <span class="hljs-comment"># Define panorama grid and get views</span><br>        latent = torch.randn((<span class="hljs-number">1</span>, self.unet.in_channels, height // <span class="hljs-number">8</span>, width // <span class="hljs-number">8</span>), device=self.device)<br>        views = get_views(height, width)<br>        count = torch.zeros_like(latent)<br>        value = torch.zeros_like(latent)<br>  <br>        self.scheduler.set_timesteps(num_inference_steps)<br>  <br>        <span class="hljs-keyword">with</span> torch.autocast(<span class="hljs-string">&#x27;cuda&#x27;</span>):<br>            <span class="hljs-keyword">for</span> i, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.scheduler.timesteps):<br>                count.zero_()<br>                value.zero_()<br>  <br>                <span class="hljs-keyword">for</span> h_start, h_end, w_start, w_end <span class="hljs-keyword">in</span> views:<br>                    <span class="hljs-comment"># TODO we can support batches, and pass multiple views at once to the unet</span><br>                    latent_view = latent[:, :, h_start:h_end, w_start:w_end]<br>  <br>                    <span class="hljs-comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span><br>                    latent_model_input = torch.cat([latent_view] * <span class="hljs-number">2</span>)<br>  <br>                    <span class="hljs-comment"># predict the noise residual</span><br>                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)[<span class="hljs-string">&#x27;sample&#x27;</span>]<br>  <br>                    <span class="hljs-comment"># perform guidance</span><br>                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(<span class="hljs-number">2</span>)<br>                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)<br>  <br>                    <span class="hljs-comment"># compute the denoising step with the reference model</span><br>                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)[<span class="hljs-string">&#x27;prev_sample&#x27;</span>]<br>                    value[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised<br>                    count[:, :, h_start:h_end, w_start:w_end] += <span class="hljs-number">1</span><br>  <br>                <span class="hljs-comment"># take the MultiDiffusion step</span><br>                latent = torch.where(count &gt; <span class="hljs-number">0</span>, value / count, value)<br>  <br>        <span class="hljs-comment"># Img latents -&gt; imgs</span><br>        imgs = self.decode_latents(latent)  <span class="hljs-comment"># [1, 3, 512, 512]</span><br>        img = T.ToPILImage()(imgs[<span class="hljs-number">0</span>].cpu())<br>        <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure><p>作用是根据给定的文本提示(prompts)，将其合成成全景图像。它接收一组提示(prompt)作为输入，将其转换为列表类型。然后，定义全景图像的网格，并获取一个一个图像。接下来，使用随机噪声向量作为输入，通过多步推理生成全景图像的潜变量向量。在推理过程中，使用UNet模型对潜变量向量进行多步推理，并根据提示进行引导，生成不同的全景图像，最后横向拼接所有图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;a photo of the dolomites&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--negative&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--sd_version&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;2.0&#x27;</span>, choices=[<span class="hljs-string">&#x27;1.5&#x27;</span>, <span class="hljs-string">&#x27;2.0&#x27;</span>],                       <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;stable diffusion version&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--H&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">512</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--W&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">4096</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">0</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--steps&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">50</span>)<br>    opt = parser.parse_args()<br>    seed_everything(opt.seed)<br>  <br>    device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>  <br>    sd = MultiDiffusion(device, opt.sd_version)<br><br>    img = sd.text2panorama(opt.prompt, opt.negative, opt.H, opt.W, opt.steps)<br>  <br>    <span class="hljs-comment"># save image</span><br>    img.save(<span class="hljs-string">&#x27;out.png&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这个是从命令行启动的方式，按照argparse的使用方法使用</p><table><thead><tr class="header"><th>参数</th><th>含义</th></tr></thead><tbody><tr class="odd"><td>prompt</td><td>提示</td></tr><tr class="even"><td>negative</td><td>反面提示</td></tr><tr class="odd"><td>sd_version</td><td>stable diffusion的版本</td></tr><tr class="even"><td>H</td><td>图像的高度</td></tr><tr class="odd"><td>W</td><td>图像的宽度</td></tr><tr class="even"><td>seed</td><td>随机数种子</td></tr><tr class="odd"><td>steps</td><td>采样步数</td></tr></tbody></table><p>最后的结果会保存为out.png</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>关注 R. 却没能成为自己​. (n.d.). <em>pytorch如何确保 可重复性/每次训练结果相同(固定了随机种子，为什么还不行)？</em>. 知乎. Retrieved May 9, 2023, from http://zhihu.com/question/345043149/answer/2940838756 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>MXnet-arcface数据集准备</title>
      <link>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/</link>
      <guid>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/</guid>
      <pubDate>Mon, 08 May 2023 21:28:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;众所周知，mxnet是一个沐神主导开发的一个深度学习框架，之前听李沐的讲论文时也听他说过很多次，但是已知没有机会使用，最近接触了一个项目，有机会感受了一些mxnet，但是也踩了很多坑。所有需要的脚本文件可以在&lt;a href=&quot;https://github.com/Study</description>
        
      
      
      
      <content:encoded><![CDATA[<p>众所周知，mxnet是一个沐神主导开发的一个深度学习框架，之前听李沐的讲论文时也听他说过很多次，但是已知没有机会使用，最近接触了一个项目，有机会感受了一些mxnet，但是也踩了很多坑。所有需要的脚本文件可以在<a href="https://github.com/StudyingLover/menet-Arcface-tools">https://github.com/StudyingLover/menet-Arcface-tools</a>下载</p><figure><img src="https://proxy.thisis.plus/202305082129080.png" alt="" /><figcaption>image.png</figcaption></figure><p>mxnet 的数据与别处的是不同的，他的训练集是两个文件，分别以<code>.idx</code> 和 <code>.rec</code> 结尾， 测试集是以<code>.bin</code> 结尾的一个二进制文件。</p><h3 id="创建lstidxrec">创建lst,idx,rec</h3><p>我们需要按照特定方式放置图片,首先创建一个大的文件夹，里面创建一个个子文件夹，每个文件夹放置相同类别的图片 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs stylus">/image_folder<br>├── <span class="hljs-number">0</span>_0_0000000<br>│   ├── <span class="hljs-number">0</span>_0<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_1<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_2<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_3<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_4<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000001<br>│   ├── <span class="hljs-number">0</span>_5<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_6<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_7<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_8<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_9<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000002<br>│   ├── <span class="hljs-number">0</span>_10<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_11<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_12<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_13<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_14<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_15<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_16<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_17<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000003<br>│   ├── <span class="hljs-number">0</span>_18<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_19<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_20<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000004<br><br></code></pre></td></tr></table></figure></p><p>接下来先生成一个<code>.lst</code> 文件，这个文件包含了所有的文件,训练集和测试集按照8：2划分 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m mxnet.tools.im2rec --list --recursive train 图片文件夹 –test-ratio 0.8<br></code></pre></td></tr></table></figure></p><p>这段代码会生成两个文件夹<code>train_train.lst</code> 和<code>train_test.lst</code></p><h3 id="生成训练集文件">生成训练集文件</h3><p>接下来生成训练集文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m mxnet.tools.im2rec train_train.lst --quality 100 图片文件夹<br></code></pre></td></tr></table></figure></p><p>需要给生成的文件改个名字 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> train_train.idx train.idx<br><span class="hljs-built_in">mv</span> train_train.rec train.rec<br></code></pre></td></tr></table></figure></p><p>下面创建property配置文件 <figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">训练集图片数量 图片大小 图片大小<br></code></pre></td></tr></table></figure></p><p>例如 <figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">10000 </span><span class="hljs-number">112</span> <span class="hljs-number">112</span><br></code></pre></td></tr></table></figure></p><h3 id="创建pair文件">创建pair文件</h3><p>这一步多少有点奇怪，pair文件里面的结构是 <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode">im<span class="hljs-name">g1</span>_path im<span class="hljs-name">g2</span>_path <span class="hljs-number">0</span><br>im<span class="hljs-name">g3</span>_path im<span class="hljs-name">g4</span>_path <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure> 生成方式也很简单啦，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 generate_image_pairs.py --data-dir 图片文件夹路径 --outputtxt train.txt --num-samepairs 3000<br></code></pre></td></tr></table></figure> <code>num-samepairs</code> 是个魔数，看心情写吧，这里我为了大量生成，我又写了个脚本，重复执行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">python repeat_cmd.py<br>python detele_empty.py<br><span class="hljs-built_in">cp</span> train.txt 图片文件夹<br></code></pre></td></tr></table></figure></p><h3 id="生成验证集bin">生成验证集bin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python lfw2pack.py --data-dir 图片文件夹 --output test.bin --num-samepairs 300<br></code></pre></td></tr></table></figure><p>ok就这样，我们生成了需要的<code>train.idx</code> <code>train.rec</code>,<code>test.bin</code></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E5%B7%A5%E5%85%B7/">工具</category>
      
      
      
      <comments>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ControlNet训练自己数据集</title>
      <link>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/</link>
      <guid>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/</guid>
      <pubDate>Thu, 27 Apr 2023 19:36:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;controlnet训练自己数据集&quot;&gt;ControlNet训练自己数据集&lt;/h1&gt;
&lt;h2 id=&quot;从官方仓库训练&quot;&gt;从官方仓库训练&lt;/h2&gt;
&lt;p&gt;官方教程 https://github.com/lllyasviel/ControlNet/blob/main/d</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="controlnet训练自己数据集">ControlNet训练自己数据集</h1><h2 id="从官方仓库训练">从官方仓库训练</h2><p>官方教程 https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md</p><h3 id="环境配置">环境配置</h3><p>先看一下有没有显卡 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvidia-smi<br></code></pre></td></tr></table></figure></p><p>首先下载整个仓库 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/lllyasviel/ControlNet.git<br></code></pre></td></tr></table></figure></p><p>然后创建conda虚拟环境(选做，只要你能配好环境) <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> create -f environment.yaml<br>conda activate control<br></code></pre></td></tr></table></figure></p><p>接下来需要下载stable diffusion和训练集，因为我们是对stable diffusion 模型做微调。</p><p>下载sd1.5到，models目录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ./models<br>wget https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt<br></code></pre></td></tr></table></figure></p><p>下载训练数据集到training文件夹 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> training <br><span class="hljs-built_in">cd</span> ./training<br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/training/fill50k.zip<br></code></pre></td></tr></table></figure> 解压数据集 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">unzip fill50k.zip<br></code></pre></td></tr></table></figure></p><p>当然这个数据集非常大，我们也可以选择小一点的 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png<br><br>wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png<br></code></pre></td></tr></table></figure> 然后将conditioning_image_1.png改名0.png放到./source目录下,conditioning_image_2.png改名放到./target目录下 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> conditioning_image_1.png 0.png <br><span class="hljs-built_in">mv</span> 0.png ./source<br><br><span class="hljs-built_in">mv</span> conditioning_image_2.png 0.png <br><span class="hljs-built_in">mv</span> 0.png ./target<br></code></pre></td></tr></table></figure></p><p>然后创建一个<code>prompt.json</code> 的文件写入 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;source&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;source/0.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;target&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;target/0.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;pale golden rod circle with old lace background&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure></p><p>无论是哪种方式，最后的文件结构是这样的<img src="https://proxy.thisis.plus/20230427191856.png" alt="image.png" /></p><h3 id="训练">训练</h3><p>首先调一下<code>tutorial_train.py</code> 里的batch_size，训练过程中如果出现out of memory 的情况可以调小。</p><p>接下来运行tutorial_train.py，闭上眼睛等待训练完成即可 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python tutorial_train.py<br></code></pre></td></tr></table></figure> 如果是完整数据集，大概6个小时一个epoch，如果是单张图片会很快。</p><p>当然，为了不要出现网不好ssh断掉导致训练终端，我们可以使用screne <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -S train <br>conda activate control <br>python tutorial_train.py<br></code></pre></td></tr></table></figure> 训练出的结果可以在<code>image_log</code> 中看到</p><figure><img src="https://proxy.thisis.plus/20230427191937.png" alt="" /><figcaption>image.png</figcaption></figure><h3 id="踩坑解决">踩坑解决</h3><h4 id="out-of-memoryoom">out of memory(oom)</h4><p>首先开启<code>save_memory</code>模式，将<code>config.py</code> 中False改为True</p><p>同时调低batch_size</p><h4 id="no-operator-found-for-memory_efficient_attention_backward">No operator found for <code>memory_efficient_attention_backward</code></h4><p>卸载 xformers <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall  xformers<br></code></pre></td></tr></table></figure></p><h4 id="typeerror-on_train_batch_start-missing-1-required-positional-argument-dataloader_idx">TypeError: on_train_batch_start() missing 1 required positional argument: 'dataloader_idx'</h4><p>这个比较坑，是论文代码有问题，改一下源码就好 1. ControlNet/ldm/models/diffusion/ddpm.py文件591行 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_start</span>(<span class="hljs-params">self, batch, batch_idx, dataloader_idx</span>):<br></code></pre></td></tr></table></figure> 删除dataloader_idx,改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_start</span>(<span class="hljs-params">self, batch, batch_idx</span>):<br></code></pre></td></tr></table></figure></p><ol start="2" type="1"><li>ControlNet/cldm/logger.py文件74行 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_end</span>(<span class="hljs-params">self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx</span>):<br></code></pre></td></tr></table></figure> 删除dataloader_idx，改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_end</span>(<span class="hljs-params">self, trainer, pl_module, outputs, batch, batch_idx</span>):<br></code></pre></td></tr></table></figure></li></ol><h2 id="diffusers-训练">Diffusers 训练</h2><p><a href="https://github.com/huggingface/diffusers">Diffusers</a> 是一个huggingface 推出的扩散模型的封装库,同时也对ControlNet做了封装，https://github.com/huggingface/diffusers/tree/main/examples/controlnet</p><h3 id="训练-1">训练</h3><p>代码跑起来其实也非常简单，首先下载diffusers整个仓库,然后安装依赖 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/huggingface/diffusers<br><span class="hljs-built_in">cd</span> diffusers<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure> 你可能会发现这样的报错 <img src="https://proxy.thisis.plus/202304272229714.png" alt="image.png" /> <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure> 别慌，依赖已经下载成功了，只是下载到了一个不在PATH的路径，接下来如果要使用这些被提到的库就需要指明路径，例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p><p>接下来运行tutorial_train <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate config<br></code></pre></td></tr></table></figure> 全部选NO就好，如果你有多卡什么的可以参考<a href="https://huggingface.co/docs/accelerate/index">官方文档</a></p><p>我们需要测试数据集 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png<br><br>wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png<br></code></pre></td></tr></table></figure></p><p>接着运行，设置基础模型和模型输出目录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;./out_models&quot;</span><br><span class="hljs-built_in">export</span> MODEL_DIR=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br></code></pre></td></tr></table></figure></p><p>运行代码，这里epoch=1，steps=1 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/ubuntu/.local/bin/accelerate launch train_controlnet.py   --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_DIR</span>  --output_dir=<span class="hljs-variable">$OUTPUT_DIR</span>   --dataset_name=fusing/fill50k   --resolution=512   --learning_rate=1e-5   --validation_image <span class="hljs-string">&quot;./conditioning_image_1.png&quot;</span> <span class="hljs-string">&quot;./conditioning_image_2.png&quot;</span>   --validation_prompt <span class="hljs-string">&quot;red circle with blue background&quot;</span> <span class="hljs-string">&quot;cyan circle with brown floral background&quot;</span>   --train_batch_size=4 --num_train_epochs=1 --max_train_steps=1<br></code></pre></td></tr></table></figure></p><h3 id="推理">推理</h3><p>新建一个文件<code>inference.py</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler<br><span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image<br><span class="hljs-keyword">import</span> torch<br><br>base_model_path = <span class="hljs-string">&quot;path to model&quot;</span><br>controlnet_path = <span class="hljs-string">&quot;path to controlnet&quot;</span><br><br>controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    base_model_path, controlnet=controlnet, torch_dtype=torch.float16<br>)<br><br><span class="hljs-comment"># speed up diffusion process with faster scheduler and memory optimization</span><br>pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)<br><span class="hljs-comment"># remove following line if xformers is not installed</span><br>pipe.enable_xformers_memory_efficient_attention()<br><br>pipe.enable_model_cpu_offload()<br><br>control_image = load_image(<span class="hljs-string">&quot;./conditioning_image_1.png&quot;</span>)<br>prompt = <span class="hljs-string">&quot;pale golden rod circle with old lace background&quot;</span><br><br><span class="hljs-comment"># generate image</span><br>generator = torch.manual_seed(<span class="hljs-number">0</span>)<br>image = pipe(<br>     prompt, num_inference_steps=<span class="hljs-number">20</span>, generator=generator, image=control_image<br>).images[<span class="hljs-number">0</span>]<br><br>image.save(<span class="hljs-string">&quot;./output.png&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>这里的base_model_path 和 controlnet_path 改成之前设置的MODEL_DIR和OUTPUT_DIR(注意顺序)</p><p>接下来运行就可 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python inference.py<br></code></pre></td></tr></table></figure></p><p>结果会被保存到<code>output.png</code></p><h3 id="踩坑解决-1">踩坑解决</h3><h4 id="warning-the-scripts-accelerate-accelerate-config-and-accelerate-launch-are-installed-in-homeubuntu.localbin-which-is-not-on-path.consider-adding-this-directory-to-path-or-if-you-prefer-to-suppress-this-warning-use---no-warn-script-location.">WARNING: The scripts accelerate, accelerate-config and accelerate-launch are installed in '/home/ubuntu/.local/bin' which is not on PATH.Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</h4><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure><p>类似的问题，这里的依赖已经安装成功了，只是被安装到了未被添加到PATH的目录，接下来运行的时候只需要指明目录即可。例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>I3D笔记</title>
      <link>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Sun, 23 Apr 2023 22:14:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;i3d笔记&quot;&gt;I3D笔记&lt;/h1&gt;
&lt;p&gt;I3D是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics&lt;/p&gt;
&lt;h2 id=&quot;工作回顾&quot;&gt;工作回顾&lt;/h2&gt;
&lt;figure&gt;
&lt;img </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="i3d笔记">I3D笔记</h1><p>I3D是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics</p><h2 id="工作回顾">工作回顾</h2><figure><img src="https://proxy.thisis.plus/20230423215707.png" alt="" /><figcaption>image.png</figcaption></figure><p>在以前，视频理解有三种做法 1. LSTM 2. 3D ConvNets 3. Two-Stream Networks（双流网络）</p><h2 id="two-stream-inflated-3d-convnets">Two-Stream Inflated 3D ConvNets</h2><p>这篇文章提出的模型被称为 Two-Stream Inflated 3D ConvNets</p><p>Inflate 是模型的核心操作，含义是将一个2d模型"膨胀"成3d模型，做法很简单，就是把一个<span class="math inline">\(N*N\)</span> 的层变成<span class="math inline">\(N*N*N\)</span> ,同时也将参数复制了<span class="math inline">\(N\)</span> 遍。</p><h2 id="kinetics">Kinetics</h2><p>在视频领域，在一个足够大的数据集上训练一个动作分类网络，当应用于不同的时间任务或数据集时，是否会有类似的性能提升是一个悬而未决的问题。构建视频数据集的挑战意味着大多数流行的动作识别基准。</p><p>Kinetics 有400个人体动作类，每个类有400多个例子，每个都来自一个独特的 YouTube 视频</p><h3 id="整体架构">整体架构</h3><p>作者选择了 Inception-v1 构建整个神经网络(作者当时不适用Inception-v1是因为当时认为Inception在视频理解更合适，但架不住ResNet 太棒了，作者在18年也换成了ResNet) <img src="https://proxy.thisis.plus/20230423220521.png" alt="image.png" /></p><p>图中的Inc. 就是经典的Inception-v1 块了，只是做了Inflating 操作</p>]]></content:encoded>
      
      
      
      
      <comments>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>clip_interrogator教程</title>
      <link>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/</link>
      <guid>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/</guid>
      <pubDate>Sat, 22 Apr 2023 22:24:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;clip_interrogator教程&quot;&gt;clip_interrogator教程&lt;/h1&gt;
&lt;p&gt;文字生成图片是近年来多模态和大模型研究的热门方向，openai提出的CLIP提供了一个方法建立起了图片和文字的联系，但是只能做到给定一张图片选择给定文本语义最相近的那</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="clip_interrogator教程">clip_interrogator教程</h1><p>文字生成图片是近年来多模态和大模型研究的热门方向，openai提出的CLIP提供了一个方法建立起了图片和文字的联系，但是只能做到给定一张图片选择给定文本语义最相近的那一个，实际项目开发中我们总是需要从一张图片获取描述，感谢社区的活力，clip-interrogator应运而生。</p><p>受限于clip-interrogator 等于没有的文档，就有了这篇文章来写一些怎么使用clip-interrogator。</p><p>clip-interrogator项目地址<a href="https://github.com/pharmapsychotic/clip-interrogator">GitHub</a></p><p>在线体验<a href="https://huggingface.co/spaces/pharma/CLIP-Interrogator">huggingface-clip-interrogator</a> <a href="https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">huggingface-clip-interrogator2</a></p><h2 id="clip-interrogator原理">clip-interrogator原理</h2><p>首先，clip-interrogator会使用BILP生成一段对图片的自然语言描述。</p><p>接下来会根据四种模式，从data文件夹下的txt文件中组合出文字生成图片常用的prompt,通过CLIP进行编码，然后将图片也用CLIP进行编码，计算出相似度最大的一组prompt,和BILP生成的prompt拼接到一起，就得到了一组prompt。</p><h2 id="安装">安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.5.4<br></code></pre></td></tr></table></figure><p>如果需要BLIP2最新的WIP支持，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.6.0<br></code></pre></td></tr></table></figure></p><h2 id="使用">使用</h2><h3 id="快速开始">快速开始</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate(image))<br></code></pre></td></tr></table></figure><p>将<code>image_path</code> 换成自己图片的路径即可</p><h3 id="模型">模型</h3><h4 id="blip">BLIP</h4><p>BLIP可以传入两种选项，<code>large</code> 和 <code>base</code>，默认使用<code>large</code></p><p>base用法是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(caption_model_name=<span class="hljs-string">&#x27;blip-base&#x27;</span>,clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><h4 id="clip">CLIP</h4><p>这里使用的模型的是openai的ViT-L-14。</p><p>我们也可以更改模型，文档在这完全没说清可以用什么，我做了试错</p><p>报错显示可用的模型有<code>'coca_base', 'coca_roberta-ViT-B-32', 'coca_ViT-B-32', 'coca_ViT-L-14', 'convnext_base', 'convnext_base_w', 'convnext_base_w_320', 'convnext_large', 'convnext_large_d', 'convnext_large_d_320', 'convnext_small', 'convnext_tiny', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_xxlarge_320', 'mt5-base-ViT-B-32', 'mt5-xl-ViT-H-14', 'RN50', 'RN50-quickgelu', 'RN50x4', 'RN50x16', 'RN50x64', 'RN101', 'RN101-quickgelu', 'roberta-ViT-B-32', 'swin_base_patch4_window7_224', 'ViT-B-16', 'ViT-B-16-plus', 'ViT-B-16-plus-240', 'ViT-B-32', 'ViT-B-32-plus-256', 'ViT-B-32-quickgelu', 'ViT-bigG-14', 'ViT-e-14', 'ViT-g-14', 'ViT-H-14', 'ViT-H-16', 'ViT-L-14', 'ViT-L-14-280', 'ViT-L-14-336', 'ViT-L-16', 'ViT-L-16-320', 'ViT-M-16', 'ViT-M-16-alt', 'ViT-M-32', 'ViT-M-32-alt', 'ViT-S-16', 'ViT-S-16-alt', 'ViT-S-32', 'ViT-S-32-alt', 'vit_medium_patch16_gap_256', 'vit_relpos_medium_patch16_cls_224', 'xlm-roberta-base-ViT-B-32', 'xlm-roberta-large-ViT-H-14'</code></p><blockquote><p>这里其实是我一直没搞懂的一个地方，经过很多次试错,<code>/</code> 前面被称为model，但是很多模型是用不了的，<code>/</code> 后面被称作 tag (通过读源码猜测是预训练模型来源) ，是可以写不同的内容，例如<code>openai</code> ，有的时候还需要不填，但是究竟可以怎么组合一直没找到，下面做了一个总结,</p></blockquote><table><thead><tr class="header"><th>模型</th><th>tag</th></tr></thead><tbody><tr class="odd"><td>coca_base</td><td>不传</td></tr><tr class="even"><td>RN50</td><td>'openai', 'yfcc15m', 'cc12m'</td></tr><tr class="odd"><td>RN50-quickgelu</td><td>'openai', 'yfcc15m', 'cc12m'</td></tr><tr class="even"><td>RN101</td><td>'openai', 'yfcc15m'</td></tr><tr class="odd"><td>RN101-quickgelu</td><td>'openai', 'yfcc15m'</td></tr><tr class="even"><td>RN50x4</td><td>'openai'</td></tr><tr class="odd"><td>RN50x16</td><td>'openai'</td></tr><tr class="even"><td>RN50x64</td><td>'openai'</td></tr><tr class="odd"><td>ViT-B-32</td><td>'openai', 'laion400m_e31', 'laion400m_e32', 'laion2b_e16', 'laion2b_s34b_b79k'</td></tr><tr class="even"><td>ViT-B-32-quickgelu</td><td>'openai', 'laion400m_e31', 'laion400m_e32'</td></tr><tr class="odd"><td>ViT-B-16</td><td>'openai', 'laion400m_e31', 'laion400m_e32', 'laion2b_s34b_b88k'</td></tr><tr class="even"><td>ViT-L-14-336</td><td>'openai'</td></tr><tr class="odd"><td>ViT-S-32-alt</td><td>不传</td></tr><tr class="even"><td>ViT-S-32</td><td>不传</td></tr><tr class="odd"><td>ViT-S-16-alt</td><td>不传</td></tr><tr class="even"><td>ViT-S-16</td><td>不传</td></tr><tr class="odd"><td>ViT-M-32-alt</td><td>不传</td></tr><tr class="even"><td>ViT-M-32</td><td>不传</td></tr><tr class="odd"><td>ViT-M-16-alt</td><td>不传</td></tr><tr class="even"><td>ViT-M-16</td><td>不传</td></tr><tr class="odd"><td>xlm-roberta-base-ViT-B-32</td><td>'laion5b_s13b_b90k'</td></tr><tr class="even"><td>xlm-roberta-large-ViT-H-14</td><td>'frozen_laion5b_s13b_b90k'</td></tr></tbody></table><blockquote><p>不传的意思是不写<code>/</code> 后面的部分不是只写模型名字，正确的用法例如<code>coca_base/</code></p></blockquote><p>例如使用<code>RN50-quickgelu/openai</code> 的用法就是<code>ci = Interrogator(Config(clip_model_name="RN50-quickgelu/openai"))</code></p><blockquote><p>文档中有这么一句ViT-L for Stable Diffusion 1, and ViT-H for Stable Diffusion 2，意思是 ViT-L 是给 Stable Diffusion 1 用的，ViT-H是给 Stable Diffusion 2 用的</p></blockquote><h3 id="模式">模式</h3><p>模式有<code>best</code> ， <code>classic</code>， <code>fast</code>和<code>negative</code> 三种，开发者在这里的设计很奇怪，不同模式的使用不是传不同的参数而是使用不同的方法。<code>best</code> 模式就是上面的用法，<code>fast</code> 模式的用法是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><p><code>classic</code> 模式用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_classic(image))<br></code></pre></td></tr></table></figure></p><p><code>negative</code> 模式用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_negative(image))<br></code></pre></td></tr></table></figure></p><h3 id="quiet">quiet</h3><p><code>quiet</code> 选项的作用是不输出中间过程，使用方法是直接写进Config 即可 ，例如 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/content/test.png&#x27;</span>).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>,quiet=<span class="hljs-literal">True</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><p>使用前，会有各种进度条 <img src="https://proxy.thisis.plus/20230422221658.png" alt="image.png" /></p><p>使用后，所有过程中的输出会被隐藏 <img src="https://proxy.thisis.plus/20230422221818.png" alt="image.png" /></p><h2 id="自定义词库">自定义词库</h2><p>如果你安装的是0.6.0，那么可以使用自定义词库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator, LabelTable, load_list<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>ci = Interrogator(Config(blip_model_type=<span class="hljs-literal">None</span>))<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>table = LabelTable(load_list(<span class="hljs-string">&#x27;terms.txt&#x27;</span>), <span class="hljs-string">&#x27;terms&#x27;</span>, ci)<br>best_match = table.rank(ci.image_to_features(image), top_count=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(best_match)<br></code></pre></td></tr></table></figure>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ControlNet代码改造计划</title>
      <link>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/</link>
      <guid>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/</guid>
      <pubDate>Fri, 21 Apr 2023 11:30:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;虽然现在webui已经支持了ControlNet，但是如果我们需要单独抽出来ControlNet做一些项目就需要对ControlNet进行改造。同时我也想加入一些开源的工具让ControlNet更加有趣，例如&lt;a href=&quot;https://github.com/pharm</description>
        
      
      
      
      <content:encoded><![CDATA[<p>虽然现在webui已经支持了ControlNet，但是如果我们需要单独抽出来ControlNet做一些项目就需要对ControlNet进行改造。同时我也想加入一些开源的工具让ControlNet更加有趣，例如<a href="https://github.com/pharmapsychotic/clip-interrogator">clip_interrogator</a>.</p><p>关于什么是Canny，Hough，可以看北邮鲁鹏老师的课程<a href="https://www.bilibili.com/video/BV1nz4y197Qv/?spm_id_from=333.999.0.0&amp;vd_source=e8f062c423dc7ce759a573dd732735a0">计算机视觉（本科）北京邮电大学 鲁鹏</a></p><p>如果你想在webui使用ControlNet，可以看我之前的<a href="https://studyinglover.com/2023/03/20/%E9%80%9A%E8%BF%87colab%E4%BD%93%E9%AA%8CControlNet/">文章</a> ，或者直接查看<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">webui</a></p><p>项目开源在<a href="https://github.com/StudyingLover/cmd_ControlNet">GitHub</a></p><p>我的博客<a href="https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/">https://studyinglover.com</a></p><h2 id="下载源码和模型">下载源码和模型</h2><p>ControlNet项目主页</p><p><a href="https://github.com/lllyasviel/ControlNet">github</a> , <a href="https://huggingface.co/lllyasviel/ControlNet">huggingface</a></p><p>先下载源码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/lllyasviel/ControlNet.git <br></code></pre></td></tr></table></figure></p><p>接下来下载需要的模型，进入huggingface 页面，选择<code>files and versions</code> <img src="https://proxy.thisis.plus/20230421105906.png" alt="image.png" /></p><p>先下载所有的annotator,进入<code>annotator/ckpts</code>文件夹,可以看到我们需要的ckpts文件，进入一个，右键download，选择复制下载链接 <img src="https://proxy.thisis.plus/20230421110144.png" alt="image.png" /></p><p>执行命令，就会将模型下载下来 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget 复制的链接<br></code></pre></td></tr></table></figure></p><p>我这里整理了ckpt文件所有的下载的链接和命令,<code>/root/ControlNet/annotator/ckpts/</code> 是我的路径，换成你自己的就行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/ControlNet/annotator/ckpts/<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth<br></code></pre></td></tr></table></figure></p><p>接下来下载模型，假如我们需要canny2image，那我就需要下载<code>control_sd15_canny.pth</code> 这个文件，类似上面的方法，命令是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth<br></code></pre></td></tr></table></figure></p><blockquote><p>其实我们可以对比网址链接和下载链接</p><p>网址链接：https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_canny.pth</p><p>下载链接：https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth</p><p>我们只需要把网址链接的blob换成resolve就可以了。</p></blockquote><h2 id="改造">改造</h2><p>我们依然以camny2image为例，打开<code>gradio_canny2image,py</code> 文件，可以看到这个文件大概是这个样子 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> 各种依赖<br><br>apply_canny = CannyDetector()<span class="hljs-comment"># 创建了一个canny算子，用来将图片转换成canny图</span><br><br>model = create_model(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>).cpu()<br>model.load_state_dict(load_state_dict(<span class="hljs-string">&#x27;./models/control_sd15_canny.pth&#x27;</span>, location=<span class="hljs-string">&#x27;cuda&#x27;</span>))<br>model = model.cuda()<br>ddim_sampler = DDIMSampler(model)<span class="hljs-comment"># 加载了模型</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>(<span class="hljs-params">input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold</span>):<br>    <span class="hljs-comment"># 一堆操作</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">255</span> - detected_map] + results<span class="hljs-comment"># 返回了canny图和生成的图片</span><br><br>block = gr.Blocks().queue()<span class="hljs-comment"># 创建一个gradio应用</span><br><span class="hljs-keyword">with</span> block:<br><span class="hljs-comment"># 又是一通操作，创建了各种gradio页面</span><br><br>block.launch(server_name=<span class="hljs-string">&#x27;0.0.0.0&#x27;</span>)<span class="hljs-comment"># 启动了gradio应用</span><br></code></pre></td></tr></table></figure></p><p>这样子我们只需要<code>process</code> 这个函数就可以了，那我们就可以把代码改成这样 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> share <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> config<br><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> einops<br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">from</span> pytorch_lightning <span class="hljs-keyword">import</span> seed_everything<br><span class="hljs-keyword">from</span> annotator.util <span class="hljs-keyword">import</span> resize_image, HWC3<br><span class="hljs-keyword">from</span> annotator.canny <span class="hljs-keyword">import</span> CannyDetector<br><span class="hljs-keyword">from</span> cldm.model <span class="hljs-keyword">import</span> create_model, load_state_dict<br><span class="hljs-keyword">from</span> cldm.ddim_hacked <span class="hljs-keyword">import</span> DDIMSampler<br><br>apply_canny = CannyDetector()<br>  <br>model = create_model(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>).cpu()<br><br>model.load_state_dict(load_state_dict(<span class="hljs-string">&#x27;./models/control_sd15_canny.pth&#x27;</span>, location=<span class="hljs-string">&#x27;cuda&#x27;</span>))<br>model = model.cuda()<br>ddim_sampler = DDIMSampler(model)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>(<span class="hljs-params">input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold</span>):<br><span class="hljs-comment"># 这块直接复制源码process函数</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">255</span> - detected_map] + results<br><br><br><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;__main__&#x27;</span> == __name__:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--image_path&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;test.png&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;original image path&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;1people&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--a_prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;best quality, extremely detailed&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;added prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--n_prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;negative prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_samples&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;number of samples&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--image_resolution&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">512</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;image resolution&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--ddim_steps&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">30</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;ddim steps&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--is_saved&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">True</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;is saved?&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--is_show&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">False</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;is show?&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--guess_mode&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">False</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;guess mode&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--strength&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">1.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;strength&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--scale&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">9.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;scale&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=-<span class="hljs-number">1</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;seed&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--eta&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;eta&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--low_threshold&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">100</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;low threshold&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--high_threshold&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">200</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;high threshold&#x27;</span>)<br>  <br><br>    opt = parser.parse_args()<br>    <br>    img=cv2.imread(opt.image_path)<br>    out=process(img, opt.prompt, opt.a_prompt, opt.n_prompt, opt.num_samples, opt.image_resolution, opt.ddim_steps, opt.guess_mode, opt.strength, opt.scale, opt.seed, opt.eta, opt.low_threshold, opt.high_threshold)<br><br>    <span class="hljs-keyword">if</span>(opt.is_show):<br>        cv2.imshow(<span class="hljs-string">&#x27;out&#x27;</span>,out[<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">if</span>(opt.is_saved):<br>        cv2.imwrite(<span class="hljs-string">&#x27;out.png&#x27;</span>,out[<span class="hljs-number">1</span>])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;saved to out.png&#x27;</span>)<br></code></pre></td></tr></table></figure></p><p>用法就是parser的用法，例如 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python cmd_canny2image.py --image_path ./test_imgs/main.png --prompt bule_hair,color_clothes <br></code></pre></td></tr></table></figure> 输出的图片会被保存到当前目录下的out.png文件</p><h2 id="clip_interrogator">clip_interrogator</h2><p>你可以在<a href="https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">huggingface</a> 直接体验，这里是代码调用相应接口。</p><p><a href="https://studyinglover.com/2023/04/22/clip_interrogator教程/">clip_interrogator教程</a></p><p>先下载 clip_interrogator <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.5.4<br></code></pre></td></tr></table></figure></p><p>接下来调用 clip_interrogator <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>img=cv.imread(<span class="hljs-string">&#x27;/content/src/test.png&#x27;</span>)<br>img = cv.cvtColor(img,cv.COLOR_BGR2RGB)<br>img = Image.fromarray(img)<br><br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br><br>describe=ci.interrogate(img)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>+describe)<br></code></pre></td></tr></table></figure></p><p>我们将他封装成函数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">app</span>(<span class="hljs-params">numpy_img</span>):<br>    img = cv.cvtColor(numpy_img,cv.COLOR_BGR2RGB)<br>    img = Image.fromarray(img)<br>    ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br>    describe=ci.interrogate(img)<br>    <span class="hljs-keyword">return</span> describe<br></code></pre></td></tr></table></figure></p><p>在 <a href="https://colab.research.google.com/github/StudyingLover/cmd_ControlNet/blob/master/fix_ControlNet_and_CLIPinterrogator.ipynb"><img src="https://proxy.thisis.plus/colab-badge.svg" alt="Open In Colab" /></a> 可以体验 ControlNet和CLIPinterrogator 混合使用，两张图片都从url引入，一张图获取prompt，ptompt和另一张图一起输入输入canny2image，生成的图片展示在输出框底部</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
