<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>plus studio</title>
    <link>https://studyinglover.com/</link>
    
    <atom:link href="https://studyinglover.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 23 Jun 2023 15:02:34 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>clip-interrogator代码解析</title>
      <link>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <guid>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <pubDate>Fri, 23 Jun 2023 22:59:40 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;clip-interrogator代码解析&quot;&gt;clip-interrogator代码解析&lt;/h1&gt;
&lt;p&gt;clip-interrogator 的的主要代码在仓库的&lt;code&gt;./clip-interrogator&lt;/code&gt; 文件夹下 &lt;figure class</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="clip-interrogator代码解析">clip-interrogator代码解析</h1><p>clip-interrogator 的的主要代码在仓库的<code>./clip-interrogator</code> 文件夹下 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">.<br>├── clip_interrogator<span class="hljs-selector-class">.py</span><br>├── data<br>│   ├── artists<span class="hljs-selector-class">.txt</span><br>│   ├── flavors<span class="hljs-selector-class">.txt</span><br>│   ├── mediums<span class="hljs-selector-class">.txt</span><br>│   ├── movements<span class="hljs-selector-class">.txt</span><br>│   └── negative<span class="hljs-selector-class">.txt</span><br>└── __init__<span class="hljs-selector-class">.py</span><br><br></code></pre></td></tr></table></figure></p><p>这里主要解析<code>clip-interrogator.py</code> 文件。</p><h2 id="init.py"><strong>init</strong>.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> .clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator, LabelTable, list_caption_models, list_clip_models, load_list<br><br>__version__ = <span class="hljs-string">&#x27;0.6.0&#x27;</span><br>__author__ = <span class="hljs-string">&#x27;pharmapsychotic&#x27;</span><br></code></pre></td></tr></table></figure><p>这个 <code>__init__.py</code> 文件的作用是在包被导入时执行初始化操作，并提供了版本号和作者信息。</p><h2 id="clip_interrogator.py">clip_interrogator.py</h2><p>文件的大致结构是这样的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> 需要的库<br><br>CAPTION_MODELS = &#123;<br><span class="hljs-string">&#x27;blip-base&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-base&#x27;</span>, <span class="hljs-comment"># 990MB</span><br><span class="hljs-string">&#x27;blip-large&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip-image-captioning-large&#x27;</span>, <span class="hljs-comment"># 1.9GB</span><br><span class="hljs-string">&#x27;blip2-2.7b&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-opt-2.7b&#x27;</span>, <span class="hljs-comment"># 15.5GB</span><br><span class="hljs-string">&#x27;blip2-flan-t5-xl&#x27;</span>: <span class="hljs-string">&#x27;Salesforce/blip2-flan-t5-xl&#x27;</span>, <span class="hljs-comment"># 15.77GB</span><br><span class="hljs-string">&#x27;git-large-coco&#x27;</span>: <span class="hljs-string">&#x27;microsoft/git-large-coco&#x27;</span>, <span class="hljs-comment"># 1.58GB</span><br>&#125;<br><br>CACHE_URL_BASE = <span class="hljs-string">&#x27;https://huggingface.co/pharma/ci-preprocess/resolve/main/&#x27;</span><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Config</span>:<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Interrogator</span>():<br>    具体实现<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LabelTable</span>():<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_download_file</span>(<span class="hljs-params">url: <span class="hljs-built_in">str</span>, filepath: <span class="hljs-built_in">str</span>, chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">4</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>, quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge_tables</span>(<span class="hljs-params">tables: <span class="hljs-type">List</span>[LabelTable], ci: Interrogator</span>) -&gt; LabelTable:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prompt_at_max_len</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_truncate_to_fit</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, tokenize</span>) -&gt; <span class="hljs-built_in">str</span>:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_caption_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">list_clip_models</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_list</span>(<span class="hljs-params">data_path: <span class="hljs-built_in">str</span>, filename: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>    具体实现<br></code></pre></td></tr></table></figure></p><p><code>CAPTION_MODELS</code> 定义了各个所需要的模型在huggingface 地址。<code>CACHE_URL_BASE</code> 是缓存地址</p><h3 id="config-class">Config class</h3><p>首先定义了CLIP和BILP模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">caption_model = <span class="hljs-literal">None</span><br>caption_processor = <span class="hljs-literal">None</span><br>clip_model = <span class="hljs-literal">None</span><br>clip_preprocess = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure></p><p>接下来对BLIP和CLIP进行了详细的设置2 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># blip settings</span><br>caption_max_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span><br>caption_model_name: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-string">&#x27;blip-large&#x27;</span> <span class="hljs-comment"># use a key from CAPTION_MODELS or None</span><br>caption_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br>  <br><span class="hljs-comment"># clip settings</span><br>clip_model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;ViT-L-14/openai&#x27;</span><br>clip_model_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span><br>clip_offload: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure></p><p>这段代码是Config类中与Interrogator类相关的配置参数。</p><p>接下来定义了interrogator的相关设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">cache_path: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;cache&#x27;</span> <span class="hljs-comment"># 存储缓存的文本嵌入的路径</span><br>download_cache: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span> <span class="hljs-comment"># 是否从huggingface下载缓存的嵌入向量</span><br>chunk_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span> <span class="hljs-comment"># CLIP的批处理大小</span><br>data_path: <span class="hljs-built_in">str</span> = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data&#x27;</span>)<span class="hljs-comment"># 数据文件的路径</span><br>device: <span class="hljs-built_in">str</span> = (<span class="hljs-string">&quot;mps&quot;</span> <span class="hljs-keyword">if</span> torch.backends.mps.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>flavor_intermediate_count: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span><br>quiet: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span> <span class="hljs-comment"># 是否显示进度条</span><br></code></pre></td></tr></table></figure></p><p><code>apply_low_vram_defaults</code>方法，用于将配置参数设置为适合低显存设备的默认值。在该方法中，将一些参数设置为较小的值，以减少显存的使用。</p><h3 id="interrogator-class">Interrogator class</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Config</span>):<br>self.config = config<br>self.device = config.device<br>self.dtype = torch.float16 <span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">else</span> torch.float32<br>self.caption_offloaded = <span class="hljs-literal">True</span><br>self.clip_offloaded = <span class="hljs-literal">True</span><br>self.load_caption_model()<br>self.load_clip_model()<br></code></pre></td></tr></table></figure><p>继承了<code>Config</code> 类中的一些配置。</p><h4 id="load_caption_model">load_caption_model</h4><p>这个方法用于加载图像描述模型。首先判断配置中是否直接传入了图像描述模型对象，并且是否指定了图像描述模型名称。如果没有直接传入模型对象并且指定了模型名称，则根据模型名称加载对应的模型。加载过程中根据模型名称的不同选择不同的加载方式。加载完成后，将模型设置为eval模式，并根据配置决定是否将模型移动到指定的设备上</p><h4 id="load_clip_model">load_clip_model</h4><p>这个方法用于加载CLIP模型。首先根据配置中指定的CLIP模型名称解析出模型名称和预训练模型名称。然后判断配置中是否直接传入了CLIP模型对象。如果没有直接传入模型对象，则根据模型名称和预训练模型名称加载模型。加载过程中会调用<code>open_clip.create_model_and_transforms()</code>方法创建模型和预处理函数，并设置模型为eval模式。加载完成后，将模型和预处理函数保存到对应的属性中。</p><p>接下来，根据配置中的数据路径加载一些标签数据，并创建<code>LabelTable</code>对象。<code>LabelTable</code>类用于管理标签和对应的嵌入向量。这里创建了artists、flavors、mediums、movements、trendings和negative等LabelTable对象。</p><p>最后，打印加载CLIP模型和数据所花费的时间。</p><h4 id="chain">chain</h4><p>这个方法用于它用于在一组短语中选择最佳的短语，以构建一个完整的提示。</p><p>首先调用_prepare_clip()方法，准备CLIP模型。</p><p>然后，将短语列表转换为一个集合，方便操作。如果没有指定最佳提示，则通过调用rank_top()方法选择当前短语列表中与图像特征最相似的短语作为最佳提示，并计算其相似度。然后从短语集合中移除最佳提示。</p><p>接下来，使用curr_prompt和curr_sim变量保存当前的提示和相似度。</p><p>定义了一个名为check的内部函数，用于检查给定的附加短语是否应该成为当前提示的一部分。该函数会根据相似度比较结果更新最佳提示和最佳相似度，并判断是否需要更新当前提示。</p><p>使用一个循环遍历max_count次，每次迭代中选择当前短语列表中与当前提示加上附加短语后最相似的短语作为最佳短语。然后将该短语的一部分（从curr_prompt的长度加2开始）作为附加短语。调用check()函数进行相似度比较和更新。</p><p>在循环过程中，如果当前提示已经达到了最大长度，则停止迭代。最后，返回最佳提示。</p><h4 id="generate_caption">generate_caption</h4><p>使用BILP生成图像的描述。它首先对图像进行预处理，然后使用图像描述模型生成描述的tokens，最后将tokens解码为文本描述。</p><h4 id="image_to_features">image_to_features</h4><p>使用CLIP的图像编码器将图片转换成torch格式的特征</p><h4 id="interrogate">interrogate</h4><p><code>interrogate_classic</code> 首先生成一个标准格式的提示，描述图像，然后列出艺术家、趋势、风格和口味等文本修饰符。它使用了mediums、artists、trendings、movements和flavors等LabelTable对象来选择相应的修饰符。</p><p><code>interrogate_fast</code> 在生成的描述后面简单地添加排名靠前的词语。它通常比经典模式产生更好的生成提示和图像之间的相似度，但提示的可读性较差。它使用了artists、flavors、mediums、movements和trendings等LabelTable对象来选择排名靠前的词语。</p><p><code>interrogate_negative</code> 主要生成负面词汇，将与图像最不相似的词语连接在一起。它可以用于构建与正面提示相对应的负面提示，并且通常可以改善生成图像的结果，特别是在使用稳定扩散2（Stable Diffusion 2）时。它使用了flavors和negative等LabelTable对象来选择最不相似的词语。</p><p><code>interrogate</code> 会生成一个完整的提示。首先生成一个基于图像的描述，然后根据图像特征和LabelTable对象生成一组修饰符。然后使用chain方法选择最佳的修饰符，并根据相似度和一些条件选择最佳提示。最后，根据生成的多个提示的相似度，选择最终的生成提示。</p><h4 id="prepare_caption">_prepare_caption</h4><p>用于加载BLIP模型。</p><h4 id="prepare_clip">_prepare_clip</h4><p>用于加载CLIP模型。</p><h4 id="rank_top">rank_top</h4><p>这个方法用于对文本进行排名，并返回排名最高的文本。</p><p>首先加载CLIP模型。使用tokenize方法将文本数组转换为文本tokens，并将其移动到设备上。</p><p>然后，使用<code>clip_model</code>的<code>encode_text</code>方法对文本tokens进行编码，得到文本的特征向量。对特征向量进行归一化处理，使其长度为1。接着，计算文本特征向量与图像特征向量之间的相似度。通过计算特征向量的点积得到相似度。如果<code>reverse</code>为<code>True</code>，则将相似度取负，以实现按相似度降序排列。最后，返回排名最高的文本，即相似度最大的文本。</p><h4 id="similarity和similarities">similarity和similarities</h4><p>通过计算点积的方式计算了相似度</p><h3 id="labeltable-class">LabelTable class</h3><p>这个类创建标签，并对标签进行排名</p><h4 id="init"><strong>init</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, labels:<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], desc:<span class="hljs-built_in">str</span>, ci: Interrogator</span>):<br>clip_model, config = ci.clip_model, ci.config<br>self.chunk_size = config.chunk_size<br>self.config = config<br>self.device = config.device<br>self.embeds = []<br>self.labels = labels<br>self.tokenize = ci.tokenize<br>  <br><span class="hljs-built_in">hash</span> = hashlib.sha256(<span class="hljs-string">&quot;,&quot;</span>.join(labels).encode()).hexdigest()<br>sanitized_name = self.config.clip_model_name.replace(<span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>).replace(<span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>)<br>self._load_cached(desc, <span class="hljs-built_in">hash</span>, sanitized_name)<br>  <br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.labels) != <span class="hljs-built_in">len</span>(self.embeds):<br>self.embeds = []<br>chunks = np.array_split(self.labels, <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(self.labels)/config.chunk_size))<br><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> tqdm(chunks, desc=<span class="hljs-string">f&quot;Preprocessing <span class="hljs-subst">&#123;desc&#125;</span>&quot;</span> <span class="hljs-keyword">if</span> desc <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>, disable=self.config.quiet):<br>text_tokens = self.tokenize(chunk).to(self.device)<br><span class="hljs-keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():<br>text_features = clip_model.encode_text(text_tokens)<br>text_features /= text_features.norm(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>text_features = text_features.half().cpu().numpy()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(text_features.shape[<span class="hljs-number">0</span>]):<br>self.embeds.append(text_features[i])<br>  <br><span class="hljs-keyword">if</span> desc <span class="hljs-keyword">and</span> self.config.cache_path:<br>os.makedirs(self.config.cache_path, exist_ok=<span class="hljs-literal">True</span>)<br>cache_filepath = os.path.join(self.config.cache_path, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sanitized_name&#125;</span>_<span class="hljs-subst">&#123;desc&#125;</span>.safetensors&quot;</span>)<br>tensors = &#123;<br><span class="hljs-string">&quot;embeds&quot;</span>: np.stack(self.embeds),<br><span class="hljs-string">&quot;hash&quot;</span>: np.array([<span class="hljs-built_in">ord</span>(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">hash</span>], dtype=np.int8)<br>&#125;<br>save_file(tensors, cache_filepath)<br>  <br><span class="hljs-keyword">if</span> self.device == <span class="hljs-string">&#x27;cpu&#x27;</span> <span class="hljs-keyword">or</span> self.device == torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>):<br>self.embeds = [e.astype(np.float32) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> self.embeds]<br></code></pre></td></tr></table></figure><p>继承了<code>Interrogator</code> 中的一些内容，同时对embeds 做了预处理。</p><h4 id="load_cached">_load_cached</h4><p>用于加载缓存的嵌入向量。</p><h4 id="rank和rank">_rank和rank</h4><p>用于对图像特征和文本嵌入向量进行排名。<code>_rank</code>方法计算图像特征与文本嵌入向量之间的相似度，并返回排名最高的文本索引。<code>rank</code>方法根据<code>chunk_size</code>的大小，将文本嵌入向量分成多个批次进行排名，然后返回排名最高的文本标签。</p><h2 id="data">data</h2><p>存储了常用的文字生成图片的prompt</p><h2 id="clip-interrogator究竟做了什么">clip-interrogator究竟做了什么</h2><p>首先，clip-interrogator会使用BILP生成一段对图片的自然语言描述。</p><p>接下来会根据四种模式，从data文件夹下的txt文件中组合出文字生成图片常用的prompt,通过CLIP进行编码，然后将图片也用CLIP进行编码，计算出相似度最大的一组prompt,和BILP生成的prompt拼接到一起，就得到了一组prompt。</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/23/clip-interrogator%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>GroundingDINO安装报错解决</title>
      <link>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</link>
      <guid>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</guid>
      <pubDate>Wed, 21 Jun 2023 17:25:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;groundingdino安装报错解决&quot;&gt;GroundingDINO安装报错解决&lt;/h1&gt;
&lt;p&gt;在安装会遇到这个错误 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="groundingdino安装报错解决">GroundingDINO安装报错解决</h1><p>在安装会遇到这个错误 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs bash">  ERROR: Command errored out with <span class="hljs-built_in">exit</span> status 1:<br>   <span class="hljs-built_in">command</span>: /usr/bin/python3 /tmp/tmpmhvo4wyp build_wheel /tmp/tmp3a4xwmi4<br>       cwd: /tmp/pip-install-x0mg8qpf/pycocotools<br>  Complete output (77 lines):<br>  running bdist_wheel<br>  running build<br>  running build_py<br>  creating build<br>  creating build/lib.linux-x86_64-cpython-38<br>  creating build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/coco.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/mask.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/cocoeval.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  copying pycocotools/__init__.py -&gt; build/lib.linux-x86_64-cpython-38/pycocotools<br>  running build_ext<br>  cythoning pycocotools/_mask.pyx to pycocotools/_mask.c<br>  building <span class="hljs-string">&#x27;pycocotools._mask&#x27;</span> extension<br>  creating build/temp.linux-x86_64-cpython-38<br>  creating build/temp.linux-x86_64-cpython-38/common<br>  creating build/temp.linux-x86_64-cpython-38/pycocotools<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c ./common/maskApi.c -o build/temp.linux-x86_64-cpython-38/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToBbox’:<br>  ./common/maskApi.c:151:32: warning: unused variable ‘xp’ [-Wunused-variable]<br>    151 |     uint h, w, xs, ys, xe, ye, xp, cc; siz j, m;<br>        |                                ^~<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrPoly’:<br>  ./common/maskApi.c:197:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |   ^~~<br>  ./common/maskApi.c:197:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    197 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];<br>        |                                                      ^<br>  ./common/maskApi.c:198:3: warning: this ‘<span class="hljs-keyword">for</span>’ clause does not guard... [-Wmisleading-indentation]<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |   ^~~<br>  ./common/maskApi.c:198:54: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">for</span>’<br>    198 |   <span class="hljs-keyword">for</span>(j=0; j&lt;k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];<br>        |                                                      ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleToString’:<br>  ./common/maskApi.c:243:7: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |       ^~<br>  ./common/maskApi.c:243:27: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    243 |       <span class="hljs-keyword">if</span>(more) c |= 0x20; c+=48; s[p++]=c;<br>        |                           ^<br>  ./common/maskApi.c: In <span class="hljs-keyword">function</span> ‘rleFrString’:<br>  ./common/maskApi.c:251:3: warning: this ‘<span class="hljs-keyword">while</span>’ clause does not guard... [-Wmisleading-indentation]<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |   ^~~~~<br>  ./common/maskApi.c:251:22: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">while</span>’<br>    251 |   <span class="hljs-keyword">while</span>( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;<br>        |                      ^~~~<br>  ./common/maskApi.c:259:5: warning: this ‘<span class="hljs-keyword">if</span>’ clause does not guard... [-Wmisleading-indentation]<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |     ^~<br>  ./common/maskApi.c:259:34: note: ...this statement, but the latter is misleadingly indented as <span class="hljs-keyword">if</span> it were guarded by the ‘<span class="hljs-keyword">if</span>’<br>    259 |     <span class="hljs-keyword">if</span>(m&gt;2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;<br>        |                                  ^~~~<br>  x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/usr/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-cpython-38/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99<br>  pycocotools/_mask.c:6:10: fatal error: Python.h: No such file or directory<br>      6 | <span class="hljs-comment">#include &quot;Python.h&quot;</span><br>        |          ^~~~~~~~~~<br>  compilation terminated.<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/setuptools/dist.py:745: SetuptoolsDeprecationWarning: Invalid dash-separated options<br>  !!<br>  <br>          ********************************************************************************<br>          Usage of dash-separated <span class="hljs-string">&#x27;index-url&#x27;</span> will not be supported <span class="hljs-keyword">in</span> future<br>          versions. Please use the underscore name <span class="hljs-string">&#x27;index_url&#x27;</span> instead.<br>  <br>          By 2023-Sep-26, you need to update your project and remove deprecated calls<br>          or your builds will no longer be supported.<br>  <br>          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html <span class="hljs-keyword">for</span> details.<br>          ********************************************************************************<br>  <br>  !!<br>    opt = self.warn_dash_deprecation(opt, section)<br>  /tmp/pip-build-env-xkmgfc0t/overlay/lib/python3.8/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive <span class="hljs-string">&#x27;language_level&#x27;</span> not <span class="hljs-built_in">set</span>, using 2 <span class="hljs-keyword">for</span> now (Py2). This will change <span class="hljs-keyword">in</span> a later release! File: /tmp/pip-install-x0mg8qpf/pycocotools/pycocotools/_mask.pyx<br>    tree = Parsing.p_module(s, pxd, full_module_name)<br>  error: <span class="hljs-built_in">command</span> <span class="hljs-string">&#x27;/usr/bin/x86_64-linux-gnu-gcc&#x27;</span> failed with <span class="hljs-built_in">exit</span> code 1<br>  ----------------------------------------<br>  ERROR: Failed building wheel <span class="hljs-keyword">for</span> pycocotools<br>Failed to build pycocotools<br>ERROR: Could not build wheels <span class="hljs-keyword">for</span> pycocotools <span class="hljs-built_in">which</span> use PEP 517 and cannot be installed directly<br></code></pre></td></tr></table></figure> 细读报错，我们会发现是编译过程中少了一个<code>Python.h</code> 的头文件导致编译pycocotools失败。</p><p>我们尝试直接安装<code>pycocotools</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install pycocotools<br></code></pre></td></tr></table></figure><p>会出现和上面一样的错误。</p><p>google一番,提示说<code>sudo apt-get install libsuitesparse-dev</code></p><p>受到报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) ... error<br> error: subprocess-exited-with-error<br> <br> × Building wheel <span class="hljs-keyword">for</span> pycocotools (pyproject.toml) did not run successfully.<br> │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">1</span><br> ╰─&gt; [<span class="hljs-number">77</span> lines of output]<br></code></pre></td></tr></table></figure></p><p>最后的结果依然是 <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs subunit">  note: This error originates from a subprocess, and is likely not a problem with pip.<br>  ERROR: Failed building wheel for pycocotools<br>Failed to build pycocotools<br><span class="hljs-keyword">ERROR: </span>Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects<br></code></pre></td></tr></table></figure></p><p>尝试通过安装<code>pip install "git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI"</code> 解决</p><p>获得报错 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs awk">fatal: unable to access <span class="hljs-string">&#x27;https://github.com/philferriere/cocoapi.git/&#x27;</span>: GnuTLS recv error (-<span class="hljs-number">110</span>): The TLS connection was non-properly terminated.<br>  error: subprocess-exited-with-error<br>  <br>  × git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>  │ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>  ╰─&gt; See above <span class="hljs-keyword">for</span> output.<br>  <br>  note: This error originates from a subprocess, and is likely not a problem with pip.<br>error: subprocess-exited-with-error<br><br>× git clone --filter=blob:none --quiet https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/philferriere/</span>cocoapi.git <span class="hljs-regexp">/tmp/</span>pip-install-a4vtujvc/pycocotools_f76f853260a94fd79f5ac4cef5f3a557 did not run successfully.<br>│ <span class="hljs-keyword">exit</span> code: <span class="hljs-number">128</span><br>╰─&gt; See above <span class="hljs-keyword">for</span> output.<br></code></pre></td></tr></table></figure></p><p>运行<code>sudo apt install python3.8-dev</code></p><p>然后<code>git clone https://github.com/cocodataset/cocoapi.git</code> , <code>cd ./cocoapi/PythonAPI</code> ,接下来 <code>make</code></p><p>运行<code>pip install -e .</code> ,成功安装<code>pycocotools</code> .</p><p>再次运行<code>pip install GroundingDINO</code> , 成功。</p><figure><img src="https://proxy.thisis.plus/202306211724652.png" alt="" /><figcaption>image.png</figcaption></figure>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/21/GroundingDINO%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</title>
      <link>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/</link>
      <guid>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/</guid>
      <pubDate>Mon, 19 Jun 2023 23:06:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&quot;&gt;2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评&lt;/h1&gt;
&lt;h2 id=&quot;鲲鹏活动&quot;&gt;鲲鹏活动&lt;/h2&gt;
&lt;p&gt;我是白吃白喝来的你真以为我是来学技术的？&lt;/p&gt;
&lt;h3 id=&quot;上午场&quot;&gt;上午场&lt;/h3</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评">2023华为鲲鹏畅想日暨西安高新国际会议中心零食午饭测评</h1><h2 id="鲲鹏活动">鲲鹏活动</h2><p>我是白吃白喝来的你真以为我是来学技术的？</p><h3 id="上午场">上午场</h3><p><img src="https://proxy.thisis.plus/202306190655993.jpg" /></p><p>院士发言，讲了从教多年开设公共课的历程，还有将核心技术掌握在自己手里的重要性，举了上世纪欧洲软件和20年哈工大哈工程被禁用matlab的例子。 <img src="https://proxy.thisis.plus/202306190655529.jpg" /></p><p>然后我就牙疼的受不了看牙去了……</p><h3 id="下午场">下午场</h3><p>下午场有三部分，星享会，鲲鹏训练营还有人才发展论坛。星享会是关于互联网+产业命题赛道和鲲鹏应用创新大赛的分享。鲲鹏训练营没有参与，我猜是用类似华为云的沙盒做实验。人才发展论坛是大佬们发言讲自己做的一些研究和人才培养模式 <img src="https://proxy.thisis.plus/202306190703495.jpg" alt="IMG_20230617_135653.jpg" /></p><figure><img src="https://proxy.thisis.plus/202306190704505.jpg" alt="" /><figcaption>IMG_20230617_173651.jpg</figcaption></figure><figure><img src="https://proxy.thisis.plus/202306190704514.jpg" alt="" /><figcaption>IMG_20230617_165145.jpg</figcaption></figure><h2 id="零食午饭测评">零食午饭测评</h2><p>因为牙疼刚做了根管的缘故没有吃的太全，所以只能聊一聊自己吃了的部分</p><h3 id="午饭">午饭</h3><p><img src="https://proxy.thisis.plus/202306190642829.jpg" /></p><p>左边粥是皮蛋瘦肉粥，绝对好评，好喝还适合我这种牙刚做了手术的人，我喝了三碗。</p><p>右边的甜点里我们最远的那一排左边的是蒸饺，正常。右边的不知道叫什么，夹心，正常水平。</p><p>中间的一排虽然长得不一样，都是千层饼。有一点咸味，有点硬不适合那天的牙。</p><p>离我们最近的一排最左边的小蛋糕，我吃了两个，第一个没啥味道，第二个有苦味。中间的豆沙。最右边的，流心绿豆糕，非常好吃，很软口感很好，很适合我的牙，吃了六个吧。</p><h3 id="零食">零食</h3><p><img src="https://proxy.thisis.plus/202306190642391.jpg" /></p><p>右边的饮料据工作人员说是他们自己调的，气泡莫吉托，挺好喝的，有碳酸饮料的感觉 <img src="https://proxy.thisis.plus/202306190642335.jpg" /></p><p>左边盘子里左上是芒果蛋糕，右上草莓蛋糕。芒果蛋糕整个是芒果和奶油，草莓蛋糕正常。左下不好吃，很硬没啥味道。右下核桃芯还是很好吃的。</p><h2 id="收获">收获</h2><p>一本基于鲲鹏的大数据挖掘de书，两个水杯，一个肩带，一个文化衫</p>]]></content:encoded>
      
      
      
      
      <comments>https://studyinglover.com/2023/06/19/2023%E5%8D%8E%E4%B8%BA%E9%B2%B2%E9%B9%8F%E7%95%85%E6%83%B3%E6%97%A5%E6%9A%A8%E8%A5%BF%E5%AE%89%E9%AB%98%E6%96%B0%E5%9B%BD%E9%99%85%E4%BC%9A%E8%AE%AE%E4%B8%AD%E5%BF%83%E9%9B%B6%E9%A3%9F%E5%8D%88%E9%A5%AD%E6%B5%8B%E8%AF%84/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RoboMaster开源仓库汇总(长期更新)</title>
      <link>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/</link>
      <guid>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/</guid>
      <pubDate>Sun, 18 Jun 2023 22:40:00 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;视觉&quot;&gt;视觉&lt;/h2&gt;
&lt;h3 id=&quot;陈君&quot;&gt;陈君&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://gitlab.com/rm_vision&quot;&gt;陈君视觉1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/rm-vision-arc</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="视觉">视觉</h2><h3 id="陈君">陈君</h3><p><a href="https://gitlab.com/rm_vision">陈君视觉1</a></p><p><a href="https://github.com/rm-vision-archive">陈君视觉2</a></p><h3 id="沈航">沈航</h3><ol type="1"><li><a href="https://github.com/tup-robomaster/TUP-NN-Train-2">TUP-NN-Train-2:项目结构修改，增加几种新网络结构</a></li><li><a href="https://github.com/tup-robomaster/TUP2023-Sentry-Framework">TUP2023-Sentry-Framework:哨兵框架开源，包括全向感知，决策，导航 提供VIO和LIO方案</a></li><li><a href="https://github.com/tup-robomaster/RM_Radar2023">RM_Radar2023:沈阳航空航天大学2023年雷达程序</a></li><li><a href="https://github.com/tup-robomaster/TRTInferenceForYolo">TRTInferenceForYoloX:YOLOXTensorRT推理</a></li></ol><h3 id="西浦">西浦</h3><p><a href="https://github.com/zRzRzRzRzRzRzR/YOLO-of-RoboMaster-Keypoints-Detection-2023">四点识别模型</a></p><h2 id="工具">工具</h2><p><a href="http://shenyibo.me/RM-labeling-tool/">装甲板数据集制作</a></p><p><a href="https://github.com/tup-robomaster/AutoLabel">AutoLabel:自动标注pipeline，集成角点坐标分布分析,图像去重等工具</a></p><p><a href="https://github.com/ifr-cv/rm_part_visual_tag_identify">RoboMaster视觉标签识别器</a></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/RoboMaster/">RoboMaster</category>
      
      
      <comments>https://studyinglover.com/2023/06/18/RoboMaster%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E6%B1%87%E6%80%BB(%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0)/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>没有手都可以在腾讯云创建镜像</title>
      <link>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</link>
      <guid>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</guid>
      <pubDate>Fri, 16 Jun 2023 21:15:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。&lt;/p&gt;
&lt;h2 id=&quot;创建&quot;&gt;创建&lt;/h2&gt;
&lt;p&gt;首先登陆腾讯云的账号，进入控制台界面&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;ht</description>
        
      
      
      
      <content:encoded><![CDATA[<p>腾讯云是国内顶级的云服务商。在大型项目上环境配置和编译是很多人的噩梦，当然也包括我。腾讯云为我们提供了一种新方式打包云服务器镜像。</p><h2 id="创建">创建</h2><p>首先登陆腾讯云的账号，进入控制台界面</p><figure><img src="https://proxy.thisis.plus/202306162104428.png" alt="" /><figcaption>image.png</figcaption></figure><p>选择实例的的更多选项 <img src="https://proxy.thisis.plus/202306162106168.png" alt="image.png" /></p><p>选择制作镜像 <img src="https://proxy.thisis.plus/202306162107031.png" alt="image.png" /></p><p>在弹出的窗口填入镜像名称，标签和备注即可 <img src="https://proxy.thisis.plus/202306162108164.png" alt="image.png" /></p><p>选择制作镜像后就会进入制作界面，稍等片刻我们就可以看到制作的镜像了 <img src="https://proxy.thisis.plus/202306162111218.png" alt="image.png" /></p><h2 id="应用">应用</h2><p>想要使用创建好的实例也很简单，在左侧选择镜像 <img src="https://proxy.thisis.plus/202306162113093.png" alt="image.png" /></p><p>然后点击创建镜像即可 <img src="https://proxy.thisis.plus/202306162113912.png" alt="image.png" /></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/16/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Arch下PicGo不能从剪切板上传图片</title>
      <link>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</link>
      <guid>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</guid>
      <pubDate>Tue, 13 Jun 2023 00:12:40 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://proxy.thisis.plus/202306162118377.p</description>
        
      
      
      
      <content:encoded><![CDATA[<p>作为一个天天写博客的，PicGo简直是传图片的神器。最近把电脑升级成了Archlinux,不出意外的出问题了，Arch从剪切板上传图片爆了错</p><figure><img src="https://proxy.thisis.plus/202306162118377.png" alt="" /><figcaption>image.png</figcaption></figure><p>解决方法很简单啦，进入PicGo设置，打开最下面的使用内置剪贴板上传。 <img src="https://proxy.thisis.plus/202306162122805.png" alt="image.png" /></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E8%B8%A9%E5%9D%91/">踩坑</category>
      
      
      
      <comments>https://studyinglover.com/2023/06/13/Arch%E4%B8%8BPicGo%E4%B8%8D%E8%83%BD%E4%BB%8E%E5%89%AA%E5%88%87%E6%9D%BF%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>LoRA 笔记</title>
      <link>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 13 Jun 2023 00:12:40 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;lora-笔记&quot;&gt;LoRA 笔记&lt;/h1&gt;
&lt;p&gt;自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA&lt;sup id=&quot;fnref:1&quot; class=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="lora-笔记">LoRA 笔记</h1><p>自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当我们预训练更大的模型时，重新训练所有模型参数的完整微调变得不那么可行。LoRA<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="LoRA: Low-Rank Adaptation of Large Language Models. (n.d.).">[1]</span></a></sup>冻结预训练模型权重并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数的数量减少了 10,000 倍，GPU 内存需求减少了 3 倍。</p><h2 id="什么是low-rank">什么是low-rank</h2><p>首先需要明确一些什么什么是矩阵的秩，rank</p><p>在国内的本科线性代数课程中我们是这样定义矩阵的秩的</p><blockquote><p>设在矩阵<span class="math inline">\(A\)</span> 中有一个有一个不等于<span class="math inline">\(0\)</span> 的<span class="math inline">\(r\)</span> 阶子式<span class="math inline">\(D\)</span> ,且所有<span class="math inline">\(r+1\)</span> 阶子式(如果存在的话)都等于<span class="math inline">\(0\)</span> ，那么<span class="math inline">\(D\)</span> 称为矩阵<span class="math inline">\(A\)</span> 的最高阶非零子式，数<span class="math inline">\(r\)</span> 成为矩阵的秩，记为<span class="math inline">\(R(A)\)</span> 。并规定零矩阵的秩为0。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="_同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解_. (2015).">[2]</span></a></sup></p></blockquote><p>怎么求矩阵的秩呢，很简单啦就是把一个矩阵化成RREF(课本上管这个叫行最简行矩阵)然后数一下每一行第一个非零元素所在列为单位向量的个数就可以了。</p><p>好的，发生了什么？好像并没有解释清楚秩到底是什么。</p><p>实际上啊，秩反映了矩阵里列向量线性相关的程度，意思就是你矩阵里的那几个向量能“支”出来几维，假如说我有一个矩阵里面有五个向量，但是他的矩阵秩是3,这就说明五个向量只能撑起一个3维空间，剩下两个向量可以被三个不能被互相表示的向量表示(课本上管这个叫线性相关和线性无关)，用李宏毅的话说就是这里有两个向量在"耍废"。</p><blockquote><p>推荐一下3Blue1Brown的视频https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0，线性代数讲的很清楚。</p></blockquote><p>该清楚了秩是什么，低秩是什么就很好理解了，就是有个矩阵他的秩很低，小于矩阵里面向量的个数(向量组线性相关/有向量在"耍废")。</p><p>你可能会想问，LoRA作为一个微调大语言模型和图文大模型的方法，关矩阵的秩什么事？在2020年，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568">[3]</span></a></sup> 指出大模型的训练实际发生在low-rank空间上的,所以说我们只需要构造一个低秩空间下的训练方法就可以了。</p><h2 id="为什么需要lora">为什么需要LoRA</h2><p>LoRA并不是第一个进行微调大模型的，从迁移学习开始有很多的尝试，以语言建模为例，在有效适应方面有两种突出的策略：添加适配器层或优化某种形式的输入层激活。然而，这两种策略都有其局限性，尤其是在大规模和延迟敏感的生产场景中。 ### 添加适配器层(引入推理延迟) 适配层(Adapter) 实际上就是在原本的架构上添加一些层，让他学到新的东西。例如<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning.">[4]</span></a></sup> <img src="https://proxy.thisis.plus/202306132022661.png" /> 左侧为每个 Transformer 层添加适配器模块两次：在多头注意力的投影和两个前馈层之后。右侧适配器由一个瓶颈组成，该瓶颈包含相对于原始模型中的注意力层和前馈层的参数很少。适配器还包含跳过连接。在适配器调整期间，绿色层在下游数据上进行训练，这包括适配器、层归一化参数和最终分类层（图中未显示）。</p><p>虽然可以通过修剪层或利用多任务设置来减少整体延迟，但没有直接的方法绕过适配器层中的额外计算。在单个 GPU 上对 GPT-2介质运行推理，我们看到在使用适配器时延迟显着增加，即使瓶颈维度非常小。</p><h3 id="优化某种形式的输入层激活很难进行">优化某种形式的输入层激活(很难进行)</h3><p>作者观察到前缀调整很难优化，并且它的性能在可训练参数中非单调地变化，证实了原始论文中的类似观察结果。更根本的是，保留序列长度的一部分进行适应必然会降低可用于处理下游任务的序列长度，所以作者怀疑与其他方法相比，调整提示的性能较低。</p><h2 id="lora到底怎么工作">LoRA到底怎么工作</h2><p>神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。对于预训练的权重矩阵 <span class="math inline">\(W_0 ∈ R^{d×k}\)</span>，我们通过使用低秩分解 <span class="math inline">\(W_0 + ΔW = W_0 + BA\)</span> 表示后者来约束其更新，其中 <span class="math inline">\(B ∈ R^{d×r} , A ∈ R^{r×k}\)</span>，秩<span class="math inline">\(r\)</span> 为 <span class="math inline">\(min(d, k)\)</span>。在训练期间，<span class="math inline">\(W_0\)</span> 被冻结并且不接收梯度更新，而 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 包含可训练的参数。注意 <span class="math inline">\(W_0\)</span> 和 <span class="math inline">\(ΔW = BA\)</span> 都乘以相同的输入，它们各自的输出向量按坐标求和。对于 <span class="math inline">\(h = W_0x\)</span>，我们修改后的前向传递产生：<span class="math display">\[h=W_0x+\Delta Wx=W_0x+BAx\]</span> 参数初始化时，我们对 A 使用随机高斯初始化，B 使用零，因此 ΔW = BA 在训练开始时为零。所以 <span class="math inline">\(\Delta W = BA\)</span> 在训练开始时为零.用<span class="math inline">\(\frac{\alpha}{r}\)</span> 缩放 <span class="math inline">\(ΔWx\)</span>，其中 <span class="math inline">\(\alpha\)</span> 是 <span class="math inline">\(r\)</span> 中的一个常数。在使用 Adam 进行优化时，如果我们适当地缩放初始化，调整 <span class="math inline">\(\alpha\)</span> 与调整学习率大致相同。因此，我们只需将 <span class="math inline">\(\alpha\)</span> 设置为我们尝试的第一个 r，而不对其进行调整。当我们改变时，这种缩放有助于减少重新调整超参数的需要</p><p>这种微调方式有两个好处</p><ol type="1"><li>完全泛化的微调方式</li><li>不会引入推理延迟</li></ol><p>在推理的时候，只需要把<span class="math inline">\(B\)</span>和<span class="math inline">\(A\)</span> 两个矩阵乘起来然后加回到原先的参数矩阵就完成了参数的更新</p><p><img src="https://proxy.thisis.plus/202306132038132.png" /></p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>LoRA: Low-Rank Adaptation of Large Language Models. (n.d.). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><em>同济大学数学系工程数学-线性代数(第6版)笔记和课后习题(含考研真题)详解</em>. (2015). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Presented at the Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. https://doi.org/10.18653/v1/2021.acl-long.568 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., Laroussilhe, Q., Gesmundo, A., … Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. International Conference on Machine Learning. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/13/LoRA%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Diffusers去除NSFW限制</title>
      <link>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/</link>
      <guid>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/</guid>
      <pubDate>Sun, 11 Jun 2023 00:02:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;众所周知，&lt;del&gt;涩涩是图像生成技术发展的重大推动力&lt;/del&gt; . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(&lt;strong&gt;not safe for work&lt;/stro</description>
        
      
      
      
      <content:encoded><![CDATA[<p>众所周知，<del>涩涩是图像生成技术发展的重大推动力</del> . Huggingface的diffusers封装了大量的算法用于生成图片。但是，很不幸的，diffusers会检测生成的图片是否存在NSFW(<strong>not safe for work</strong>)的内容，<del>这就给我们涩涩带来了不必要的麻烦</del>。所以我将介绍如何去除限制</p><p>该方法来自网友，<a href="https://www.reddit.com/r/StableDiffusion/comments/wxba44/disable_hugging_face_nsfw_filter_in_three_step/">原链接</a></p><p>先给一段示例代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure><p>我们只需要设置<code>StableDiffusionPipeline</code> 这个类的<code>safety_checker</code>函数，更改之后的代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dummy</span>(<span class="hljs-params">images, **kwargs</span>): <br><span class="hljs-keyword">return</span> images, <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)<br>pipe.safety_checker = dummy<br>new_image = pipe(prompt, num_inference_steps=<span class="hljs-number">20</span>).images[<span class="hljs-number">0</span>]<br>plt.save(<span class="hljs-string">&#x27;image.png&#x27;</span>,new_image)<br></code></pre></td></tr></table></figure></p><p>成功实现<del>涩涩自由</del></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/06/11/Diffusers%E5%8E%BB%E9%99%A4NSFW%E9%99%90%E5%88%B6/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>StableDiffusion笔记</title>
      <link>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Mon, 29 May 2023 15:36:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;Stable Diffusion 是一个图像生成方法，由 &lt;em&gt;&lt;a href=&quot;https://stability.ai/&quot;&gt;Stability AI&lt;/a&gt; and &lt;a href=&quot;https://runwayml.com/&quot;&gt;Runway&lt;/a&gt;&lt;/em&gt; 在LD</description>
        
      
      
      
      <content:encoded><![CDATA[<p>Stable Diffusion 是一个图像生成方法，由 <em><a href="https://stability.ai/">Stability AI</a> and <a href="https://runwayml.com/">Runway</a></em> 在LDM<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[1]</span></a></sup> 的基础上提出。在GitHub有很多他的实现和应用<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="AUTOMATIC1111. (n.d.). _GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI_. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui">[4]</span></a></sup> ,其中<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="CompVis. (n.d.). _GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model_. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion">[2]</span></a></sup> 是最早的实现版本，<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Stability-AI. (n.d.). _GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models_. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion">[3]</span></a></sup> 是V2版本，由 Stability AI 完成。</p><h2 id="整体结构">整体结构</h2><pre><code class=" mermaid">flowchart TDsubgraph Input-noisyRandom-seed --&gt; latent-Gaussian-noise endsubgraph Input-promptprompt --&gt; TextEncoder --&gt; TextEmbaddingsendlatent-Gaussian-noise --&gt;Unet&#123;Unet-with-MultiAttention&#125;TextEmbaddings--&gt;UnetUnet --&gt; predict-noisy --sampling-steps--&gt;Unetpredict-noisy --&gt; Decoder --&gt; Image </code></pre><p>在一开始，StableDiffusion会通过一个随机数种子生成一张在隐空间下的随机噪声，同时通过一个文本编码器对输入的prompt进行编码，生成一个文本向量。随机噪声和文本向量会一块送入Unet，经过DDPM的步骤得到一张隐空间下的图片，通过一个解码器得到完整的图片。这里的Unet做出了改进，中间加入了交叉注意力机制。</p><h3 id="unet-with-multiattention">Unet-with-MultiAttention</h3><p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IRTbG2rYv0IUH8HHAxWRrQ.png" alt="Unet-with-MultiAttention 图源medium.com" /> 图中Switch用于在不同的输入之间调整。</p><ul><li>文本数据通过一个文本编码器(一般是CLIP的文本编码器)将文本转换为向量，投影到Unet上</li><li>图像，语义图，表示等直接送入Unet</li></ul><p>反向扩散过程中输入的文本向量和隐空间下的噪声图片需要经过 <span class="math inline">\(t\)</span>轮的Unet网络，每一轮预测一个噪声，噪声图减去这个噪声，得到的图片继续送入Unet进行下一轮</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>CompVis. (n.d.). <em>GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model</em>. GitHub. Retrieved May 29, 2023, from https://github.com/CompVis/stable-diffusion <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Stability-AI. (n.d.). <em>GitHub - Stability-AI/stablediffusion: High-Resolution image synthesis with latent diffusion models</em>. GitHub. Retrieved May 29, 2023, from https://github.com/Stability-AI/stablediffusion <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>AUTOMATIC1111. (n.d.). <em>GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI</em>. GitHub. Retrieved May 29, 2023, from https://github.com/automatic1111/stable-diffusion-webui <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Steins. (2023, January 2). Stable diffusion clearly explained! - Steins. <em>Medium</em>. https://medium.com/<span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e"><span class="citation" data-cites="steinsfu/stable-diffusion-clearly-explained-ed008044e07e">@steinsfu/stable-diffusion-clearly-explained-ed008044e07e</span></span> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/">笔记</category>
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/29/StableDiffusion%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>huggingface 和相关库</title>
      <link>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/</link>
      <guid>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/</guid>
      <pubDate>Tue, 09 May 2023 12:35:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;huggingface-和相关库&quot;&gt;huggingface 和相关库&lt;/h1&gt;
&lt;h2 id=&quot;huggingface&quot;&gt;huggingface&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;是</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="huggingface-和相关库">huggingface 和相关库</h1><h2 id="huggingface">huggingface</h2><p><a href="https://huggingface.co/">Hugging Face</a>是一个专注于自然语言处理（NLP）的开源平台，它旨在让NLP变得更加易用和普及。Hugging Face推出了多个库，例如Transformers，Datasets，Tokenizers和Accelerate，它们分别提供了预训练的模型，大规模的数据集，高效的分词器和分布式训练的工具。Hugging Face还拥有一个活跃的社区，其中有数千名研究人员，开发者和爱好者共同交流和贡献NLP的最新进展。 <img src="https://proxy.thisis.plus/202305092233241.png" alt="image.png" /></p><p>Hugging Face的名字来源于一个可爱的表情符号，它代表了平台的愿景：让人类和机器之间的交流更加自然和亲密。Hugging Face的核心产品是Transformers库，它包含了超过10000个预训练的模型，涵盖了各种NLP任务，如文本分类，问答，文本生成，情感分析等。Transformers库支持多种深度学习框架，如PyTorch，TensorFlow，JAX和Flax，并且可以轻松地在不同的设备上运行，如CPU，GPU和TPU。Hugging Face还提供了一个在线平台，Spaces，它可以让用户快速地部署和分享他们的模型和应用。 <img src="https://proxy.thisis.plus/202305092235498.png" alt="image.png" /></p><p>近年来，Hugging Face托管的模型已经不局限于NLP领域，而是涉及到了更多的领域，如计算机视觉（CV），语音识别（ASR），音乐生成（MG）等。这些模型都可以在Hugging Face的网站上找到，并且可以通过Transformers库或者其他的库来使用。Hugging Face还提供了一个数据集库，叫做Datasets，它包含了超过1000个数据集，覆盖了各种领域和语言。Datasets库可以帮助用户快速地加载，处理和缓存数据，以及进行数据分析和可视化。</p><h2 id="accelerate">Accelerate</h2><p>Accelerate 是一个可以让训练变得更加简单的库，它可以通过几行代码来在分布式设备上运行相同的pytorch代码</p><p>可以通过pypi 和 conda安装 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install accelerate<br>conda install -c conda-forge accelerate<br></code></pre></td></tr></table></figure></p><p>你可能会遇到这种报错 <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  <br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.  <br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure></p><p>这里的依赖已经安装成功了，只是被安装到了未被添加到PATH的目录，接下来运行的时候只需要指明目录即可。例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p><p>通过<code>accelerate config</code> 命令可以配置当前文件夹启用。(如果啥都不知道就全部选No)</p><h2 id="transformers">Transformers</h2><p><a href="https://huggingface.co/docs/transformers/index">Transformers</a> 收集了所有的SOTA的NLP研究方法，并提供了对应的预训练模型和接口。Transformers 支持 PyTorch、TensorFlow 和 JAX 之间的框架互操作性。这提供了在模型生命周期的每个阶段使用不同框架的灵活性；在一个框架中用三行代码训练一个模型，然后将其加载到另一个框架中进行推理。模型还可以导出为 ONNX 和 TorchScript 等格式，以便在生产环境中部署。</p><p>安装非常简单 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers[torch]&#x27;</span><br></code></pre></td></tr></table></figure> 这回安装torch对应的api，当然也可以安装完整版 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;transformers&#x27;</span> <br></code></pre></td></tr></table></figure></p><p>21年冬天在家上网课的时候，我看到了这样的一个教程<a href="https://zhuanlan.zhihu.com/p/421642560">这篇文章是我用AI生成出来的</a> ,他就是使用了transformers 库构建了一个生成式语言模型。</p><h2 id="diffusers">Diffusers</h2><p><a href="https://huggingface.co/docs/diffusers/index">Diffusers</a> 收集了所有SOTA的扩散模型，用于生成图像、音频，甚至分子的 3D 结构。diffusers提供了扩散模型的完整pipeline ，包括DDPM，DDIM，stable_diffusion_2，VAE，controlnet等等，可以使用简单的几行代码完成推理。</p><p>安装和上面一样 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers[<span class="hljs-string">&quot;torch&quot;</span>]<br></code></pre></td></tr></table></figure> 或者 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install diffusers <br></code></pre></td></tr></table></figure></p><h3 id="pipeline">pipeline</h3><p>pipeline 是diffusers 甚至huggingface各个库的一个重要概念，他封装了各个模型加载权重，构建网络结构，推理和训练的全部过程。</p><p>这里以stable diffusion 1.5为例，首先创建pipeline，并指明stable diffusion 的版本 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DiffusionPipeline<br><br>model_id = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br>pipeline = DiffusionPipeline.from_pretrained(model_id)<br></code></pre></td></tr></table></figure></p><p>接下来给出提示(prompt) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;portrait photo of a old warrior chief&quot;</span><br></code></pre></td></tr></table></figure></p><p>为了加速推理，我们可以把数据放到gpu上 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>设置生成器，并生成图像 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><p>当然，huggingface推荐我们在float16上做推理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)<br>pipeline = pipeline.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br>generator = torch.Generator(<span class="hljs-string">&quot;cuda&quot;</span>).manual_seed(<span class="hljs-number">0</span>)<br>image = pipeline(prompt, generator=generator).images[<span class="hljs-number">0</span>]<br>image<br></code></pre></td></tr></table></figure></p><h3 id="lora">LoRA</h3><p><a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation of Large Language Models (LoRA)</a>是一种训练方法，可以加速大型模型的训练，同时消耗更少的内存，最有用的例子莫过于生成人脸了。Diffusers 现在支持使用 LoRA 进行<a href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-lora">文本到图像生成</a>和<a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora">DreamBooth</a>微调。</p><p><a href="https://arxiv.org/abs/2208.12242">DreamBooth</a>是Google提出的微调技术，用于个性化文本到图像模型（如 Stable Diffusion），可以以在给定几张主题图像的情况下生成不同背景下主题的逼真图像。</p><p>在<a href="https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py">这里</a> 你可以找到完整的代码，在<a href="https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ">Google Drive</a> 下载完整的图像用于训练</p><p>先设置基本信息，分别是模型名，示例图片和模型输出文件夹 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MODEL_NAME=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br><span class="hljs-built_in">export</span> INSTANCE_DIR=<span class="hljs-string">&quot;path-to-instance-images&quot;</span><br><span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;path-to-save-model&quot;</span><br></code></pre></td></tr></table></figure></p><p>接下来运行代码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate launch train_dreambooth_lora.py \<br>  --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_NAME</span>  \<br>  --instance_data_dir=<span class="hljs-variable">$INSTANCE_DIR</span> \<br>  --output_dir=<span class="hljs-variable">$OUTPUT_DIR</span> \<br>  --instance_prompt=<span class="hljs-string">&quot;a photo of sks dog&quot;</span> \<br>  --resolution=512 \<br>  --train_batch_size=1 \<br>  --gradient_accumulation_steps=1 \<br>  --checkpointing_steps=100 \<br>  --learning_rate=1e-4 \<br>  --report_to=<span class="hljs-string">&quot;wandb&quot;</span> \<br>  --lr_scheduler=<span class="hljs-string">&quot;constant&quot;</span> \<br>  --lr_warmup_steps=0 \<br>  --max_train_steps=500 \<br>  --validation_prompt=<span class="hljs-string">&quot;A photo of sks dog in a bucket&quot;</span> \<br>  --validation_epochs=50 \<br>  --seed=<span class="hljs-string">&quot;0&quot;</span> \<br>  --push_to_hub<br></code></pre></td></tr></table></figure></p><p>推理也是使用起来很简单的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline<br><br>pipe.unet.load_attn_procs(lora_model_path)<br>pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>image = pipe(<span class="hljs-string">&quot;A picture of a sks dog in a bucket.&quot;</span>, num_inference_steps=<span class="hljs-number">25</span>, guidance_scale=<span class="hljs-number">7.5</span>).images[<span class="hljs-number">0</span>]<br>image.save(<span class="hljs-string">&quot;bucket-dog.png&quot;</span>)<br></code></pre></td></tr></table></figure></p><h3 id="controlnet">[[ControlNet]]</h3><p>可以看之前的文章<a href="https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/">ControlNet训练自己数据集</a></p><h2 id="gradio">Gradio</h2><p>gradio 是一个可以快速构建交互式网页的工具，Webui就是用它做出来的，使用他的核心代码就是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">demo = gradio.Interface(fn, inputs, outputs, ···)<br>demo.launch()<br></code></pre></td></tr></table></figure> 传入一个函数和参数，获取返回值</p><p>剩下的就是你写好fn，设计一个好看的界面，然后launch就可以了。</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/09/huggingface%E5%92%8C%E7%9B%B8%E5%85%B3%E5%BA%93/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Multidiffusion代码分析</title>
      <link>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <guid>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <pubDate>Tue, 09 May 2023 12:35:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;multidiffusion代码分析&quot;&gt;Multidiffusion代码分析&lt;/h1&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;当我们使用计算机生成图像时，经常会遇到一些困难，例如如何生成高质量、高分辨率的图像，如何控制图像的风格和内容等。近年来，深度学习技</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="multidiffusion代码分析">Multidiffusion代码分析</h1><h2 id="前言">前言</h2><p>当我们使用计算机生成图像时，经常会遇到一些困难，例如如何生成高质量、高分辨率的图像，如何控制图像的风格和内容等。近年来，深度学习技术在图像生成领域取得了很大的进展，其中一种流行的方法是使用变分自编码器（VAE）和生成对抗网络（GAN）等模型。然而，这些方法通常需要大量的训练数据和计算资源，而且生成的图像可能会出现一些问题，例如模糊、失真和不连续等。</p><p>为了解决这些问题，一些研究人员提出了一种新的合成全景图的方法，称为MultiDiffusion。该方法使用了一种多步推理的策略，将全景图像的生成过程分解成多个步骤，并在每个步骤中对潜变量向量进行微调，从而生成高质量、高分辨率的全景图像。MultiDiffusion方法不需要大量的训练数据和计算资源，而且能够生成具有良好视觉效果的全景图像。本文将介绍MultiDiffusion方法的实现细节，并提供相应的代码和解释。(chatgpt写的，大家凑活着看)</p><p><a href="https://multidiffusion.github.io/">官方主页</a> <a href="https://github.com/omerbt/MultiDiffusion">代码</a> <a href="https://huggingface.co/spaces/weizmannscience/MultiDiffusion">在线体验</a></p><h2 id="分析">分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTextModel, CLIPTokenizer, logging<br><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoencoderKL, UNet2DConditionModel, DDIMScheduler<br><span class="hljs-comment"># suppress partial model loading warning</span><br>logging.set_verbosity_error()<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><span class="hljs-keyword">import</span> argparse<br></code></pre></td></tr></table></figure><p>这里导入了所有的库，包括huggingface推出的transformers 和 diffusers。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seed_everything</span>(<span class="hljs-params">seed</span>):<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed(seed)<br>    <span class="hljs-comment"># torch.backends.cudnn.deterministic = True</span><br>    <span class="hljs-comment"># torch.backends.cudnn.benchmark = True</span><br></code></pre></td></tr></table></figure><p>常规操作，设置随机数，实际上还有另一种写法<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="关注 R. 却没能成为自己​. (n.d.). _pytorch如何确保 可重复性/每次训练结果相同(固定了随机种子，为什么还不行)？_. 知乎. Retrieved May 9, 2023, from http://zhihu.com/question/345043149/answer/2940838756">[1]</span></a></sup> . 这里是设置了torch 在CPU 和 GPU 的随机数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seed_torch</span>(<span class="hljs-params">seed=<span class="hljs-number">1029</span></span>):<br>    random.seed(seed)   <span class="hljs-comment"># Python的随机性</span><br>    os.environ[<span class="hljs-string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="hljs-built_in">str</span>(seed)    <span class="hljs-comment"># 设置Python哈希种子，为了禁止hash随机化，使得实验可复现</span><br>    np.random.seed(seed)   <span class="hljs-comment"># numpy的随机性</span><br>    torch.manual_seed(seed)   <span class="hljs-comment"># torch的CPU随机性，为CPU设置随机种子</span><br>    torch.cuda.manual_seed(seed)   <span class="hljs-comment"># torch的GPU随机性，为当前GPU设置随机种子</span><br>    torch.cuda.manual_seed_all(seed)  <span class="hljs-comment"># if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span>   <span class="hljs-comment"># if benchmark=True, deterministic will be False</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span>   <span class="hljs-comment"># 选择确定性算法</span><br></code></pre></td></tr></table></figure> 事实上，涉及到一些类似upsample 的层，因为原子加操作带来的浮点误差，永远也对不齐。 <code>a + b） + c != a + (b + c)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_views</span>(<span class="hljs-params">panorama_height, panorama_width, window_size=<span class="hljs-number">64</span>, stride=<span class="hljs-number">8</span></span>):<br>    panorama_height /= <span class="hljs-number">8</span><br>    panorama_width /= <span class="hljs-number">8</span><br>    num_blocks_height = (panorama_height - window_size) // stride + <span class="hljs-number">1</span><br>    num_blocks_width = (panorama_width - window_size) // stride + <span class="hljs-number">1</span><br>    total_num_blocks = <span class="hljs-built_in">int</span>(num_blocks_height * num_blocks_width)<br>    views = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_num_blocks):<br>        h_start = <span class="hljs-built_in">int</span>((i // num_blocks_width) * stride)<br>        h_end = h_start + window_size<br>        w_start = <span class="hljs-built_in">int</span>((i % num_blocks_width) * stride)<br>        w_end = w_start + window_size<br>        views.append((h_start, h_end, w_start, w_end))<br>    <span class="hljs-keyword">return</span> views<br></code></pre></td></tr></table></figure><p>这段代码的作用是将一个全景图像分成多个小块，每个块的大小为<span class="math inline">\(window_{size} * window_{size}\)</span>，步长为<span class="math inline">\(stride\)</span>，返回每个小块的位置信息。</p><p>下面类定义了整个multidiffusion的所有操作 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self.device = device<br>self.sd_version = sd_version<br></code></pre></td></tr></table></figure> 定义了设备(CPU/GPU)和stable diffusion的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] loading stable diffusion...&#x27;</span>)<br><br><span class="hljs-keyword">if</span> hf_key <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] using hugging face custom model key: <span class="hljs-subst">&#123;hf_key&#125;</span>&#x27;</span>)<br>model_key = hf_key<br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;2.1&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;stabilityai/stable-diffusion-2-1-base&quot;</span><br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;2.0&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;stabilityai/stable-diffusion-2-base&quot;</span><br><span class="hljs-keyword">elif</span> self.sd_version == <span class="hljs-string">&#x27;1.5&#x27;</span>:<br>model_key = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br><span class="hljs-keyword">else</span>:<br><span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&#x27;Stable-diffusion version <span class="hljs-subst">&#123;self.sd_version&#125;</span> not supported.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>加载了stable diffusion的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create model</span><br>self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;vae&quot;</span>).to(self.device)<br>self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;tokenizer&quot;</span>)<br>self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;text_encoder&quot;</span>).to(self.device)<br>self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;unet&quot;</span>).to(self.device)<br>self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=<span class="hljs-string">&quot;scheduler&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[INFO] loaded stable diffusion!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这里是从预训练模型加载并创建模型，分别加载了VAE，tokenizer，text_encoder</p><table><thead><tr class="header"><th>模型</th><th>内容</th></tr></thead><tbody><tr class="odd"><td>VAE</td><td>变分自动编码器</td></tr><tr class="even"><td>tokenizer</td><td>分词器,负责将一句话分割成一个一个词，这里是CLIPTokenizer</td></tr><tr class="odd"><td>text_encoder</td><td>文本编码器</td></tr><tr class="even"><td>UNet2DConditionModel</td><td>Unet，负责重建和预测</td></tr><tr class="odd"><td>DDIMScheduler</td><td>DDIM采样器</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_embeds</span>(<span class="hljs-params">self, prompt, negative_prompt</span>):<br><span class="hljs-comment"># prompt, negative_prompt: [str]</span><br><span class="hljs-comment"># Tokenize text and get embeddings</span><br>text_input = self.tokenizer(prompt, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=self.tokenizer.model_max_length,<br>truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># Do the same for unconditional embeddings</span><br>uncond_input = self.tokenizer(negative_prompt, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=self.tokenizer.model_max_length, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># Cat for final embeddings</span><br>text_embeddings = torch.cat([uncond_embeddings, text_embeddings])<br><span class="hljs-keyword">return</span> text_embeddings<br></code></pre></td></tr></table></figure><p>这里是将提示(prompt) 转换成了text_embeddings、</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_latents</span>(<span class="hljs-params">self, latents</span>):<br>        latents = <span class="hljs-number">1</span> / <span class="hljs-number">0.18215</span> * latents<br>        imgs = self.vae.decode(latents).sample<br>        imgs = (imgs / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>).clamp(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> imgs<br></code></pre></td></tr></table></figure><p>这段代码作用是将一个向量从latent space 解码成一个图像。</p><p>它接收一个潜变量向量集合作为输入，并使用变分自编码器（VAE）将其解码成图像。他将输入的潜变量向量集合除以0.18215进行缩放(魔数，不知原因)，然后调用VAE的decode方法来生成一组图像同时使用sample方法产生一些随机性，从而增加输出图像的多样性。最后缩放到<span class="math inline">\([0,1]\)</span> 范围内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">text2panorama</span>(<span class="hljs-params">self, prompts, negative_prompts=<span class="hljs-string">&#x27;&#x27;</span>, height=<span class="hljs-number">512</span>, width=<span class="hljs-number">2048</span>, num_inference_steps=<span class="hljs-number">50</span>, guidance_scale=<span class="hljs-number">7.5</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(prompts, <span class="hljs-built_in">str</span>):<br>            prompts = [prompts]<br>  <br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(negative_prompts, <span class="hljs-built_in">str</span>):<br>            negative_prompts = [negative_prompts]<br>  <br>        <span class="hljs-comment"># Prompts -&gt; text embeds</span><br>        text_embeds = self.get_text_embeds(prompts, negative_prompts)  <span class="hljs-comment"># [2, 77, 768]</span><br>  <br>        <span class="hljs-comment"># Define panorama grid and get views</span><br>        latent = torch.randn((<span class="hljs-number">1</span>, self.unet.in_channels, height // <span class="hljs-number">8</span>, width // <span class="hljs-number">8</span>), device=self.device)<br>        views = get_views(height, width)<br>        count = torch.zeros_like(latent)<br>        value = torch.zeros_like(latent)<br>  <br>        self.scheduler.set_timesteps(num_inference_steps)<br>  <br>        <span class="hljs-keyword">with</span> torch.autocast(<span class="hljs-string">&#x27;cuda&#x27;</span>):<br>            <span class="hljs-keyword">for</span> i, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.scheduler.timesteps):<br>                count.zero_()<br>                value.zero_()<br>  <br>                <span class="hljs-keyword">for</span> h_start, h_end, w_start, w_end <span class="hljs-keyword">in</span> views:<br>                    <span class="hljs-comment"># TODO we can support batches, and pass multiple views at once to the unet</span><br>                    latent_view = latent[:, :, h_start:h_end, w_start:w_end]<br>  <br>                    <span class="hljs-comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span><br>                    latent_model_input = torch.cat([latent_view] * <span class="hljs-number">2</span>)<br>  <br>                    <span class="hljs-comment"># predict the noise residual</span><br>                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)[<span class="hljs-string">&#x27;sample&#x27;</span>]<br>  <br>                    <span class="hljs-comment"># perform guidance</span><br>                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(<span class="hljs-number">2</span>)<br>                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)<br>  <br>                    <span class="hljs-comment"># compute the denoising step with the reference model</span><br>                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)[<span class="hljs-string">&#x27;prev_sample&#x27;</span>]<br>                    value[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised<br>                    count[:, :, h_start:h_end, w_start:w_end] += <span class="hljs-number">1</span><br>  <br>                <span class="hljs-comment"># take the MultiDiffusion step</span><br>                latent = torch.where(count &gt; <span class="hljs-number">0</span>, value / count, value)<br>  <br>        <span class="hljs-comment"># Img latents -&gt; imgs</span><br>        imgs = self.decode_latents(latent)  <span class="hljs-comment"># [1, 3, 512, 512]</span><br>        img = T.ToPILImage()(imgs[<span class="hljs-number">0</span>].cpu())<br>        <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure><p>作用是根据给定的文本提示(prompts)，将其合成成全景图像。它接收一组提示(prompt)作为输入，将其转换为列表类型。然后，定义全景图像的网格，并获取一个一个图像。接下来，使用随机噪声向量作为输入，通过多步推理生成全景图像的潜变量向量。在推理过程中，使用UNet模型对潜变量向量进行多步推理，并根据提示进行引导，生成不同的全景图像，最后横向拼接所有图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;a photo of the dolomites&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--negative&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--sd_version&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;2.0&#x27;</span>, choices=[<span class="hljs-string">&#x27;1.5&#x27;</span>, <span class="hljs-string">&#x27;2.0&#x27;</span>],                       <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;stable diffusion version&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--H&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">512</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--W&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">4096</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">0</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--steps&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">50</span>)<br>    opt = parser.parse_args()<br>    seed_everything(opt.seed)<br>  <br>    device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>  <br>    sd = MultiDiffusion(device, opt.sd_version)<br><br>    img = sd.text2panorama(opt.prompt, opt.negative, opt.H, opt.W, opt.steps)<br>  <br>    <span class="hljs-comment"># save image</span><br>    img.save(<span class="hljs-string">&#x27;out.png&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这个是从命令行启动的方式，按照argparse的使用方法使用</p><table><thead><tr class="header"><th>参数</th><th>含义</th></tr></thead><tbody><tr class="odd"><td>prompt</td><td>提示</td></tr><tr class="even"><td>negative</td><td>反面提示</td></tr><tr class="odd"><td>sd_version</td><td>stable diffusion的版本</td></tr><tr class="even"><td>H</td><td>图像的高度</td></tr><tr class="odd"><td>W</td><td>图像的宽度</td></tr><tr class="even"><td>seed</td><td>随机数种子</td></tr><tr class="odd"><td>steps</td><td>采样步数</td></tr></tbody></table><p>最后的结果会保存为out.png</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>关注 R. 却没能成为自己​. (n.d.). <em>pytorch如何确保 可重复性/每次训练结果相同(固定了随机种子，为什么还不行)？</em>. 知乎. Retrieved May 9, 2023, from http://zhihu.com/question/345043149/answer/2940838756 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/05/09/multidiffusion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>MXnet-arcface数据集准备</title>
      <link>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/</link>
      <guid>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/</guid>
      <pubDate>Mon, 08 May 2023 21:28:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;众所周知，mxnet是一个沐神主导开发的一个深度学习框架，之前听李沐的讲论文时也听他说过很多次，但是已知没有机会使用，最近接触了一个项目，有机会感受了一些mxnet，但是也踩了很多坑。所有需要的脚本文件可以在&lt;a href=&quot;https://github.com/Study</description>
        
      
      
      
      <content:encoded><![CDATA[<p>众所周知，mxnet是一个沐神主导开发的一个深度学习框架，之前听李沐的讲论文时也听他说过很多次，但是已知没有机会使用，最近接触了一个项目，有机会感受了一些mxnet，但是也踩了很多坑。所有需要的脚本文件可以在<a href="https://github.com/StudyingLover/menet-Arcface-tools">https://github.com/StudyingLover/menet-Arcface-tools</a>下载</p><figure><img src="https://proxy.thisis.plus/202305082129080.png" alt="" /><figcaption>image.png</figcaption></figure><p>mxnet 的数据与别处的是不同的，他的训练集是两个文件，分别以<code>.idx</code> 和 <code>.rec</code> 结尾， 测试集是以<code>.bin</code> 结尾的一个二进制文件。</p><h3 id="创建lstidxrec">创建lst,idx,rec</h3><p>我们需要按照特定方式放置图片,首先创建一个大的文件夹，里面创建一个个子文件夹，每个文件夹放置相同类别的图片 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs stylus">/image_folder<br>├── <span class="hljs-number">0</span>_0_0000000<br>│   ├── <span class="hljs-number">0</span>_0<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_1<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_2<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_3<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_4<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000001<br>│   ├── <span class="hljs-number">0</span>_5<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_6<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_7<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_8<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_9<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000002<br>│   ├── <span class="hljs-number">0</span>_10<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_11<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_12<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_13<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_14<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_15<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_16<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_17<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000003<br>│   ├── <span class="hljs-number">0</span>_18<span class="hljs-selector-class">.jpg</span><br>│   ├── <span class="hljs-number">0</span>_19<span class="hljs-selector-class">.jpg</span><br>│   └── <span class="hljs-number">0</span>_20<span class="hljs-selector-class">.jpg</span><br>├── <span class="hljs-number">0</span>_0_0000004<br><br></code></pre></td></tr></table></figure></p><p>接下来先生成一个<code>.lst</code> 文件，这个文件包含了所有的文件,训练集和测试集按照8：2划分 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m mxnet.tools.im2rec --list --recursive train 图片文件夹 –test-ratio 0.8<br></code></pre></td></tr></table></figure></p><p>这段代码会生成两个文件夹<code>train_train.lst</code> 和<code>train_test.lst</code></p><h3 id="生成训练集文件">生成训练集文件</h3><p>接下来生成训练集文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m mxnet.tools.im2rec train_train.lst --quality 100 图片文件夹<br></code></pre></td></tr></table></figure></p><p>需要给生成的文件改个名字 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> train_train.idx train.idx<br><span class="hljs-built_in">mv</span> train_train.rec train.rec<br></code></pre></td></tr></table></figure></p><p>下面创建property配置文件 <figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">训练集图片数量 图片大小 图片大小<br></code></pre></td></tr></table></figure></p><p>例如 <figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">10000 </span><span class="hljs-number">112</span> <span class="hljs-number">112</span><br></code></pre></td></tr></table></figure></p><h3 id="创建pair文件">创建pair文件</h3><p>这一步多少有点奇怪，pair文件里面的结构是 <figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gcode">im<span class="hljs-name">g1</span>_path im<span class="hljs-name">g2</span>_path <span class="hljs-number">0</span><br>im<span class="hljs-name">g3</span>_path im<span class="hljs-name">g4</span>_path <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure> 生成方式也很简单啦，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3 generate_image_pairs.py --data-dir 图片文件夹路径 --outputtxt train.txt --num-samepairs 3000<br></code></pre></td></tr></table></figure> <code>num-samepairs</code> 是个魔数，看心情写吧，这里我为了大量生成，我又写了个脚本，重复执行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">python repeat_cmd.py<br>python detele_empty.py<br><span class="hljs-built_in">cp</span> train.txt 图片文件夹<br></code></pre></td></tr></table></figure></p><h3 id="生成验证集bin">生成验证集bin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python lfw2pack.py --data-dir 图片文件夹 --output test.bin --num-samepairs 300<br></code></pre></td></tr></table></figure><p>ok就这样，我们生成了需要的<code>train.idx</code> <code>train.rec</code>,<code>test.bin</code></p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E5%B7%A5%E5%85%B7/">工具</category>
      
      
      
      <comments>https://studyinglover.com/2023/05/08/MXnet-arcface%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ControlNet训练自己数据集</title>
      <link>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/</link>
      <guid>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/</guid>
      <pubDate>Thu, 27 Apr 2023 19:36:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;controlnet训练自己数据集&quot;&gt;ControlNet训练自己数据集&lt;/h1&gt;
&lt;h2 id=&quot;从官方仓库训练&quot;&gt;从官方仓库训练&lt;/h2&gt;
&lt;p&gt;官方教程 https://github.com/lllyasviel/ControlNet/blob/main/d</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="controlnet训练自己数据集">ControlNet训练自己数据集</h1><h2 id="从官方仓库训练">从官方仓库训练</h2><p>官方教程 https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md</p><h3 id="环境配置">环境配置</h3><p>先看一下有没有显卡 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvidia-smi<br></code></pre></td></tr></table></figure></p><p>首先下载整个仓库 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/lllyasviel/ControlNet.git<br></code></pre></td></tr></table></figure></p><p>然后创建conda虚拟环境(选做，只要你能配好环境) <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> create -f environment.yaml<br>conda activate control<br></code></pre></td></tr></table></figure></p><p>接下来需要下载stable diffusion和训练集，因为我们是对stable diffusion 模型做微调。</p><p>下载sd1.5到，models目录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ./models<br>wget https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt<br></code></pre></td></tr></table></figure></p><p>下载训练数据集到training文件夹 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> training <br><span class="hljs-built_in">cd</span> ./training<br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/training/fill50k.zip<br></code></pre></td></tr></table></figure> 解压数据集 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">unzip fill50k.zip<br></code></pre></td></tr></table></figure></p><p>当然这个数据集非常大，我们也可以选择小一点的 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png<br><br>wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png<br></code></pre></td></tr></table></figure> 然后将conditioning_image_1.png改名0.png放到./source目录下,conditioning_image_2.png改名放到./target目录下 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mv</span> conditioning_image_1.png 0.png <br><span class="hljs-built_in">mv</span> 0.png ./source<br><br><span class="hljs-built_in">mv</span> conditioning_image_2.png 0.png <br><span class="hljs-built_in">mv</span> 0.png ./target<br></code></pre></td></tr></table></figure></p><p>然后创建一个<code>prompt.json</code> 的文件写入 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;source&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;source/0.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;target&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;target/0.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;pale golden rod circle with old lace background&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure></p><p>无论是哪种方式，最后的文件结构是这样的<img src="https://proxy.thisis.plus/20230427191856.png" alt="image.png" /></p><h3 id="训练">训练</h3><p>首先调一下<code>tutorial_train.py</code> 里的batch_size，训练过程中如果出现out of memory 的情况可以调小。</p><p>接下来运行tutorial_train.py，闭上眼睛等待训练完成即可 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python tutorial_train.py<br></code></pre></td></tr></table></figure> 如果是完整数据集，大概6个小时一个epoch，如果是单张图片会很快。</p><p>当然，为了不要出现网不好ssh断掉导致训练终端，我们可以使用screne <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">screen -S train <br>conda activate control <br>python tutorial_train.py<br></code></pre></td></tr></table></figure> 训练出的结果可以在<code>image_log</code> 中看到</p><figure><img src="https://proxy.thisis.plus/20230427191937.png" alt="" /><figcaption>image.png</figcaption></figure><h3 id="踩坑解决">踩坑解决</h3><h4 id="out-of-memoryoom">out of memory(oom)</h4><p>首先开启<code>save_memory</code>模式，将<code>config.py</code> 中False改为True</p><p>同时调低batch_size</p><h4 id="no-operator-found-for-memory_efficient_attention_backward">No operator found for <code>memory_efficient_attention_backward</code></h4><p>卸载 xformers <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall  xformers<br></code></pre></td></tr></table></figure></p><h4 id="typeerror-on_train_batch_start-missing-1-required-positional-argument-dataloader_idx">TypeError: on_train_batch_start() missing 1 required positional argument: 'dataloader_idx'</h4><p>这个比较坑，是论文代码有问题，改一下源码就好 1. ControlNet/ldm/models/diffusion/ddpm.py文件591行 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_start</span>(<span class="hljs-params">self, batch, batch_idx, dataloader_idx</span>):<br></code></pre></td></tr></table></figure> 删除dataloader_idx,改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_start</span>(<span class="hljs-params">self, batch, batch_idx</span>):<br></code></pre></td></tr></table></figure></p><ol start="2" type="1"><li>ControlNet/cldm/logger.py文件74行 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_end</span>(<span class="hljs-params">self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx</span>):<br></code></pre></td></tr></table></figure> 删除dataloader_idx，改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">on_train_batch_end</span>(<span class="hljs-params">self, trainer, pl_module, outputs, batch, batch_idx</span>):<br></code></pre></td></tr></table></figure></li></ol><h2 id="diffusers-训练">Diffusers 训练</h2><p><a href="https://github.com/huggingface/diffusers">Diffusers</a> 是一个huggingface 推出的扩散模型的封装库,同时也对ControlNet做了封装，https://github.com/huggingface/diffusers/tree/main/examples/controlnet</p><h3 id="训练-1">训练</h3><p>代码跑起来其实也非常简单，首先下载diffusers整个仓库,然后安装依赖 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/huggingface/diffusers<br><span class="hljs-built_in">cd</span> diffusers<br>pip install -r requirements.txt<br></code></pre></td></tr></table></figure> 你可能会发现这样的报错 <img src="https://proxy.thisis.plus/202304272229714.png" alt="image.png" /> <figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure> 别慌，依赖已经下载成功了，只是下载到了一个不在PATH的路径，接下来如果要使用这些被提到的库就需要指明路径，例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p><p>接下来运行tutorial_train <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate config<br></code></pre></td></tr></table></figure> 全部选NO就好，如果你有多卡什么的可以参考<a href="https://huggingface.co/docs/accelerate/index">官方文档</a></p><p>我们需要测试数据集 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png<br><br>wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png<br></code></pre></td></tr></table></figure></p><p>接着运行，设置基础模型和模型输出目录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;./out_models&quot;</span><br><span class="hljs-built_in">export</span> MODEL_DIR=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span><br></code></pre></td></tr></table></figure></p><p>运行代码，这里epoch=1，steps=1 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/home/ubuntu/.local/bin/accelerate launch train_controlnet.py   --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_DIR</span>  --output_dir=<span class="hljs-variable">$OUTPUT_DIR</span>   --dataset_name=fusing/fill50k   --resolution=512   --learning_rate=1e-5   --validation_image <span class="hljs-string">&quot;./conditioning_image_1.png&quot;</span> <span class="hljs-string">&quot;./conditioning_image_2.png&quot;</span>   --validation_prompt <span class="hljs-string">&quot;red circle with blue background&quot;</span> <span class="hljs-string">&quot;cyan circle with brown floral background&quot;</span>   --train_batch_size=4 --num_train_epochs=1 --max_train_steps=1<br></code></pre></td></tr></table></figure></p><h3 id="推理">推理</h3><p>新建一个文件<code>inference.py</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler<br><span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image<br><span class="hljs-keyword">import</span> torch<br><br>base_model_path = <span class="hljs-string">&quot;path to model&quot;</span><br>controlnet_path = <span class="hljs-string">&quot;path to controlnet&quot;</span><br><br>controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    base_model_path, controlnet=controlnet, torch_dtype=torch.float16<br>)<br><br><span class="hljs-comment"># speed up diffusion process with faster scheduler and memory optimization</span><br>pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)<br><span class="hljs-comment"># remove following line if xformers is not installed</span><br>pipe.enable_xformers_memory_efficient_attention()<br><br>pipe.enable_model_cpu_offload()<br><br>control_image = load_image(<span class="hljs-string">&quot;./conditioning_image_1.png&quot;</span>)<br>prompt = <span class="hljs-string">&quot;pale golden rod circle with old lace background&quot;</span><br><br><span class="hljs-comment"># generate image</span><br>generator = torch.manual_seed(<span class="hljs-number">0</span>)<br>image = pipe(<br>     prompt, num_inference_steps=<span class="hljs-number">20</span>, generator=generator, image=control_image<br>).images[<span class="hljs-number">0</span>]<br><br>image.save(<span class="hljs-string">&quot;./output.png&quot;</span>)<br></code></pre></td></tr></table></figure></p><p>这里的base_model_path 和 controlnet_path 改成之前设置的MODEL_DIR和OUTPUT_DIR(注意顺序)</p><p>接下来运行就可 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python inference.py<br></code></pre></td></tr></table></figure></p><p>结果会被保存到<code>output.png</code></p><h3 id="踩坑解决-1">踩坑解决</h3><h4 id="warning-the-scripts-accelerate-accelerate-config-and-accelerate-launch-are-installed-in-homeubuntu.localbin-which-is-not-on-path.consider-adding-this-directory-to-path-or-if-you-prefer-to-suppress-this-warning-use---no-warn-script-location.">WARNING: The scripts accelerate, accelerate-config and accelerate-launch are installed in '/home/ubuntu/.local/bin' which is not on PATH.Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</h4><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>The scripts accelerate, accelerate-config and accelerate-launch are installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script transformers-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script ftfy is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script tensorboard is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br><span class="hljs-symbol">WARNING: </span>The script datasets-cli is installed in <span class="hljs-emphasis">&#x27;/home/ubuntu/.local/bin&#x27;</span> which is not on PATH.<br>Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.<br></code></pre></td></tr></table></figure><p>类似的问题，这里的依赖已经安装成功了，只是被安装到了未被添加到PATH的目录，接下来运行的时候只需要指明目录即可。例如下面我们要使用accelerate，正常的用法是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">accelerate 你要执行的东西<br></code></pre></td></tr></table></figure> 我们只需要改成 <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/.local/</span>bin/accelerate 你要执行的东西<br></code></pre></td></tr></table></figure></p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/27/ControlNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>I3D笔记</title>
      <link>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Sun, 23 Apr 2023 22:14:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;i3d笔记&quot;&gt;I3D笔记&lt;/h1&gt;
&lt;p&gt;I3D是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics&lt;/p&gt;
&lt;h2 id=&quot;工作回顾&quot;&gt;工作回顾&lt;/h2&gt;
&lt;figure&gt;
&lt;img </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="i3d笔记">I3D笔记</h1><p>I3D是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics</p><h2 id="工作回顾">工作回顾</h2><figure><img src="https://proxy.thisis.plus/20230423215707.png" alt="" /><figcaption>image.png</figcaption></figure><p>在以前，视频理解有三种做法 1. LSTM 2. 3D ConvNets 3. Two-Stream Networks（双流网络）</p><h2 id="two-stream-inflated-3d-convnets">Two-Stream Inflated 3D ConvNets</h2><p>这篇文章提出的模型被称为 Two-Stream Inflated 3D ConvNets</p><p>Inflate 是模型的核心操作，含义是将一个2d模型"膨胀"成3d模型，做法很简单，就是把一个<span class="math inline">\(N*N\)</span> 的层变成<span class="math inline">\(N*N*N\)</span> ,同时也将参数复制了<span class="math inline">\(N\)</span> 遍。</p><h2 id="kinetics">Kinetics</h2><p>在视频领域，在一个足够大的数据集上训练一个动作分类网络，当应用于不同的时间任务或数据集时，是否会有类似的性能提升是一个悬而未决的问题。构建视频数据集的挑战意味着大多数流行的动作识别基准。</p><p>Kinetics 有400个人体动作类，每个类有400多个例子，每个都来自一个独特的 YouTube 视频</p><h3 id="整体架构">整体架构</h3><p>作者选择了 Inception-v1 构建整个神经网络(作者当时不适用Inception-v1是因为当时认为Inception在视频理解更合适，但架不住ResNet 太棒了，作者在18年也换成了ResNet) <img src="https://proxy.thisis.plus/20230423220521.png" alt="image.png" /></p><p>图中的Inc. 就是经典的Inception-v1 块了，只是做了Inflating 操作</p>]]></content:encoded>
      
      
      
      
      <comments>https://studyinglover.com/2023/04/23/I3D%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>clip_interrogator教程</title>
      <link>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/</link>
      <guid>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/</guid>
      <pubDate>Sat, 22 Apr 2023 22:24:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;clip_interrogator教程&quot;&gt;clip_interrogator教程&lt;/h1&gt;
&lt;p&gt;文字生成图片是近年来多模态和大模型研究的热门方向，openai提出的CLIP提供了一个方法建立起了图片和文字的联系，但是只能做到给定一张图片选择给定文本语义最相近的那</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="clip_interrogator教程">clip_interrogator教程</h1><p>文字生成图片是近年来多模态和大模型研究的热门方向，openai提出的CLIP提供了一个方法建立起了图片和文字的联系，但是只能做到给定一张图片选择给定文本语义最相近的那一个，实际项目开发中我们总是需要从一张图片获取描述，感谢社区的活力，clip-interrogator应运而生。</p><p>受限于clip-interrogator 等于没有的文档，就有了这篇文章来写一些怎么使用clip-interrogator。</p><p>clip-interrogator项目地址<a href="https://github.com/pharmapsychotic/clip-interrogator">GitHub</a></p><p>在线体验<a href="https://huggingface.co/spaces/pharma/CLIP-Interrogator">huggingface-clip-interrogator</a> <a href="https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">huggingface-clip-interrogator2</a></p><h2 id="clip-interrogator原理">clip-interrogator原理</h2><p>首先，clip-interrogator会使用BILP生成一段对图片的自然语言描述。</p><p>接下来会根据四种模式，从data文件夹下的txt文件中组合出文字生成图片常用的prompt,通过CLIP进行编码，然后将图片也用CLIP进行编码，计算出相似度最大的一组prompt,和BILP生成的prompt拼接到一起，就得到了一组prompt。</p><h2 id="安装">安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.5.4<br></code></pre></td></tr></table></figure><p>如果需要BLIP2最新的WIP支持，运行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.6.0<br></code></pre></td></tr></table></figure></p><h2 id="使用">使用</h2><h3 id="快速开始">快速开始</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate(image))<br></code></pre></td></tr></table></figure><p>将<code>image_path</code> 换成自己图片的路径即可</p><h3 id="模型">模型</h3><h4 id="blip">BLIP</h4><p>BLIP可以传入两种选项，<code>large</code> 和 <code>base</code>，默认使用<code>large</code></p><p>base用法是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(caption_model_name=<span class="hljs-string">&#x27;blip-base&#x27;</span>,clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><h4 id="clip">CLIP</h4><p>这里使用的模型的是openai的ViT-L-14。</p><p>我们也可以更改模型，文档在这完全没说清可以用什么，我做了试错</p><p>报错显示可用的模型有<code>'coca_base', 'coca_roberta-ViT-B-32', 'coca_ViT-B-32', 'coca_ViT-L-14', 'convnext_base', 'convnext_base_w', 'convnext_base_w_320', 'convnext_large', 'convnext_large_d', 'convnext_large_d_320', 'convnext_small', 'convnext_tiny', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_xxlarge_320', 'mt5-base-ViT-B-32', 'mt5-xl-ViT-H-14', 'RN50', 'RN50-quickgelu', 'RN50x4', 'RN50x16', 'RN50x64', 'RN101', 'RN101-quickgelu', 'roberta-ViT-B-32', 'swin_base_patch4_window7_224', 'ViT-B-16', 'ViT-B-16-plus', 'ViT-B-16-plus-240', 'ViT-B-32', 'ViT-B-32-plus-256', 'ViT-B-32-quickgelu', 'ViT-bigG-14', 'ViT-e-14', 'ViT-g-14', 'ViT-H-14', 'ViT-H-16', 'ViT-L-14', 'ViT-L-14-280', 'ViT-L-14-336', 'ViT-L-16', 'ViT-L-16-320', 'ViT-M-16', 'ViT-M-16-alt', 'ViT-M-32', 'ViT-M-32-alt', 'ViT-S-16', 'ViT-S-16-alt', 'ViT-S-32', 'ViT-S-32-alt', 'vit_medium_patch16_gap_256', 'vit_relpos_medium_patch16_cls_224', 'xlm-roberta-base-ViT-B-32', 'xlm-roberta-large-ViT-H-14'</code></p><blockquote><p>这里其实是我一直没搞懂的一个地方，经过很多次试错,<code>/</code> 前面被称为model，但是很多模型是用不了的，<code>/</code> 后面被称作 tag (通过读源码猜测是预训练模型来源) ，是可以写不同的内容，例如<code>openai</code> ，有的时候还需要不填，但是究竟可以怎么组合一直没找到，下面做了一个总结,</p></blockquote><table><thead><tr class="header"><th>模型</th><th>tag</th></tr></thead><tbody><tr class="odd"><td>coca_base</td><td>不传</td></tr><tr class="even"><td>RN50</td><td>'openai', 'yfcc15m', 'cc12m'</td></tr><tr class="odd"><td>RN50-quickgelu</td><td>'openai', 'yfcc15m', 'cc12m'</td></tr><tr class="even"><td>RN101</td><td>'openai', 'yfcc15m'</td></tr><tr class="odd"><td>RN101-quickgelu</td><td>'openai', 'yfcc15m'</td></tr><tr class="even"><td>RN50x4</td><td>'openai'</td></tr><tr class="odd"><td>RN50x16</td><td>'openai'</td></tr><tr class="even"><td>RN50x64</td><td>'openai'</td></tr><tr class="odd"><td>ViT-B-32</td><td>'openai', 'laion400m_e31', 'laion400m_e32', 'laion2b_e16', 'laion2b_s34b_b79k'</td></tr><tr class="even"><td>ViT-B-32-quickgelu</td><td>'openai', 'laion400m_e31', 'laion400m_e32'</td></tr><tr class="odd"><td>ViT-B-16</td><td>'openai', 'laion400m_e31', 'laion400m_e32', 'laion2b_s34b_b88k'</td></tr><tr class="even"><td>ViT-L-14-336</td><td>'openai'</td></tr><tr class="odd"><td>ViT-S-32-alt</td><td>不传</td></tr><tr class="even"><td>ViT-S-32</td><td>不传</td></tr><tr class="odd"><td>ViT-S-16-alt</td><td>不传</td></tr><tr class="even"><td>ViT-S-16</td><td>不传</td></tr><tr class="odd"><td>ViT-M-32-alt</td><td>不传</td></tr><tr class="even"><td>ViT-M-32</td><td>不传</td></tr><tr class="odd"><td>ViT-M-16-alt</td><td>不传</td></tr><tr class="even"><td>ViT-M-16</td><td>不传</td></tr><tr class="odd"><td>xlm-roberta-base-ViT-B-32</td><td>'laion5b_s13b_b90k'</td></tr><tr class="even"><td>xlm-roberta-large-ViT-H-14</td><td>'frozen_laion5b_s13b_b90k'</td></tr></tbody></table><blockquote><p>不传的意思是不写<code>/</code> 后面的部分不是只写模型名字，正确的用法例如<code>coca_base/</code></p></blockquote><p>例如使用<code>RN50-quickgelu/openai</code> 的用法就是<code>ci = Interrogator(Config(clip_model_name="RN50-quickgelu/openai"))</code></p><blockquote><p>文档中有这么一句ViT-L for Stable Diffusion 1, and ViT-H for Stable Diffusion 2，意思是 ViT-L 是给 Stable Diffusion 1 用的，ViT-H是给 Stable Diffusion 2 用的</p></blockquote><h3 id="模式">模式</h3><p>模式有<code>best</code> ， <code>classic</code>， <code>fast</code>和<code>negative</code> 三种，开发者在这里的设计很奇怪，不同模式的使用不是传不同的参数而是使用不同的方法。<code>best</code> 模式就是上面的用法，<code>fast</code> 模式的用法是 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><p><code>classic</code> 模式用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_classic(image))<br></code></pre></td></tr></table></figure></p><p><code>negative</code> 模式用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_negative(image))<br></code></pre></td></tr></table></figure></p><h3 id="quiet">quiet</h3><p><code>quiet</code> 选项的作用是不输出中间过程，使用方法是直接写进Config 即可 ，例如 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/content/test.png&#x27;</span>).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;RN50-quickgelu/openai&quot;</span>,quiet=<span class="hljs-literal">True</span>))<br><span class="hljs-built_in">print</span>(ci.interrogate_fast(image))<br></code></pre></td></tr></table></figure></p><p>使用前，会有各种进度条 <img src="https://proxy.thisis.plus/20230422221658.png" alt="image.png" /></p><p>使用后，所有过程中的输出会被隐藏 <img src="https://proxy.thisis.plus/20230422221818.png" alt="image.png" /></p><h2 id="自定义词库">自定义词库</h2><p>如果你安装的是0.6.0，那么可以使用自定义词库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator, LabelTable, load_list<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>ci = Interrogator(Config(blip_model_type=<span class="hljs-literal">None</span>))<br>image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>table = LabelTable(load_list(<span class="hljs-string">&#x27;terms.txt&#x27;</span>), <span class="hljs-string">&#x27;terms&#x27;</span>, ci)<br>best_match = table.rank(ci.image_to_features(image), top_count=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(best_match)<br></code></pre></td></tr></table></figure>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/22/clip_interrogator%E6%95%99%E7%A8%8B/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ControlNet代码改造计划</title>
      <link>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/</link>
      <guid>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/</guid>
      <pubDate>Fri, 21 Apr 2023 11:30:00 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;虽然现在webui已经支持了ControlNet，但是如果我们需要单独抽出来ControlNet做一些项目就需要对ControlNet进行改造。同时我也想加入一些开源的工具让ControlNet更加有趣，例如&lt;a href=&quot;https://github.com/pharm</description>
        
      
      
      
      <content:encoded><![CDATA[<p>虽然现在webui已经支持了ControlNet，但是如果我们需要单独抽出来ControlNet做一些项目就需要对ControlNet进行改造。同时我也想加入一些开源的工具让ControlNet更加有趣，例如<a href="https://github.com/pharmapsychotic/clip-interrogator">clip_interrogator</a>.</p><p>关于什么是Canny，Hough，可以看北邮鲁鹏老师的课程<a href="https://www.bilibili.com/video/BV1nz4y197Qv/?spm_id_from=333.999.0.0&amp;vd_source=e8f062c423dc7ce759a573dd732735a0">计算机视觉（本科）北京邮电大学 鲁鹏</a></p><p>如果你想在webui使用ControlNet，可以看我之前的<a href="https://studyinglover.com/2023/03/20/%E9%80%9A%E8%BF%87colab%E4%BD%93%E9%AA%8CControlNet/">文章</a> ，或者直接查看<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">webui</a></p><p>项目开源在<a href="https://github.com/StudyingLover/cmd_ControlNet">GitHub</a></p><p>我的博客<a href="https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/">https://studyinglover.com</a></p><h2 id="下载源码和模型">下载源码和模型</h2><p>ControlNet项目主页</p><p><a href="https://github.com/lllyasviel/ControlNet">github</a> , <a href="https://huggingface.co/lllyasviel/ControlNet">huggingface</a></p><p>先下载源码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/lllyasviel/ControlNet.git <br></code></pre></td></tr></table></figure></p><p>接下来下载需要的模型，进入huggingface 页面，选择<code>files and versions</code> <img src="https://proxy.thisis.plus/20230421105906.png" alt="image.png" /></p><p>先下载所有的annotator,进入<code>annotator/ckpts</code>文件夹,可以看到我们需要的ckpts文件，进入一个，右键download，选择复制下载链接 <img src="https://proxy.thisis.plus/20230421110144.png" alt="image.png" /></p><p>执行命令，就会将模型下载下来 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget 复制的链接<br></code></pre></td></tr></table></figure></p><p>我这里整理了ckpt文件所有的下载的链接和命令,<code>/root/ControlNet/annotator/ckpts/</code> 是我的路径，换成你自己的就行 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/ControlNet/annotator/ckpts/<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt<br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.depth <br><br>wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth<br></code></pre></td></tr></table></figure></p><p>接下来下载模型，假如我们需要canny2image，那我就需要下载<code>control_sd15_canny.pth</code> 这个文件，类似上面的方法，命令是 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth<br></code></pre></td></tr></table></figure></p><blockquote><p>其实我们可以对比网址链接和下载链接</p><p>网址链接：https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_canny.pth</p><p>下载链接：https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth</p><p>我们只需要把网址链接的blob换成resolve就可以了。</p></blockquote><h2 id="改造">改造</h2><p>我们依然以camny2image为例，打开<code>gradio_canny2image,py</code> 文件，可以看到这个文件大概是这个样子 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> 各种依赖<br><br>apply_canny = CannyDetector()<span class="hljs-comment"># 创建了一个canny算子，用来将图片转换成canny图</span><br><br>model = create_model(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>).cpu()<br>model.load_state_dict(load_state_dict(<span class="hljs-string">&#x27;./models/control_sd15_canny.pth&#x27;</span>, location=<span class="hljs-string">&#x27;cuda&#x27;</span>))<br>model = model.cuda()<br>ddim_sampler = DDIMSampler(model)<span class="hljs-comment"># 加载了模型</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>(<span class="hljs-params">input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold</span>):<br>    <span class="hljs-comment"># 一堆操作</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">255</span> - detected_map] + results<span class="hljs-comment"># 返回了canny图和生成的图片</span><br><br>block = gr.Blocks().queue()<span class="hljs-comment"># 创建一个gradio应用</span><br><span class="hljs-keyword">with</span> block:<br><span class="hljs-comment"># 又是一通操作，创建了各种gradio页面</span><br><br>block.launch(server_name=<span class="hljs-string">&#x27;0.0.0.0&#x27;</span>)<span class="hljs-comment"># 启动了gradio应用</span><br></code></pre></td></tr></table></figure></p><p>这样子我们只需要<code>process</code> 这个函数就可以了，那我们就可以把代码改成这样 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> share <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> config<br><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> einops<br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">from</span> pytorch_lightning <span class="hljs-keyword">import</span> seed_everything<br><span class="hljs-keyword">from</span> annotator.util <span class="hljs-keyword">import</span> resize_image, HWC3<br><span class="hljs-keyword">from</span> annotator.canny <span class="hljs-keyword">import</span> CannyDetector<br><span class="hljs-keyword">from</span> cldm.model <span class="hljs-keyword">import</span> create_model, load_state_dict<br><span class="hljs-keyword">from</span> cldm.ddim_hacked <span class="hljs-keyword">import</span> DDIMSampler<br><br>apply_canny = CannyDetector()<br>  <br>model = create_model(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>).cpu()<br><br>model.load_state_dict(load_state_dict(<span class="hljs-string">&#x27;./models/control_sd15_canny.pth&#x27;</span>, location=<span class="hljs-string">&#x27;cuda&#x27;</span>))<br>model = model.cuda()<br>ddim_sampler = DDIMSampler(model)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>(<span class="hljs-params">input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold</span>):<br><span class="hljs-comment"># 这块直接复制源码process函数</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">255</span> - detected_map] + results<br><br><br><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;__main__&#x27;</span> == __name__:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--image_path&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;test.png&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;original image path&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;1people&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--a_prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;best quality, extremely detailed&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;added prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--n_prompt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;negative prompt&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_samples&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;number of samples&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--image_resolution&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">512</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;image resolution&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--ddim_steps&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">30</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;ddim steps&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--is_saved&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">True</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;is saved?&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--is_show&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">False</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;is show?&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--guess_mode&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bool</span>, default=<span class="hljs-literal">False</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;guess mode&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--strength&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">1.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;strength&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--scale&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">9.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;scale&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=-<span class="hljs-number">1</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;seed&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--eta&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;eta&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--low_threshold&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">100</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;low threshold&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--high_threshold&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">200</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;high threshold&#x27;</span>)<br>  <br><br>    opt = parser.parse_args()<br>    <br>    img=cv2.imread(opt.image_path)<br>    out=process(img, opt.prompt, opt.a_prompt, opt.n_prompt, opt.num_samples, opt.image_resolution, opt.ddim_steps, opt.guess_mode, opt.strength, opt.scale, opt.seed, opt.eta, opt.low_threshold, opt.high_threshold)<br><br>    <span class="hljs-keyword">if</span>(opt.is_show):<br>        cv2.imshow(<span class="hljs-string">&#x27;out&#x27;</span>,out[<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">if</span>(opt.is_saved):<br>        cv2.imwrite(<span class="hljs-string">&#x27;out.png&#x27;</span>,out[<span class="hljs-number">1</span>])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;saved to out.png&#x27;</span>)<br></code></pre></td></tr></table></figure></p><p>用法就是parser的用法，例如 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python cmd_canny2image.py --image_path ./test_imgs/main.png --prompt bule_hair,color_clothes <br></code></pre></td></tr></table></figure> 输出的图片会被保存到当前目录下的out.png文件</p><h2 id="clip_interrogator">clip_interrogator</h2><p>你可以在<a href="https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">huggingface</a> 直接体验，这里是代码调用相应接口。</p><p><a href="https://studyinglover.com/2023/04/22/clip_interrogator教程/">clip_interrogator教程</a></p><p>先下载 clip_interrogator <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install clip-interrogator==0.5.4<br></code></pre></td></tr></table></figure></p><p>接下来调用 clip_interrogator <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> clip_interrogator <span class="hljs-keyword">import</span> Config, Interrogator<br><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>img=cv.imread(<span class="hljs-string">&#x27;/content/src/test.png&#x27;</span>)<br>img = cv.cvtColor(img,cv.COLOR_BGR2RGB)<br>img = Image.fromarray(img)<br><br>ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br><br>describe=ci.interrogate(img)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#x27;</span>+describe)<br></code></pre></td></tr></table></figure></p><p>我们将他封装成函数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">app</span>(<span class="hljs-params">numpy_img</span>):<br>    img = cv.cvtColor(numpy_img,cv.COLOR_BGR2RGB)<br>    img = Image.fromarray(img)<br>    ci = Interrogator(Config(clip_model_name=<span class="hljs-string">&quot;ViT-L-14/openai&quot;</span>))<br>    describe=ci.interrogate(img)<br>    <span class="hljs-keyword">return</span> describe<br></code></pre></td></tr></table></figure></p><p>在 <a href="https://colab.research.google.com/github/StudyingLover/cmd_ControlNet/blob/master/fix_ControlNet_and_CLIPinterrogator.ipynb"><img src="https://proxy.thisis.plus/colab-badge.svg" alt="Open In Colab" /></a> 可以体验 ControlNet和CLIPinterrogator 混合使用，两张图片都从url引入，一张图获取prompt，ptompt和另一张图一起输入输入canny2image，生成的图片展示在输出框底部</p>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/21/ControlNet%E4%BB%A3%E7%A0%81%E6%94%B9%E9%80%A0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>文字生成图片综述</title>
      <link>https://studyinglover.com/2023/04/20/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87%E7%BB%BC%E8%BF%B0/</link>
      <guid>https://studyinglover.com/2023/04/20/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87%E7%BB%BC%E8%BF%B0/</guid>
      <pubDate>Thu, 20 Apr 2023 15:30:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 style=&quot;text-align: center&quot;&gt;
文字生成图片综述
&lt;/h1&gt;
&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;根据文字生成图像，是近几年大模型领域和多模态比较热门的研究。以NovelAI，waifu等为代表的二次元模型极大地拓展了 stable di</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 style="text-align: center">文字生成图片综述</h1><h2 id="背景">背景</h2><p>根据文字生成图像，是近几年大模型领域和多模态比较热门的研究。以NovelAI，waifu等为代表的二次元模型极大地拓展了 stable diffusion <sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup><sup id="fnref:24" class="footnote-ref"><a href="#fn:24" rel="footnote"><span class="hint--top hint--rounded" aria-label="Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022.">[24]</span></a></sup>模型和生态的想象空间。例如原本做AIGC生成小说的NovelAI推出了自己的二次元图像生成模型，基于 SD 算法框架和 Danbooru 二次元图库数据集进行训练和优化。像 NovelAI 这类的二次元模型对于用户输入的描述词的专业程度要求较高，也由社区自发整理了大量的魔典(prompt).精确控制图像的生成也是AI绘画的一个发展方向，各种可以控制人物动作，位置的方法<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models.">[10]</span></a></sup><sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup><sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup>被提出.最近openai也开源了他们最新的研究Consistency Models<sup id="fnref:20" class="footnote-ref"><a href="#fn:20" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models.">[20]</span></a></sup> ,可以1s内生成多张图片。此外，stable diffusion也被用在了3d模型的生成方面，例如 dreamfusion<sup id="fnref:25" class="footnote-ref"><a href="#fn:25" rel="footnote"><span class="hint--top hint--rounded" aria-label="Poole, B., Jain, A., Barron, J., Mildenhall, B., Research, G., &amp; Berkeley, U. (n.d.). DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION.">[25]</span></a></sup>,Point-E<sup id="fnref:26" class="footnote-ref"><a href="#fn:26" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., &amp; Chen, M. (2022). Point-E: A System for Generating 3D Point Clouds from Complex Prompts.">[26]</span></a></sup> 等。</p><h2 id="图像生成">图像生成</h2><h3 id="hypernetwork">hypernetwork</h3><p>hypernetwork是一种神经网络的处理方法<sup id="fnref:21" class="footnote-ref"><a href="#fn:21" rel="footnote"><span class="hint--top hint--rounded" aria-label="Abbas, M., Kivinen, J., &amp; Raiko, T. (2016). International Conference on Learning Representations (ICLR).">[21]</span></a></sup> 主要方法是通过一个神经网络影响另一个神经网络的参数，其中最具有代表性的就是GAN<sup id="fnref:22" class="footnote-ref"><a href="#fn:22" rel="footnote"><span class="hint--top hint--rounded" aria-label="Alaluf, Y., Tov, O., Mokady, R., Gal, R., &amp; Bermano, A. (2021). HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.">[22]</span></a></sup><sup id="fnref:23" class="footnote-ref"><a href="#fn:23" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dinh, TanM., Tran, A., Nguyen, R., &amp; Hua, B.-S. (n.d.). HyperInverter: Improving StyleGAN Inversion via Hypernetwork.">[23]</span></a></sup> 了.</p><h3 id="扩散模型">扩散模型</h3><p>扩散模型第一次在<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sohl-Dickstein, J., Weiss, EricL., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics.">[1]</span></a></sup> 中被提出,被称为Diffusion Probabilistic Model,之后提出的DDPM<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.">[2]</span></a></sup>中被改进。之后DDPM也衍生出了诸多版本。发布在CVPR2022的LDM<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>将图片放到隐空间上实现了图片高质量合成并提出了内容引导机制，可以通过prompt让图片生成特定内容。一年后LDM衍生除了stable diffusion<sup id="fnref:24" class="footnote-ref"><a href="#fn:24" rel="footnote"><span class="hint--top hint--rounded" aria-label="Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022.">[24]</span></a></sup><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>，掀起了ai画图的热潮。</p><h3 id="ddpm">DDPM</h3><p>DDPM分为前向过程和反向过程。DDPM假定整个过程都是一个参数化的马尔科夫链，在前向过程中对数据逐步增加高斯噪声直到数据变成一个高斯噪声，反向过程中使用U-Net<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234–241). https://doi.org/10.1007/978-3-319-24574-4_28">[4]</span></a></sup> 预测反向添加的噪声进行去噪。</p><p>从 <span class="math inline">\(\mathbf{X}_{T}\to\mathbf{X}_{o}\)</span> 是扩散模型的逆过程，这是在生成数据的时候是从一个随机的高斯分布采样一个信号，逐步通过去噪声恢复目标信号， <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_{t})\)</span> 这个过程的解析式是未知的。前向过程是从 <span class="math inline">\(\mathbf{X}_{0}\rightarrow\mathbf{X}_{T}\)</span> ，对一个真实信号逐步加噪声，通过选取合适的噪声尺度，理论上在一定步数以后真实信号也会变成高斯信号，可以把这个过程表示为 <span class="math inline">\(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})\)</span> 。</p><p>利用重参数化技术，我们可以得到从0到t的直接采样可以得到<span class="math inline">\(q(\mathbf{x}_t|\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}}\mathbf{x}_0,(1-\bar{\alpha})\mathbf{I})\)</span> ,其中 <span class="math inline">\(\alpha_{t}=1-\beta_{t},\bar{\alpha}t=\prod i=1^{t}\alpha_{i}\,\beta_{t}\)</span> 表示前向过程每一步的方差。这样在训练的时候我们就可以随机采样一个时刻，然后计算处这个时刻的 <span class="math inline">\(\mathbf{X}_t\)</span>。<span class="math display">\[\mathbf{x}_t(\mathbf{x}_0,\epsilon_t)=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t,\epsilon_t\sim\mathcal{N}(\mathbf{0},\mathbf{I})\]</span> <span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span> 是未知的，但是可以求出<span class="math inline">\(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})\)</span> 的解析解<span class="math display">\[q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}(\mathbf{x}_t,\mathbf{x}_0),\tilde{\beta}_t\mathbf{I})\]</span> <span class="math display">\[\tilde{\mu}_t\bigl(\mathbf{x}_t,\mathbf{x}_0\bigr)=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0,\tilde{\beta}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\]</span> DDPM中推导出了基于噪声误差的损失函数，即通过网络估计噪声，而不是直接估计 <span class="math inline">\(\mathbf{X}_t\)</span> ,损失函数是<span class="math display">\[\begin{aligned}L_t^{\text{simple}}&amp;=\mathbb{E}_{t\sim[1,T],\mathbf{x}_0,\epsilon_t}\left[\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\|^2\right]\\ &amp;=\mathbb{E}_{t\sim[1,T]\mathbf{x}_0,\epsilon_t}\left[\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_\theta(\sqrt{\boldsymbol{\alpha}_t}\mathbf{x}_0+\sqrt{1-\overline{\alpha}_t}\boldsymbol{\epsilon}_t,t)\|^2\right]\end{aligned}\]</span></p><figure><img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191252.png" alt="" /><figcaption>image.png</figcaption></figure><p>DDPM也有几个改进版本，例如DDIM<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising Diffusion Implicit Models.">[3]</span></a></sup> . DDIM采用更小的采样步数来加速生成过程。</p><h3 id="ldm">LDM</h3><p>为了降低训练模型时所需要的训练资源，使用latent space的LDM<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042">[5]</span></a></sup>被提出.尽管允许通过对相应损失项的低采样忽略感知上无关的细节，但这一步仍然需要在像素空间中进行昂贵的函数计算，这导致了巨大的计算时间和能源需求。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420095529.png" alt="image.png" /></p><p>横轴是隐变量每个维度压缩的bit率，纵坐标是模型的损失。模型在学习的过程中，随着压缩率变大，刚开始模型的损失下降很快，后面下降很慢，但仍然在优化。模型首先学习到的是semantic部分的压缩/转换（大框架），这一阶段是人物semantic部分转变，然后学习到的是细节部分的压缩/转换，这是perceptual细节处的转变。</p><p>LDM将图像从变换到latent space上，采用了encoder-decoder的机制，图像生成又回到了以前的架构上,并引入了自注意力机制，将扩散模型转换为更有效的图像生成器。给定图像<span class="math inline">\(x\in\mathbb{R}^{H\times W\times3}\)</span> ,编码器<span class="math inline">\(\mathcal{E}\)</span> 会将图片编码到<span class="math inline">\(z=\mathcal{E}(x)\)</span> ,解码器<span class="math inline">\(\mathcal{D}\)</span> 会从latent space中重建图像。给定<span class="math inline">\(\quad\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))\)</span> ,<span class="math inline">\(z\in\mathbb{R}^{h\times w\times c}\)</span> 。更重要的是下采样倍数<span class="math inline">\({f}={H/h}=W/w\)</span> ,作者采用的是<span class="math inline">\(f=2^m,m \in N\)</span> <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419191342.png" alt="image.png" /></p><p>Conditioning Mechanisms,这里的条件可以是文字、图像等。将不同模态不同大小的条件转换为一个中间表达空间。通过这种方法可以实现prompt指导图象生成。</p><h3 id="consistency-models">Consistency Models</h3><p>Consistency Models<sup id="fnref:20" class="footnote-ref"><a href="#fn:20" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models.">[20]</span></a></sup> 是openai提出的最新的一种图片生成方法</p><p>diffusion<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.">[2]</span></a></sup> 的采样过程，从先验分布<span class="math inline">\(\left(x_{t_N},t_N\right)\)</span> 出发，推导采样过程<span class="math inline">\(\left(x_{t_N},t_N\right)\to\left(x_{t_{N-1}},t_{N-1}\right)\to...\to\left(x_{t_0},t_0\right)\)</span> .</p><p>Consistency Models 假设存在一个函数<span class="math inline">\(f\)</span>，对于上述过程中的每个点，<span class="math inline">\(f\)</span>都能输出一个相同的值,即<span class="math display">\[\begin{aligned}\boldsymbol{f}(\mathbf{x}_t,t)=\boldsymbol{f}(\mathbf{x}_{t&#39;},t&#39;)\text{for all}t,t&#39;\in[\epsilon,T]\end{aligned}\]</span> 对于轨迹起点<span class="math inline">\(x_0=\epsilon\)</span> ,有<span class="math inline">\(\boldsymbol{f}(\mathbf{x}_{\boldsymbol{\epsilon}},\epsilon)=\mathbf{x}_{\boldsymbol{\epsilon}}\)</span> .那么对于轨迹中任意一点，我们代入先验分布, 即可得到 <span class="math inline">\(f(x_{T},T)=x_{\epsilon}\)</span>  。这样也就完成了一步采样。</p><h2 id="文字生成图片">文字生成图片</h2><p>文字生成图片一个重要的前提条件是建立文字和图片的联系。CLIP首先通过对比学习的方式实现了文字图片联系。FLIP和A-CLIP对CLIP进行了改进。DALLE，GLIDE，DALLE2是OPENAI发布的文生图模型，GLIDE实现了无分类器引导的图片生成，DALLE2引入CLIP进行图片生成。Imagen主要使用文字内容进行训练，图片则先生成小图再超分放大。</p><h3 id="clip">CLIP</h3><p>OPENAI提出的CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup>通过对比学习的方式建立了文字和图片的联系.在训练过程文字和图像分别经过一个文字编码器和图像编码器得到一个对应的向量，将对应的文字向量和图像向量作为正样本，不对应的向量作为负样本进行对比学习。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190351.png" alt="image.png" /> 考虑到大部分的数据集的标签都是以单词的形式存在的，比如“bird”，“cat”等等，然而在预训练阶段的文本描述大多都是某个短句，为了填补这种数据分布上的差别，作者考虑用“指示上下文”（guide context）对标签进行扩展。可以用<code>a photo of a &#123;object&#125;</code>作为文本端的输入。推理过程先给定一个提示<code>A photo of a &#123;object&#125;</code> ,这里的object可以填入任意的内容，然后通过一个文字编码器得到与输入内容分别对应的一组向量。同时图片经过一个图像编码器得到一个向量，将图片得到的向量分别和填入内容得到的向量计算余弦相似度，相似度最大的则是目标的描述。</p><h3 id="clip改进">CLIP改进</h3><p>何凯明团队提出的FLIP<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Li, Y., Fan, H., Hu, R., Feichtenhofer, C., &amp; He, K. (2022). Scaling Language-Image Pre-training via Masking.">[7]</span></a></sup>通过对图片加入mask有效提升了CLIP的推理速度，同期的A-CLIP<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., … Research, M. (n.d.). Attentive Mask CLIP.">[8]</span></a></sup>通过加入注意力机制保留了图像中具有语义信息的部分，避免随意加入mask对模型的训练造成影响。如图左侧是A-CLIP的过程，右侧是FLIP的结果。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419190233.png" alt="image.png" /></p><p>GLIDE<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … Chen, M. (n.d.). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.">[9]</span></a></sup>采用无分类器指导的扩散模型实现了图片生成。GLIDE， Guided Language to Image Diffusion for Generation and Editing，是 OpenAI 推出的文本引导图像生成模型，，但受到的关注相对较少。它甚至在 OpenAI 网站上也没有专门的帖子。GLIDE 生成分辨率为 256×256 像素的图像。实际上在论文中DALLE2被称为unCLIP。参数量上5B的GLIDE的FID得分超过了12B的DALLE</p><h3 id="dlall-e2">DLALL-E2</h3><p>DALL·E2<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (n.d.). Hierarchical Text-Conditional Image Generation with CLIP Latents.">[11]</span></a></sup>的架构加入了CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup>，通过锁住CLIP的文本编码器和图像编码器可以建立文字和图像的联系，加入prior和img decoder两个先验， 训练prior，使文本编码可以转换为图像编码，并训练decoder生成最终图像。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419192846.png" alt="image.png" /></p><h3 id="imagen">Imagen</h3><p>谷歌的 Imagen<sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … Norouzi, M. (n.d.). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.">[12]</span></a></sup>的语言模型替换成了谷歌自家的T5-XXL，图像生成部分则是先生成小图像再上采样生成大图像，这是因为纯文本训练数据要比高质量图文对数据容易获取的多.</p><h3 id="lora微调">LoRA微调</h3><p>Low-Rank Adaptation of Large Language Models (LoRA)<sup id="fnref:29" class="footnote-ref"><a href="#fn:29" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hu, EdwardJ., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models.">[29]</span></a></sup> 是一种训练方法，可以加速大型模型的训练，同时消耗更少的内存。最初是被用在语言模型上的，在文本理解，文本生成上都取得了不错的效果</p><figure><img src="https://proxy.thisis.plus/202305130808347.png" alt="" /><figcaption>image.png</figcaption></figure><p>做法是在原模型旁边增加一个旁路，通过低秩分解（先降维再升维）来模拟参数的更新量。训练时，原模型固定，只训练降维矩阵A和升维矩阵B，推理时，可将BA加到原参数上，不引入额外的推理延迟。此外这还是一个可拔插的模块，可以根据需要选择不同的rank</p><p>LoRA 的应用包括文字生成图片和图片生成图片.<sup id="fnref:31" class="footnote-ref"><a href="#fn:31" rel="footnote"><span class="hint--top hint--rounded" aria-label="cloneofsimo. (n.d.). _GitHub - Cloneofsimo/lora: Using Low-rank adaptation to quickly fine-tune diffusion models._ GitHub. Retrieved May 13, 2023, from https://github.com/cloneofsimo/lora">[31]</span></a></sup> 是第一个使用LoRA微调扩散模型的项目。Chinese-alpaca-lora<sup id="fnref:30" class="footnote-ref"><a href="#fn:30" rel="footnote"><span class="hint--top hint--rounded" aria-label="LC1332. (n.d.). _GitHub - LC1332/Chinese-alpaca-lora: 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技_. GitHub. Retrieved May 13, 2023, from https://github.com/LC1332/的应用包括文字生成图片和图片生成图片，Chinese-alpaca-lora">[30]</span></a></sup> 是一个由华中师范大学同学维护的中文语言模型，使用LoRA进行微调。</p><p>DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 也可以与LoRA结合进行微调<sup id="fnref:33" class="footnote-ref"><a href="#fn:33" rel="footnote"><span class="hint--top hint--rounded" aria-label="_Low-Rank adaptation of large language models (lora)_. (n.d.). Retrieved May 13, 2023, from https://huggingface.co/docs/diffusers/training/lora#dreambooth">[33]</span></a></sup></p><h2 id="图像编辑">图像编辑</h2><p>图像编辑也是文字生成图片的重要应用。Imagic输入一个文本图像和目标文本，通过多阶段的方法对齐文本和图像编辑图像。ControlNet通过复制参数为锁住的和可训练的，使模型可以为特定任务进行微调。同时ControlNet可以传入openpose人的位姿图，canny边缘图，深度图，Hough变换生成的图等各种图片可控得生成图片。Google的DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 提出了一种使用少量图片进行微调的方式，提供一种用户训练自己模型的方法。prompt2prompt通过更改图片对应的map的方式特定更改图片。</p><h3 id="imagic">Imagic</h3><p>Imagic<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models.">[10]</span></a></sup>提出的方法只需要一个输入图像和一个目标文本(所需的编辑)。它生成一个与输入图像和目标文本一致的文本嵌入，同时微调扩散模型以捕获特定于图像的外观。Imagic通过多阶段的方法实现了图片编辑，分为优化文本嵌入，微调扩散模型。在优化的嵌入和目标文本嵌入之间进行线性插值三个过程。首先优化文本嵌入，使其生成与输入图像相似的图像。然后，对预训练的生成扩散模型(以优化的嵌入为条件)进行微调，以更好地重建输入图像。最后，在目标文本嵌入和优化后的文本之间进行线性插值，得到一个结合了输入图像和目标文本的表示。然后将这种表示传递给带有微调模型的生成扩散过程，输出最终编辑的图像。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312151044.png" alt="image.png" /></p><h3 id="controlnet">ControlNet</h3><p>ControlNet<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup>通过对参数复制，将参数分为锁住的和可训练的。锁着的参数从大量的图片文本对中学习更通用的信息，可学习的参数在特定的任务上进行微调,让模型在个人电脑和大型计算集群上都可以获得很好的训练效果。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419193542.png" alt="image.png" /> 以2D图像为例，给定一张图像(特征图)<span class="math inline">\(\boldsymbol{x}\in\mathbb{R}^{h\times w\times c}\)</span> ,<span class="math inline">\(h,w,c\)</span> 分别代表高度，宽度，深度。一个将x转换为y的神经网络我们可以以将他记作<span class="math inline">\(\mathcal{F(\cdot;\Theta)}\)</span> 我们把zero convolution(就是1 <span class="math inline">\(*\)</span> 1卷积)记作<span class="math inline">\(\mathcal{Z}(\cdot;\cdot)\)</span> ,那么ControlNet就可以记作<span class="math display">\[\begin{matrix}\boldsymbol{y_c}=\mathcal{F}(\boldsymbol{x};\Theta)+\mathcal{Z}(\boldsymbol{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{z2}})\end{matrix}\]</span> 由于zero convolution的权重初始为0，那么就有<span class="math display">\[\begin{cases}\mathcal{Z}(c;\Theta_{\text{z1}})=\mathbf{0}\\ \mathcal{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{c}})=\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}})=\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}})\\ \mathcal{Z}(\mathcal{F}(\boldsymbol{x}+\mathcal{Z}(\boldsymbol{c};\Theta_{\text{z1}});\Theta_{\text{c}});\Theta_{\text{z2}})=\mathcal{Z}(\mathcal{F}(\boldsymbol{x};\Theta_{\text{c}});\Theta_{\text{z2}})=\mathbf{0}\end{cases}\]</span> 可以得出<span class="math inline">\(y_c=y\)</span>,即当ControlNet被应用到任何一个网络上时，不会对这个网络的效果产生任何影响。它完美保留了任何神经网络块的能力、功能和结果质量，任何进一步优化都将随着微调而变得很快。</p><p>在训练过程中，作者随机的将50%的prompt换成了空的prompt，作者认为这可以增强模型从文本内容识别语义信息的能力。这主要是因为当 stable diffusion 模型看不到提示时，decoder倾向于从输入控制图中学习更多的语义作为提示的替代品。</p><p>ControlNet还给出了在个人电脑和大型计算集群上进行训练的方式。当计算设备有限时，作者发现部分打破ControlNet与stable diffusion之间的联系可以加速收敛。默认情况下是将ControlNet连接到“SD Middle Block”和“SD Decoder Block 1,2,3,4”(stable diffuion的模块)。作者发现，只连接Middle Block而不连接Decoder Block 1,2,3,4可以将训练速度提高1.6倍(在RTX 3070TI笔记本电脑GPU上测试)。当模型在结果和条件之间表现出合理的关联时，这些断开连接的链接可以在持续训练中再次连接，以促进精确控制。</p><p>openai在论文还比较了在不同的数据集上不同的编码器的效果</p><figure><img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230420145907.png" alt="" /><figcaption>image.png</figcaption></figure><p>MutilDiffusion<sup id="fnref:14" class="footnote-ref"><a href="#fn:14" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bar-Tal, O., Yariv, L., Lipman, Y., &amp; Dekel, T. (2023). MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation.">[14]</span></a></sup> 将图片分为几个部分分别进行diffusion，然后将他们拼在一起通过一个全局去噪网络可以更好的控制生成图片中物体的位置。</p><h3 id="dreambooth">DreamBooth</h3><p>DreamBooth<sup id="fnref:32" class="footnote-ref"><a href="#fn:32" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.">[32]</span></a></sup> 是Google提出的一个通过少量图片微调diffusion model 的方法。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195507.png" alt="image.png" /> 要训练自己数据最直观的方法，就是把自己的图片加入模型迭代时一起训练。但会带来两个问题，一个是过拟合，另一个是语义漂移(language drift)。总的来说DreamBooth的贡献在两方面，一方面是给定主题可以生成特定的主题的图片，一方面给定少数镜头微调diffusion model的方法同时保留输入图片的语义信息。 而Dreambooth的优势就在于能避免上述的两个问题。主要方法就是使用一个具有特殊含义而且比较少见的词，训练的图片最好有不同角度和光线下的图片。下图是DreamBooth论文给出的不同模型效果的对比图 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230419195421.png" alt="image.png" /></p><h4 id="cascade-ef-gan">Cascade EF-GAN</h4><p>Cascade EF-GAN<sup id="fnref:28" class="footnote-ref"><a href="#fn:28" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wu, R., Zhang, G., Lu, S., &amp; Chen, T. (2020, March 12). _Cascade EF-GAN: Progressive facial expression editing with local focuses_. arXiv.Org. https://arxiv.org/abs/2003.05905">[28]</span></a></sup> 是一种级联式的人脸编辑方式，可以更好地保留与身份相关的特征和细节，特别是在眼睛、鼻子和嘴巴周围，进一步帮助减少生成的面部图像中的伪影和模糊。</p><p>作者设计了一种级联式网络，同原本对一张人脸做更改变成了对一张人脸和脸上几个部分同时做更改。因为对一个人类来说分辨一个人的方式就是看人的眼睛，鼻子和嘴巴。Cascade EF-GAN能够识别面部表情编辑中局部重点的重要性，并通过几个局部重点捕捉身份相关特征，有效地减轻编辑产生的伪影和模糊。</p><figure><img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312143755.png" alt="" /><figcaption>image.png</figcaption></figure><p>Cascade EF-GAN中的生成模型由一个Expression Transformer和一个Refiner组成。Expression Transformer执行带有局部焦点的表情编辑，Refiner融合表情转换器的输出并细化最终编辑。</p><p>Expression Transformer通过在全局和局部分支中处理面部图像来解决这个问题，其中全局分支捕获全局面部结构，局部分支专注于更详细的面部特征。Transformer将面部图像和目标表情标签作为输入。<strong>所有分支共享相似的网络架构，但不共享权重</strong></p><p>此外注意力被引入到全局和局部分支，以更好地捕捉细节和抑制伪影。在GANimation [32]中，使用视觉注意力来引导网络集中于转换与表情相关的区域。然而，在单个全局图像中应用注意力往往会引入模糊的注意力响应，如图3的第4列所示。这是因为全局注意力倾向于关注最显著的变化，例如图3中的嘴部区域，而眼睛和鼻子周围的细微变化则没有受到足够的关注。前面提到的局部分支中的独占式注意力有助于在局部区域实现更锐利的响应，如图3的第3列所示。 <img src="https://cdn.jsdelivr.net/gh/StudyingLover/anything/20230312144711.png" alt="image.png" /></p><p>每个分支输出颜色特征图M_C和注意图M_A。对于原始输入图像I_in，每个分支的初始输出通过以下方式生成 <span class="math display">\[\mathcal{I}_{init}=M_A\otimes M_C+(1-M_A)\otimes I_{in}\]</span></p><p>Refiner负责融合表情转换器不同分支的输出，生成最终的表情编辑。如图2所示，三个局部分支的输出首先根据它们在面部图像中的各自位置缝合成单个图像。缝合的图像然后与全局分支的输出连接，并馈送到细化器以生成最终的表情编辑。</p><h4 id="prompt2prompt">prompt2prompt</h4><p>prompt2prompt<sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup> 是Google提出的一种基于Imagen的图像编辑方法，相比于直接text2image生成，文本引导图片生成要求原来图像绝大部分区变化不大，先前的方法需要用户指定mask来引导生成。prompt2prompt的主要方法是将交叉注意力机制引入diffusion中，得到每个token对应的attention map，一种有三种操作的方式 1. token换词，那么直接替换attention map即可。 2. 加词，则是直接在对应位置加入新的attention map。 3. token增强——直接提高对应的map的权重。</p><p>但这种方法也有一些局限，例如需要用户给一个合理的prompt，细节的生成不太好，不能对图中的物体进行移位操作。</p><h4 id="instructpix2pix">InstructPix2Pix</h4><p>InstructPix2Pix<sup id="fnref:27" class="footnote-ref"><a href="#fn:27" rel="footnote"><span class="hint--top hint--rounded" aria-label="Brooks, T., Holynski, A., &amp; Efros, AlexeiA. (2022). InstructPix2Pix: Learning to Follow Image Editing Instructions.">[27]</span></a></sup> 是一种无需微调就可以快速编辑图像的方法，结合了两个大型预训练模型的知识——语言模型和文本到图像模型——生成了大量的图像编辑示例数据集。通过在这些数据上进行训练，并在推理时能够适用于真实图像和用户编写的指令。但也有一些局限例如数据带来的偏差，能会对图像进行不必要的过度更改。</p><h2 id="多模态">多模态</h2><p>多模态学习是指从多个模态表达或感知事物。 多模态可归类为同质性的模态，例如从两台相机中分别拍摄的图片，异质性的模态，例如图片与文本语言的关系。Jeff Dean在2019年年底NeurIPS大会上的一个采访报道，讲到了2020年机器学习趋势：多任务和多模态学习将成为突破口。</p><h3 id="clip-1">CLIP</h3><p>CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 是openai关于文本和图像的一片工作，采用对比学习实现了图片的理解。</p><p>AudioCLIP 在原本的CLIP架构中加入了声音的模态。</p><h3 id="i3d">I3D</h3><p>I3D<sup id="fnref:34" class="footnote-ref"><a href="#fn:34" rel="footnote"><span class="hint--top hint--rounded" aria-label="Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502">[34]</span></a></sup> 是一个视频理解模型，采用双流网络的架构，他的核心贡献是提出了如何对2d网络进行膨胀操作，同时提出了一个新的数据集 Kinetics</p><figure><img src="https://proxy.thisis.plus/20230423220521.png" alt="" /><figcaption>image.png</figcaption></figure><p>这篇文章提出的模型被称为 Two-Stream Inflated 3D ConvNets</p><p>Inflate 是模型的核心操作，含义是将一个2d模型"膨胀"成3d模型，做法很简单，就是把一个<span class="math inline">\(N*N\)</span> 的层变成<span class="math inline">\(N*N*N\)</span> ,同时也将参数复制了<span class="math inline">\(N\)</span> 遍。</p><h3 id="segement-anything">Segement anything</h3><p>Segement anything<sup id="fnref:15" class="footnote-ref"><a href="#fn:15" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything.">[15]</span></a></sup> 是meta最近一篇图像分割的工作，使用一个SOTA的zero-shot目标检测器提取物体box和类别，然后输入给SAM模型出mask，使得模型可以根据文本输入检测和分割任意物体。</p><p>在社区的努力下，实现了Segement anything和stable diffusion的协同<sup id="fnref:36" class="footnote-ref"><a href="#fn:36" rel="footnote"><span class="hint--top hint--rounded" aria-label="IDEA-Research. (n.d.). _GitHub - IDEA-Research/Grounded-Segment-Anything: Marrying grounding DINO with segment anything &amp; stable diffusion &amp; tag2text &amp; BLIP &amp; whisper &amp; chatbot - Automatically detect , segment and generate anything with image, text, and audio inputs_. GitHub. Retrieved May 13, 2023, from https://github.com/IDEA-Research/Grounded-Segment-Anything">[36]</span></a></sup></p><h3 id="imagebind">ImageBind</h3><p>ImageBind<sup id="fnref:35" class="footnote-ref"><a href="#fn:35" rel="footnote"><span class="hint--top hint--rounded" aria-label="Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K., Joulin, A., &amp; Misra, I. (2023). ImageBind: One Embedding Space To Bind Them All.">[35]</span></a></sup> 是meta 的最新工作,是一个学习六种不同模态的方法-图像、文本、音频、深度、温度和IMU数据。此外在学习过程中不需要提供所有模态的信息，作者发现只要将每个模态的嵌入对齐到图像嵌入，就会导致所有模态的emergent alignment(涌现现象)。</p><p>ImageBind的目标是通过使用图像将所有模态绑定在一起，学习所有模态的单一联合嵌入空间。通过使用 Web 数据将每个模态的嵌入与图像嵌入对齐，例如使用带有 IMU 的自我中心相机捕获的视频数据将文本对齐到图像，将 IMU 对齐到视频，最后可以获得zero-shot的能力。然而，一个模态的不能直接应用于其他两个模态的组合，例如视频不能直接在图片-IMU上使用。</p><p>IMAGEBIND使用模态对<span class="math inline">\((I，M)\)</span> , 其中<span class="math inline">\(I\)</span>代表图像，<span class="math inline">\(M\)</span>是另一种模态，来学习单个联合嵌入.作者使用包含广泛语义概念的（图像，文本）配对的大规模网络数据集。也使用其他模态本身的自我监督配对，如音频、深度、热和惯性测量单元（IMU）与图像。使用InfoNCE<sup id="fnref:38" class="footnote-ref"><a href="#fn:38" rel="footnote"><span class="hint--top hint--rounded" aria-label="Oord, A., Li, Y., &amp; Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding.">[38]</span></a></sup>损失优化嵌入和编码器。<span class="math display">\[L_{\mathcal{I},\mathcal{M}}=-\log\frac{\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_i/\tau)}{\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_i/\tau)+\sum_{j\neq i}\exp(\mathbf{q}_i^{\text{T}}\mathbf{k}_j/\tau)}\]</span></p><p>尽管ImageBind只是用了六种模态进行训练，但是未来可以使用更多的数据和模态进行训练，将实现更丰富的以人为中心的 AI 模型。</p><h2 id="prompt">prompt</h2><p>prompt提示可以给文字生成图片提供语义信息。</p><h4 id="clip-2">CLIP</h4><p>CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 使用了<code>A photo of a &#123;object&#125;</code>作为prompt，<code>object</code> 是推理过程中的选择项，作者也讨论了大量的prompt相关的问题。一个常见的问题是一词多义。当一个类的名称是提供给CLIP文本编码器的唯一信息时，由于缺乏上下文，它无法区分哪个词的含义。在某些情况下，同一个单词的多个含义可能作为不同的类包含在同一个数据集中。另一个问题是，在我们的预训练数据集中，与图像配对的文本只是一个单词的情况相对较少。通常文本是一个完整的句子，以某种方式描述图像。通过使用<code>A photo of a &#123;object&#125;</code> 就可以使ImageNet的准确率提高1.3%。作者还发现在不同的数据集上使用不同的prompt可以取得不同的结果。</p><h4 id="controlnet-1">ControlNet</h4><p>在 ControlNet<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models.">[13]</span></a></sup> 模型采取了<em>三种</em> prompt 1. No prompt：也就是"" 2. Default prompt:由于stable diffusion本质上是使用prompt进行训练的，因此空字符串可能是模型的意外输入，如果没有提供提示，SD 往往会生成随机纹理图。更好的设置是使用无意义的提示，"an image", "a nice image", "a professional image",etc。在作者的设置中，使用"a professional, detailed, high-quality image"作为default prompt。 3. Automatic prompt:为了测试fully automatic pipeline的SOTA，作者还尝试使用fully automatic pipeline（例如，BLIP）使用“default prompt”模式获得的结果生成prompts。作者会使用生成的提示再次扩散。 4. User prompt：用户自定义的输入</p><h4 id="prompt2prompt-1">prompt2prompt</h4><p>prompt2prompt<sup id="fnref:19" class="footnote-ref"><a href="#fn:19" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models.">[19]</span></a></sup>的操作就是通过prompt进行的。</p><h3 id="clip-interrogator">CLIP-interrogator</h3><p>在背景图生成这个任务下有一个可能需要的步骤，从给出的人物图得到一些prompt生成图片，CLIP-interrogator 就是为了这样的任务而生的，开源的有CLIP-Interrogator<sup id="fnref:16" class="footnote-ref"><a href="#fn:16" rel="footnote"><span class="hint--top hint--rounded" aria-label="_CLIP interrogator_. (n.d.). A Hugging Face Space by Pharma. Retrieved April 19, 2023, from https://huggingface.co/spaces/pharma/CLIP-Interrogator">[16]</span></a></sup><sup id="fnref:18" class="footnote-ref"><a href="#fn:18" rel="footnote"><span class="hint--top hint--rounded" aria-label="pharmapsychotic. (n.d.). _GitHub - Pharmapsychotic/clip-interrogator: Image to prompt with BLIP and CLIP_. GitHub. Retrieved April 19, 2023, from https://github.com/pharmapsychotic/clip-interrogator">[18]</span></a></sup> 和CLIP-Interrogator2<sup id="fnref:17" class="footnote-ref"><a href="#fn:17" rel="footnote"><span class="hint--top hint--rounded" aria-label="_CLIP interrogator 2_. (n.d.). A Hugging Face Space by Fffiloni. Retrieved April 19, 2023, from https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2">[17]</span></a></sup> .模型主要通过CLIP对以后数据集进行匹配获取prompt、而通过BLIP获得图像最直观的理解。Code底层也是需要CLIP和BLIP作为核心完成后面的工作。</p><h2 id="数据集">数据集</h2><h3 id="wit">WIT</h3><p>openai 在CLIP<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision.">[6]</span></a></sup> 提到他们构建了一个新的数据集，从互联网上各种公开可用的资源中收集了4亿(图像，文本)对。为了尝试覆盖尽可能广泛的视觉内容，对每一类都有大概有2000对对整个数据集进行平衡。结果数据集的总字数与用于训练GPT-2的WebText数据集相似。这个数据集称为WebImageText的WIT。</p><p>作者团队从直接与摄影师一起工作的提供商那里获得了一组新的高分辨率的11M图像。即使在下采样之后，这些图像的分辨率也明显高于许多现有的视觉数据集。</p><h3 id="sa-1b">SA-1B</h3><p>segment anything<sup id="fnref:15" class="footnote-ref"><a href="#fn:15" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything.">[15]</span></a></sup> 是Meta最新提出的一个用于目标分割的方法，他们为了更好的训练模型制作了一个迄今为止最大的分割数据集，1100万张在10亿次授权且尊重隐私的图像上的数据集，同时开源了他们的数据集，此外还有一种Data engine 的方法来快速生成数据集。</p><p>Data engine 分为三个阶段：（1）模型辅助手动注释阶段，（2）混合自动预测掩码和模型辅助注释的半自动阶段，以及（3）全自动阶段，</p><h4 id="手动阶段">手动阶段</h4><p>在第一阶段，类似于经典的交互式分割，一组专业注释者通过使用由 SAM 驱动的基于浏览器的交互分割工具点击前景/背景对象点来标记掩码。可以使用像素精确的“刷”和“擦除”工具来细化掩码。模型辅助注释直接在浏览器内实时运行（使用预先计算的图像嵌入），从而实现真正的交互体验。标注不受语义约束，可以自由地标注"stuff" and "things"</p><p><strong>注释者被要求按突出顺序标记对象，一旦掩码需要超过 30 秒进行注释，便鼓励继续下一个图像。</strong></p><p>在SOTA之后，SAM就开始使用公共数据集进行训练，在经过了足够多的数据标注后，就用新标注的数据重新训练。随着收集更多的掩码，图像使用了ViT-H作为编码器。这样的模型训练一共进行了六次。随着模型的改进，每个掩码的平均注释时间从 34 秒减少到 14 秒。随着SAM的改进，每张图像的平均掩码数从20个掩码增加到44个掩码。总体而言，作者在这个阶段从 120k 张图像收集了 4.3M 掩码。</p><h4 id="半自动化阶段">半自动化阶段</h4><p>这个阶段的目标是增加mask的多样性。为了将标记集中在不太突出的对象上，首先自动检测confident masks。然后向注释者展示了用这些掩码预先填充的图像，并要求他们注释任何额外的未注释对象。为了检测confident masks，作者使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界框检测器。在这个阶段，作者在 180k 图像中收集了一个额外的 5.9M 掩码（总共 10.2M 掩码）。在第一阶段，在新收集的数据（5 次）上定期重新训练模型。每个掩码的平均注释时间可以回到了到 34 秒（不包括自动掩码），因为这些对象对标签更具挑战性。每张图像的平均掩码数从 44 个掩码到 72 个掩码（包括自动掩码）。</p><h4 id="全自动化阶段">全自动化阶段</h4><p>这个阶段的主要目的是解决歧义</p><p>这个过程作者使用<span class="math inline">\(32*32\)</span>网格的点对图像进行预测，并为每个点预测一组可能对应于有效对象的掩码。如果说一个点位于一个部件或子部件上，我们的模型将返回子部分，部分和整个对象(subpart, part, and whole object)。利用模型中的IoU预测模块来选择confident mask,IOU阈值是0.7，那么这个掩码就被认为是是稳定的。为了进一步提高小mask的质量，还处理了多个重叠的放大mask。</p><h3 id="kinetics">Kinetics</h3><p>Kinetics 是<sup id="fnref:34" class="footnote-ref"><a href="#fn:34" rel="footnote"><span class="hint--top hint--rounded" aria-label="Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502">[34]</span></a></sup> 提出的一个视频理解数据集，Kinetics 有400个人体动作类，每个类有400多个例子，每个都来自一个独特的 YouTube 视频。</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Sohl-Dickstein, J., Weiss, EricL., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ho, JonathanC., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising Diffusion Implicit Models. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Lecture Notes in Computer Science,Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234–241). https://doi.org/10.1007/978-3-319-24574-4_28 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. https://doi.org/10.1109/cvpr52688.2022.01042 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Li, Y., Fan, H., Hu, R., Feichtenhofer, C., &amp; He, K. (2022). Scaling Language-Image Pre-training via Masking. <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., … Research, M. (n.d.). Attentive Mask CLIP. <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … Chen, M. (n.d.). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., … Irani, M. (2022). Imagic: Text-Based Real Image Editing with Diffusion Models. <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (n.d.). Hierarchical Text-Conditional Image Generation with CLIP Latents. <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … Norouzi, M. (n.d.). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>Zhang, L., &amp; Agrawala, M. (n.d.). Adding Conditional Control to Text-to-Image Diffusion Models. <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:14" class="footnote-text"><span>Bar-Tal, O., Yariv, L., Lipman, Y., &amp; Dekel, T. (2023). MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. <a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:15" class="footnote-text"><span>Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … Girshick, R. (n.d.). Segment Anything. <a href="#fnref:15" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:16" class="footnote-text"><span><em>CLIP interrogator</em>. (n.d.). A Hugging Face Space by Pharma. Retrieved April 19, 2023, from https://huggingface.co/spaces/pharma/CLIP-Interrogator <a href="#fnref:16" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:17" class="footnote-text"><span><em>CLIP interrogator 2</em>. (n.d.). A Hugging Face Space by Fffiloni. Retrieved April 19, 2023, from https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2 <a href="#fnref:17" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:18" class="footnote-text"><span>pharmapsychotic. (n.d.). <em>GitHub - Pharmapsychotic/clip-interrogator: Image to prompt with BLIP and CLIP</em>. GitHub. Retrieved April 19, 2023, from https://github.com/pharmapsychotic/clip-interrogator <a href="#fnref:18" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:19" class="footnote-text"><span>Mokady, R., Hertz, A., Aberman, K., Pritch, Y., &amp; Cohen-Or, D. (2022). Null-text Inversion for Editing Real Images using Guided Diffusion Models. <a href="#fnref:19" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:20" class="footnote-text"><span>Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (n.d.). Consistency Models. <a href="#fnref:20" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:21" class="footnote-text"><span>Abbas, M., Kivinen, J., &amp; Raiko, T. (2016). International Conference on Learning Representations (ICLR). <a href="#fnref:21" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:22" class="footnote-text"><span>Alaluf, Y., Tov, O., Mokady, R., Gal, R., &amp; Bermano, A. (2021). HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing. <a href="#fnref:22" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:23" class="footnote-text"><span>Dinh, TanM., Tran, A., Nguyen, R., &amp; Hua, B.-S. (n.d.). HyperInverter: Improving StyleGAN Inversion via Hypernetwork. <a href="#fnref:23" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:24" class="footnote-text"><span>Heathen. github.com/automatic1111/stable-diffusion-webui/discussions/2670, hypernetwork style training, a tiny guide, 2022. <a href="#fnref:24" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:25" class="footnote-text"><span>Poole, B., Jain, A., Barron, J., Mildenhall, B., Research, G., &amp; Berkeley, U. (n.d.). DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION. <a href="#fnref:25" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:26" class="footnote-text"><span>Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., &amp; Chen, M. (2022). Point-E: A System for Generating 3D Point Clouds from Complex Prompts. <a href="#fnref:26" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:27" class="footnote-text"><span>Brooks, T., Holynski, A., &amp; Efros, AlexeiA. (2022). InstructPix2Pix: Learning to Follow Image Editing Instructions. <a href="#fnref:27" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:28" class="footnote-text"><span>Wu, R., Zhang, G., Lu, S., &amp; Chen, T. (2020, March 12). <em>Cascade EF-GAN: Progressive facial expression editing with local focuses</em>. arXiv.Org. https://arxiv.org/abs/2003.05905 <a href="#fnref:28" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:29" class="footnote-text"><span>Hu, EdwardJ., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <a href="#fnref:29" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:30" class="footnote-text"><span>LC1332. (n.d.). <em>GitHub - LC1332/Chinese-alpaca-lora: 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 &amp; 李鲁鲁 @ 商汤科技 &amp; 冷子昂 @ 商汤科技</em>. GitHub. Retrieved May 13, 2023, from https://github.com/LC1332/的应用包括文字生成图片和图片生成图片，Chinese-alpaca-lora <a href="#fnref:30" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:31" class="footnote-text"><span>cloneofsimo. (n.d.). <em>GitHub - Cloneofsimo/lora: Using Low-rank adaptation to quickly fine-tune diffusion models.</em> GitHub. Retrieved May 13, 2023, from https://github.com/cloneofsimo/lora <a href="#fnref:31" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:32" class="footnote-text"><span>Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &amp; Aberman, K. (2022). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. <a href="#fnref:32" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:33" class="footnote-text"><span><em>Low-Rank adaptation of large language models (lora)</em>. (n.d.). Retrieved May 13, 2023, from https://huggingface.co/docs/diffusers/training/lora#dreambooth <a href="#fnref:33" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:34" class="footnote-text"><span>Carreira, J., &amp; Zisserman, A. (2017). Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI. https://doi.org/10.1109/cvpr.2017.502 <a href="#fnref:34" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:35" class="footnote-text"><span>Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K., Joulin, A., &amp; Misra, I. (2023). ImageBind: One Embedding Space To Bind Them All. <a href="#fnref:35" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:36" class="footnote-text"><span>IDEA-Research. (n.d.). <em>GitHub - IDEA-Research/Grounded-Segment-Anything: Marrying grounding DINO with segment anything &amp; stable diffusion &amp; tag2text &amp; BLIP &amp; whisper &amp; chatbot - Automatically detect , segment and generate anything with image, text, and audio inputs</em>. GitHub. Retrieved May 13, 2023, from https://github.com/IDEA-Research/Grounded-Segment-Anything <a href="#fnref:36" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:37" class="footnote-text"><span>Guzhov, A., Raue, F., Hees, J., &amp; Dengel, A. (2021). AudioCLIP: Extending CLIP to Image, Text and Audio. <a href="#fnref:37" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:38" class="footnote-text"><span>Oord, A., Li, Y., &amp; Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. <a href="#fnref:38" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/">笔记</category>
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/04/20/%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87%E7%BB%BC%E8%BF%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Segment Anything笔记</title>
      <link>https://studyinglover.com/2023/04/07/Segment%20Anything%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/04/07/Segment%20Anything%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Fri, 07 Apr 2023 21:40:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;segment-anything笔记&quot;&gt;Segment Anything笔记&lt;/h1&gt;
&lt;p&gt;Segment Anything project是一个用于图像分割的新任务、模型和数据集。在他刚出来的那一天，知乎等平台就已经高呼CV已死。为了这个项目，作者创建了迄今为</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="segment-anything笔记">Segment Anything笔记</h1><p>Segment Anything project是一个用于图像分割的新任务、模型和数据集。在他刚出来的那一天，知乎等平台就已经高呼CV已死。为了这个项目，作者创建了迄今为止最大的分割数据集，1100万张在10亿次授权且尊重隐私的图像上的数据集。模型也被设计和训练成了promptable,就是说可以给他一些提示。作者在多个数据集测试了他的结果并认为结果令人满意。</p><figure><img src="https://proxy.thisis.plus/20230407073917.png" alt="" /><figcaption>image.png</figcaption></figure><p>代码开源<a href="https://github.com/facebookresearch/segment-anything">GitHub</a></p><p>项目地址<a href="https://segment-anything.com/">https://segment-anything.com/</a></p><h2 id="引言">引言</h2><p>作者在引言中讨论了NLP工作中prompt的巨大作用，并回顾了视觉领域多模态的重要工作CLIP和ALIGN，最后说明了他们的目标和工作。</p><p>首先，在网络上经过预训练的大型语言模型凭借其强大的zero-shot和few-shot能力革新NLP，prompt的引入使得这些模型zero-shot和few-shot性能与微调模型出奇的好。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算的增加而改善。</p><p>CLIP和ALIGN使用对比学习来训练对齐两种模态的文本和图像编码器。经过训练后，prompt可以实现对新视觉概念和数据分布的zero-shot概括。这种编码器还与其他模块有效组合，以实现下游任务，如图像生成（例如，DALL·E）。虽然在视觉和语言编码器方面已经取得了很大进展，但计算机视觉包括了超出这一范围的广泛问题，而且对于其中许多问题，还不存在丰富的训练数据。</p><p>作者提到在这项工作中，他们的目标是建立一个图像分割的基础模型。也就是说，他们在寻求开发一个可提示的模型，并使用能够实现强大泛化的任务在广泛的数据集上对其进行预训练。有了这个模型，他们的目标是使用即时工程解决新数据分布上的一系列下游分割问题。</p><p>这个计划的成功取决于三个组成部分：任务、模型和数据。为了开发它们，作者解决了以下关于图像分割的问题： 1. 什么样的样本可以实现零样本泛化 2. 相应的模型架构是什么 3. 什么样的数据可以支撑这个人物和模型</p><p>这些问题错综复杂，作者首先定义了一个promptable的分割任务，这可以提供强大的预训练目标，并且有广泛的下游任务可以应用。这个任务需要一个支持灵活的prompt的模型并且可以输出分割结果。为了训练这个模型，作者需要一个多样性的，大型的数据集，因此作者构建了一个数据引擎，使用高效的模型进行迭代。作者介绍了每个组件然后是创建的数据集和有效性的实验。</p><h2 id="任务">任务</h2><p>作者从NLP领域获得灵感，在NLP的任务中，预测下一个token用于基础模型的训练，并通过prompt engineering 解决不同的下游任务。为了建立这样一个分割的基础模型，作者的目标书建立一个具有类似能力的任务 ### Task promptable的分割任务是给定任何prompt都能返回有效的分割掩码。有效的mask意味着即使prompt是不准确的或者涉及到多个对象的也应该的能够输出正确的或者合理的掩码。如图所示，每列显示SAM从单个不明确的点提示生成的3个有效掩码。 <img src="https://proxy.thisis.plus/20230407194759.png" alt="image.png" /></p><h3 id="预训练">预训练</h3><p>promptable segmentation task 提出了一种自然的预训练算法，该算法模拟每个训练样本的提示序列（例如，点、框、掩码），并将模型的掩码预测与基本事实进行比较。作者将这种方法从交互式分割中改编出来，尽管与交互式分割不同，交互式分割的目的是在足够的用户输入后最终预测有效的掩码，但promptable segmentation task 的目的是始终预测任何提示的有效掩码，即使提示不明确的/错误的/荒谬的。</p><h3 id="zero-shot-推理">Zero-shot 推理</h3><p>直观地说，预训练任务赋予了模型在推理时对任何提示做出适当响应的能力，因此下游任务可以通过设计适当的提示来解决。一般来说，一系列实用的分割任务可以作为提示。除了自动数据集标记外，作者还在第7部分中的实验中探索了五个不同的示例任务。 ## 模型 SAM包括了三个部分 一个 image encoder, 一个 flexible prompt encoder, 和一个 fast mask decoder. <img src="https://proxy.thisis.plus/20230407203046.png" alt="image.png" /></p><h3 id="image-encoder">image encoder</h3><p>受可扩展性和强大的预训练方法的启发，作者使用了MAE预训练的视觉转换器（ViT），该转换器至少适用于处理高分辨率输入。图像编码器每个图像运行一次，并且在prompt运行之前运行</p><h3 id="prompt-encoder">prompt encoder</h3><p>作者考虑了两组提示：稀疏(sparse)（点、框、文本）和密集(dense)（掩码）。MAE通过位置编码来表示点和框，这些位置编码与每个使用CLIP的现成文本编码器来编码过的prompt的学习嵌入相加。dense prompt（即掩码）使用卷积嵌入，并与图像嵌入逐元素求和。</p><h3 id="mask-decoder">mask decoder</h3><p>掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。这种设计受到的启发，对 Transformer decoder 进行了修改，然后是动态掩码预测头。修改后的解码器块在两个方向上使用提示自注意力和交叉注意力（(prompt-to-image embedding，反之亦然）来更新所有嵌入。在运行两个块后，对图像嵌入进行上采样，MLP将输出标记映射到动态线性分类器，然后计算每个图像位置的mask foreground 概率。</p><h2 id="data-engine">Data engine</h2><p>由于分割掩码在互联网上并不丰富，作者构建了一个数据引擎来实现1.1B 掩码数据集 SA-1B 的集合。</p><p>数据引擎分为三个阶段：（1）模型辅助手动注释阶段，（2）混合自动预测掩码和模型辅助注释的半自动阶段，以及（3）全自动阶段，</p><h3 id="手动阶段">手动阶段</h3><p>在第一阶段，类似于经典的交互式分割，一组专业注释者通过使用由 SAM 驱动的基于浏览器的交互分割工具点击前景/背景对象点来标记掩码。可以使用像素精确的“刷”和“擦除”工具来细化掩码。模型辅助注释直接在浏览器内实时运行（使用预先计算的图像嵌入），从而实现真正的交互体验。标注不受语义约束，可以自由地标注"stuff" and "things"</p><p><strong>注释者被要求按突出顺序标记对象，一旦掩码需要超过 30 秒进行注释，便鼓励继续下一个图像。</strong></p><p>在SOTA之后，SAM就开始使用公共数据集进行训练，在经过了足够多的数据标注后，就用新标注的数据重新训练。随着收集更多的掩码，图像使用了ViT-H作为编码器。这样的模型训练一共进行了六次。随着模型的改进，每个掩码的平均注释时间从 34 秒减少到 14 秒。随着SAM的改进，每张图像的平均掩码数从20个掩码增加到44个掩码。总体而言，作者在这个阶段从 120k 张图像收集了 4.3M 掩码。</p><h3 id="半自动化阶段">半自动化阶段</h3><p>这个阶段的目标是增加mask的多样性。为了将标记集中在不太突出的对象上，首先自动检测confident masks。然后向注释者展示了用这些掩码预先填充的图像，并要求他们注释任何额外的未注释对象。为了检测confident masks，作者使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界框检测器。在这个阶段，作者在 180k 图像中收集了一个额外的 5.9M 掩码（总共 10.2M 掩码）。在第一阶段，在新收集的数据（5 次）上定期重新训练模型。每个掩码的平均注释时间可以回到了到 34 秒（不包括自动掩码），因为这些对象对标签更具挑战性。每张图像的平均掩码数从 44 个掩码到 72 个掩码（包括自动掩码）。</p><h3 id="全自动化阶段">全自动化阶段</h3><p>这个阶段的主要目的是解决歧义</p><p>正文部分说的不太清楚，在附录部分作者做了详细的解释。一共分为四个部分，Cropping，Filtering，Postprocessing，Automatic mask generation model。</p><p>这个过程作者使用<span class="math inline">\(32*32\)</span>网格的点对图像进行预测，并为每个点预测一组可能对应于有效对象的掩码。如果说一个点位于一个部件或子部件上，我们的模型将返回子部分，部分和整个对象(subpart, part, and whole object)。利用模型中的IoU预测模块来选择confident mask.如果将概率图阈值设为0.5−δ和0.5 + δ会产生相似的掩码，那么这个掩码就被认为是是稳定的。为了进一步提高小mask的质量，还处理了多个重叠的放大mask。</p><p>全自动掩码生成应用于数据集中的所有 11M 图像，总共产生了 1.1B 的高质量掩码。</p><h2 id="数据集">数据集</h2><p>SA的数据集使用data engine 构建的多样的高分辨率的有隐私保护的图像和1.1B个掩码组成。作者发布了这个这个数据集来帮助未来计算机视觉基础模型。SA-1B 将在某些研究用途的有利许可协议下发布，并为研究人员保护。 ### 图像 作者团队从直接与摄影师一起工作的提供商那里获得了一组新的高分辨率的11M图像。即使在下采样之后，这些图像的分辨率也明显高于许多现有的视觉数据集</p><h3 id="掩码">掩码</h3><p>数据引擎产生了 1.1B 掩码，其中 99.1% 是全自动生成的。因此，自动掩码的质量至关重要。作者团队将这些mask与专业标记的数据集进行标记，发现自动掩码对于训练模型是高质量和有效的。受这些发现的启发，SA-1B 仅包含自动生成的掩码。</p><blockquote><p>To estimate mask quality, we randomly sampled 500 images (∼50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise "brush" and "eraser" editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94% of pairs have greater than 90% IoU (and 97% of pairs have greater than 75% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91% IoU [44, 60]. Our experiments in §7 confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine. ## Responsible AI</p></blockquote><h2 id="zero-shot推理实验">Zero-Shot推理实验</h2><p>作者在这里讨论了五个任务，其中四个与训练数据完全不同。这也避免了模型训练过程中能够看到答案。这几个任务分别是 1. zero-shot单点有效掩码评估 2. 执行边缘检测 3. 分割所有内容，即对象提议生成 4. 分割检测到的对象，即实例分割， 5. 作为概念验证，从自由形式的文本中分割对象。</p><h2 id="讨论">讨论</h2><p>自机器学习的早期以来，预训练模型已经适应下游任务。近年来，随着对规模的日益重视，这种范式变得越来越重要，并且此类模型最近被称为为“基础模型”，即"大规模在广泛的数据上训练并适应广泛的下游任务"</p><p>作者的工作是与此高度相关的，尽管分割只是计算机视觉任务的一个子集。作者还将他们的方法的一个方面与另一项工作进行了对比<a href="https://arxiv.org/abs/2108.07258">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv:2108.07258, 2021. 1, 12</a>，# On the Opportunities and Risks of Foundation Models强调了自监督学习在基础模型中的作用。虽然SA模型是用自监督技术(MAE)初始化的，但它的绝大多数能力来自于大规模的监督训练。在数据引擎可以扩展可用注释的情况下，监督训练提供了一种有效的解决方案。</p><p>SA不可避免地也有一些局限性，SAM是为通用性和使用广度而设计的，不同于以往的很多工作，它不是高IoU交互式分割。虽然SAM可以执行许多任务，但目前尚不清楚如何设计简单的提示符来实现语义和全景分割。最后，还有一些领域特定的工具，它们在各自的领域中依然有希望优于SAM。</p><p>总而言之，Segment Anything项目是将图像分割提升到基础模型时代的一种尝试。这项工作的主要贡献是一个新的任务(提示分割)，模型(SAM)和数据集(SA-1B)，使这一飞跃成为可能。SAM是否达到了基础模型的地位，仍然要看它在社区中是如何使用的，但这项工作的前景，超过1B个掩模的发布，以及作者的快速分割模型将有助于铺平前进的道路。</p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/">笔记</category>
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</category>
      
      <category domain="https://studyinglover.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/">多模态</category>
      
      
      <comments>https://studyinglover.com/2023/04/07/Segment%20Anything%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Imagic笔记</title>
      <link>https://studyinglover.com/2023/03/29/Imagic%E7%AC%94%E8%AE%B0/</link>
      <guid>https://studyinglover.com/2023/03/29/Imagic%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Wed, 29 Mar 2023 19:42:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;imagic笔记&quot;&gt;Imagic笔记&lt;/h1&gt;
&lt;p&gt;先前的工作大多数方法目前仅限于以下一种:特定的编辑类型(例如，对象叠加，样式转换)，合成生成的图像，或需要一个共同对象的多个输入图像。文章作者展示了将复杂的基于文本的语义编辑应用于单个真实图像的能力。与之前的工</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="imagic笔记">Imagic笔记</h1><p>先前的工作大多数方法目前仅限于以下一种:特定的编辑类型(例如，对象叠加，样式转换)，合成生成的图像，或需要一个共同对象的多个输入图像。文章作者展示了将复杂的基于文本的语义编辑应用于单个真实图像的能力。与之前的工作相反，这篇文章提出的方法只需要一个输入图像和一个目标文本(所需的编辑)。它生成一个与输入图像和目标文本一致的文本嵌入，同时微调扩散模型以捕获特定于图像的外观。</p><p>扩散模型是一种强大的最先进的生成模型，能够进行高质量的图像合成。在自然语言文本提示的条件下，它们能够生成与所请求的文本很好地对齐的图像。在工作中使用它们来编辑真实的图像，而不是合成新的图像。文章作者通过一个简单的3步过程来实现这一点，如图所示:首先优化文本嵌入，使其生成与输入图像相似的图像。然后，对预训练的生成扩散模型(以优化的嵌入为条件)进行微调，以更好地重建输入图像。最后，在目标文本嵌入和优化后的文本之间进行线性插值，得到一个结合了输入图像和目标文本的表示。然后将这种表示传递给带有微调模型的生成扩散过程，输出最终编辑的图像。 <img src="https://proxy.thisis.plus/20230312151044.png" alt="image.png" /></p><p>作者这进一步得到了一项人类感知评估研究的支持，在一项名为TEdBench -文本编辑基准的新基准测试中，评分者强烈倾向于图像而不是其他方法。</p><h2 id="方法">方法</h2><p>作者将整个过程分成三个部分</p><ul><li>优化文本嵌入，以在目标文本嵌入附近找到与给定图像最匹配的文本嵌入</li><li>微调扩散模型，以更好地匹配给定的图像</li><li>在优化的嵌入和目标文本嵌入之间进行线性插值，以找到一个既能达到输入图像的保真度又能达到目标文本对齐的点。</li></ul><h3 id="text-embedding-optimization">Text embedding optimization</h3><p>目标文本首先通过文本编码器，它输出其对应的文本嵌入<span class="math inline">\(\textbf{e}_{tgt}\in\mathbb{R}^{T\times d}\)</span> ，其中<span class="math inline">\(T\)</span>是给定目标文本中的标记数，<span class="math inline">\(d\)</span>是标记嵌入维数。然后冻结生成扩散模型<span class="math inline">\(f_\theta\)</span>的 参数，并使用[[DDPM]]目标<span class="math display">\[\mathcal{L}(\mathbf{x},\mathbf{e},\theta)=\mathbb{E}_{t,\epsilon}\left[\left\|\epsilon-f_{\theta}(\mathbf{x}_{t},t,\mathbf{e})\right\|_{2}^{2}\right]\]</span>优化目标文本嵌入<span class="math inline">\(E_{tgt}\)</span>.其中<span class="math inline">\(t\sim Uniform[1,T]\)</span> , <span class="math inline">\(x_t\)</span>是使用<span class="math inline">\(\boldsymbol{\epsilon}{\sim}\mathcal{N}(0,\text{I})\)</span>和方程1获得的x(输入图像)的噪声版本，<span class="math inline">\(\theta\)</span>是预训练的扩散模型权重。这将产生与输入图像尽可能匹配的文本嵌入。作者运行这个过程的步骤相对较少，以保持接近最初的目标文本嵌入，获得<span class="math inline">\(E_{opt}\)</span>。这种接近性在嵌入空间中实现了有意义的线性插值，而对于遥远的嵌入不表现出线性行为。</p><figure><img src="https://proxy.thisis.plus/20230312163802.png" alt="" /><figcaption>image.png</figcaption></figure><h3 id="model-fine-tuning">Model fine-tuning</h3><p>请注意，当经过生成扩散过程时，得到的优化嵌入<span class="math inline">\(E_{opt}\)</span>并不一定会导致输入图像<span class="math inline">\(X_{exactly}\)</span>，因为作者的方法优化运行了少量步骤(见图7中的左上角图像)。因此，在方法的第二阶段，通过使用公式2中所示的相同损失函数优化模型参数<span class="math inline">\(\theta\)</span>来缩小这一差距，同时冻结优化的嵌入。这个过程移动模型以适应输入图像<span class="math inline">\(x\)</span>在点<span class="math inline">\(E_{opt}\)</span>处的位置。同时，微调底层生成方法中出现的任何辅助扩散模型，例如超分辨率模型。作者用相同的重构损失对它们进行微调，但以<span class="math inline">\(E_{tgt}\)</span>为条件，因为<span class="math inline">\(E_{opt}\)</span>仅针对基本模型进行了优化。这些辅助模型的优化确保了保留基本分辨率中不存在的<span class="math inline">\(x\)</span>的高频细节 <img src="https://proxy.thisis.plus/20230312164454.png" alt="image.png" /></p><h3 id="text-embedding-interpolation">Text embedding interpolation</h3><p>由于生成扩散模型被训练为在优化的嵌入<span class="math inline">\(E_{opt}\)</span>处完全重建输入图像，作者使用它来应用所需的编辑，从而沿着目标文本嵌入的方向前进。更正式地说，第三阶段是<span class="math inline">\(E_{tgt}\)</span>和<span class="math inline">\(E_{opt}\)</span>之间的简单线性插值。对于给定的超参数<span class="math inline">\(\eta\in[0,1]\)</span>，就得到了<span class="math display">\[\bar{\mathbf{e}}=\eta\cdot\mathbf{e}_{tgt}+(1-\eta)\cdot\mathbf{e}_{opt}\]</span> 这是表示期望编辑图像的嵌入。然后，应用基础生成扩散过程使用微调模型，条件是<span class="math inline">\(\bar{\mathbf{e}}\)</span>。这将导致低分辨率的编辑图像，然后使用微调辅助模型，以目标文本为条件进行超分辨。这个生成过程输出最终的高分辨率编辑图像<span class="math inline">\(x\)</span>。</p><h2 id="实验">实验</h2><h3 id="消融实验">消融实验</h3><p>作者在消融研究中发现微调会强制引入来自输入图像的细节，超出了仅优化的嵌入，使他们的方案能够保留这些细节用于中间的η值，从而实现语义上有意义的线性插值。因此作者得出结论，模型微调对其方法的成功至关重要。</p><p>作者尝试了尝试了文本嵌入优化步骤的数量。作者通过实验表明通过较少的步骤优化文本嵌入将限制模型的编辑能力，而通过超过100步的优化几乎没有额外的价值。</p><h3 id="局限性">局限性</h3><p>作者在研究中发现了两种方法失败的情况：一种是所需编辑的效果非常微弱（如果有的话），因此与目标文本不太匹配；另一种是编辑效果很好，但会影响到外部图像细节，如缩放或摄像机角度。作者在第10张图中分别展示了这两种失败情况的示例。当编辑效果不够强烈时，增加η通常可以实现期望的结果，但在少数情况下会导致原始图像细节的显著丢失（对于所有测试的随机种子）。至于缩放和摄像机角度的变化，这通常发生在我们从低η值逐渐增加到较大值时，因此很难避免。作者在附录中展示了这一点，并在TEdBench中包含了额外的失败案例。这些局限性可能可以通过不同的方式优化文本嵌入或扩散模型来缓解，或者类似于<a href="https://arxiv.org/abs/2208.01626">Hertz etal.</a>的交叉关注控制。作者将这些选项留给未来的工作。</p><figure><img src="https://proxy.thisis.plus/20230329195843.png" alt="" /><figcaption>image.png</figcaption></figure><p>此外，由于该方法依赖于预训练的文本到图像扩散模型，因此继承了模型的生成限制和偏见。因此，当所需编辑涉及生成底层模型的失败案例时，会产生不必要的伪像。例如，Imagen在人脸方面的生成性能不佳</p><h2 id="结论和未来的工作">结论和未来的工作</h2><p>作者认为下一步的工作主要有两个方面 - 一是进一步提高算法对输入图像的准确性和对身份的保护，同时增强对随机种子和插值参数 η 的敏感性； - 二是开发自动选择每个请求编辑的 η 值的方法</p><p>社会影响方面作者则认为模型容易受到基于文本的生成模型的社会偏见的影响，这些技术可能被恶意方用于合成虚假的图像以误导观众。为了缓解这种情况，需要进一步研究如何识别合成编辑或生成内容</p>]]></content:encoded>
      
      
      <category domain="https://studyinglover.com/categories/%E7%AC%94%E8%AE%B0/">笔记</category>
      
      
      <category domain="https://studyinglover.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</category>
      
      
      <comments>https://studyinglover.com/2023/03/29/Imagic%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>UE,Unity和WebGL技术对比</title>
      <link>https://studyinglover.com/2023/03/21/UE,Unity%E5%92%8CWebGL%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/</link>
      <guid>https://studyinglover.com/2023/03/21/UE,Unity%E5%92%8CWebGL%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/</guid>
      <pubDate>Tue, 21 Mar 2023 19:00:00 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;ueunity和webgl技术对比&quot;&gt;UE,Unity和WebGL技术对比&lt;/h1&gt;
&lt;p&gt;随着科技的不断进步和数字化时代的到来，元宇宙（Metaverse）已经成为了人们对未来虚拟世界的向往和探索。构建一个具有真实感、互动性和无限可能性的元宇宙需要借助于强大的技</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="ueunity和webgl技术对比">UE,Unity和WebGL技术对比</h1><p>随着科技的不断进步和数字化时代的到来，元宇宙（Metaverse）已经成为了人们对未来虚拟世界的向往和探索。构建一个具有真实感、互动性和无限可能性的元宇宙需要借助于强大的技术平台。UE（Unreal Engine）、Unity和WebGL作为当前最主流的数字娱乐和游戏开发引擎，被广泛应用于游戏、影视等领域。那么在构建元宇宙的过程中，这三种技术平台各自有什么优缺点？它们又分别适用于哪些场景呢？本文将会深入比较这三种技术平台的差异与联系，并探讨它们在构建元宇宙方面的优缺点及适用场景。</p><h2 id="ue">UE</h2><p>Unreal Engine（以下简称UE）是一个流行的游戏引擎，它被广泛用于创建2D和3D游戏、虚拟现实应用程序和模拟器等。</p><h3 id="优势">优势</h3><ol type="1"><li><p>强大的可视化编辑器：UE具有易于使用的可视化编辑器，使得开发人员可以快速构建游戏场景和世界，而无需编写复杂的代码。</p></li><li><p>跨平台支持：UE支持多个操作系统和平台，包括Windows、Mac、Linux、iOS和Android等，因此开发者可以轻松地将游戏或应用程序移植到不同的设备上。</p></li><li><p>模块化设计：UE允许开发者将游戏逻辑和功能划分为独立的模块，从而提高了代码的可维护性和可重用性。</p></li><li><p>社区支持：UE拥有庞大的用户社区，其中包括开源代码、教程、示例项目、脚本和插件等资源，这些都能够加速开发过程。</p></li><li><p>支持虚幻市场：UE拥有一个虚幻市场，使开发者可以购买和出售游戏资产、工具和插件等，从而节省开发成本和时间。</p></li></ol><h3 id="劣势">劣势</h3><ol type="1"><li><p>入门门槛较高：UE可能需要一些技术能力和时间才能学会，尤其是对于初学者来说，它的学习曲线可能相对陡峭。</p></li><li><p>高昂的费用：虽然UE是免费的，但如果要在商业项目中使用，则需要支付5%的版税。此外，如果您需要访问源代码，则需要购买专业版本。</p></li><li><p>性能问题：与其他游戏引擎相比，UE可能需要更高的硬件配置以及更长的加载时间，导致游戏运行时的性能受到影响。</p></li></ol><h3 id="发展前景">发展前景</h3><p>UE的未来前景非常光明。随着虚拟现实和增强现实技术的不断发展，UE作为一个全面的游戏引擎，在这个领域有着巨大的潜力。UE拥有强大的开发工具和社区支持，可以加速游戏和应用程序的开发过程。此外，UE还支持跨平台开发，在全球范围内开发者社区都很活跃，这也预示着UE未来将在游戏和虚拟现实市场占据更大的份额。</p><h2 id="unity">Unity</h2><p>Unity是一个流行的跨平台游戏引擎，被广泛用于创建2D和3D游戏、虚拟现实应用程序和模拟器等。以下是Unity的优势和劣势以及发展前景的详细分析：</p><h3 id="优势-1">优势</h3><ol type="1"><li><p>跨平台支持：Unity支持多个操作系统和平台，包括Windows、Mac、Linux、iOS和Android等，因此开发者可以轻松地将游戏或应用程序移植到不同的设备上。</p></li><li><p>可视化编辑器：Unity具有易于使用的可视化编辑器，使得开发人员可以快速构建游戏场景和世界，而无需编写复杂的代码。</p></li><li><p>大量资源：Unity拥有庞大的用户社区，其中包括开源代码、教程、示例项目、脚本和插件等资源，这些都能够加速开发过程。</p></li><li><p>支持多种语言：Unity支持多种编程语言，包括C#、JavaScript和Boo等，使开发者可以选择最适合自己的编程语言。</p></li><li><p>发布方便：使用Unity可以轻松地发布游戏到多个平台和应用商店中，如Steam、App Store和Google Play等。</p></li></ol><h3 id="劣势-1">劣势</h3><ol type="1"><li><p>性能问题：与其他游戏引擎相比，Unity可能需要更高的硬件配置以及更长的加载时间，导致游戏运行时的性能受到影响。</p></li><li><p>版本管理问题：由于Unity的版本更新频繁，更新后可能会出现一些兼容性问题，需要花费更多的时间来解决。</p></li><li><p>费用较高：虽然Unity可以免费使用，但如果需要访问高级功能和技术支持，则需要购买专业版。</p></li></ol><h3 id="发展前景-1">发展前景</h3><p>随着虚拟现实和增强现实技术的不断发展，Unity作为一个全面的游戏引擎，在这个领域有着巨大的潜力。Unity已经成为了VR/AR应用程序开发中的主要玩家之一，其3D渲染能力和跨平台支持也使它在游戏市场占据着重要地位。未来，Unity将继续扩大其功能和工具集，以满足不断变化的市场需求。同时，Unity还在积极改进其开发者体验，增加可访问性和可扩展性，以吸引更多的开发者。总体来说，Unity具有非常广阔的发展前景，将在游戏和虚拟现实市场占据重要地位。</p><h2 id="webgl">WebGL</h2><p>WebGL是一种基于Web的3D图形渲染技术，它可以在浏览器中运行3D游戏和应用程序。以下是WebGL的优势和劣势以及发展前景的详细分析：</p><h3 id="优势-2">优势</h3><ol type="1"><li><p>开放性：WebGL是一个开放标准，无需安装任何插件或软件即可在Web浏览器中运行，任何人都可以轻松访问。</p></li><li><p>跨平台支持：由于WebGL技术的跨平台特性，游戏和应用程序可以在多个设备和操作系统上运行。</p></li><li><p>安全性：WebGL仅在用户的浏览器环境中运行，保护了用户的计算机不受恶意软件攻击和病毒感染的风险。</p></li><li><p>易于使用：WebGL允许开发者使用JavaScript等常见的Web编程语言，而无需学习专门的游戏引擎或开发工具。</p></li><li><p>可扩展性：WebGL允许开发者使用各种第三方库和框架，从而扩展其功能和性能。</p></li></ol><h3 id="劣势-2">劣势</h3><ol type="1"><li><p>性能问题：由于WebGL是基于Web技术构建的，因此它的性能可能受到浏览器和设备的限制，导致游戏运行时的性能相对较低。</p></li><li><p>兼容性问题：WebGL在旧版浏览器上可能无法正常工作，而且不同的浏览器可能会有不同的兼容性问题。</p></li><li><p>复杂性问题：WebGL需要掌握一定的Web编程知识和技能，并且需要处理底层的图形渲染细节和优化问题。</p></li></ol><h3 id="发展前景-2">发展前景</h3><p>随着云游戏、虚拟现实和增强现实等新技术的出现，WebGL作为一种开放、易用和跨平台的3D图形渲染技术，具有非常广阔的发展前景。未来，WebGL将继续推进其性能和兼容性的提高，以满足不断增长的游戏和应用程序市场需求。同时，WebGL还将与WebAssembly等新技术结合使用，提高其在游戏领域的可扩展性和性能表现。总体来说，WebGL将成为未来游戏和应用程序开发的重要趋势之一，为开发者提供更加灵活和创新的解决方案。</p><h2 id="实例">实例</h2><ol type="1"><li><p>使用Unreal Engine：《堡垒之夜》、《生化奇兵无限》、《失落的章节》等游戏都是使用UE引擎开发的。</p></li><li><p>使用Unity：《跑跑卡丁车》、《絕地求生：刺激战场》、《Beat Saber》等游戏都是使用Unity引擎开发的。</p></li><li><p>使用WebGL：Sketchfab是一个基于WebGL技术的在线3D模型库，用户可以在浏览器中查看和分享3D模型。Google Maps也使用了WebGL技术来呈现3D建筑和景点的效果。</p></li><li><p>使用Unreal Engine 和 Unity：虚拟现实设备 Oculus Rift 和 HTC Vive 都支持 UE 和 Unity 游戏引擎，许多 VR 游戏和应用程序都是使用这两个引擎开发的。</p></li><li><p>使用Unity 和 WebGL：Clara.io 是一个基于WebGL技术的三维建模和渲染平台，它使用Unity作为前端工具，并支持导出到WebGL格式以供在线展示。</p></li></ol>]]></content:encoded>
      
      
      
      <category domain="https://studyinglover.com/tags/ThreeJS/">ThreeJS</category>
      
      <category domain="https://studyinglover.com/tags/%E5%85%83%E5%AE%87%E5%AE%99/">元宇宙</category>
      
      
      <comments>https://studyinglover.com/2023/03/21/UE,Unity%E5%92%8CWebGL%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
